\input{../header.tex}

\title{Numerical Analysis Course Notes}
\author{Isaac Holt}

\begin{document}

\input{../titletoc.tex}

\section{Chebyshev Polynomials}

\begin{theorem}
let $w_n(x) = (x - x_0) \dots (x - x_n)$ with distinct nodes ${x_0, \dots, x_n}$, $x_j \subset [-1, 1]$. Then the maximum of $|w_n(x)|$ on $[-1, 1]$ attains its smallest value ($2^{-n}$) iff ${x_j}$ are the zeros of $T_{n + 1}(x)$.
\end{theorem}

\begin{proof}
	($\Longleftarrow$): By construction $2^{-n} T_{n+1}(x)$ is a monic polynomial (highest power of $x$ is 1) with $n + 1$ roots in $[-1, 1]$. Suppose $S_{n+1}(x) = (x - z_0)\dots(x - z_n)$ is another monic polynomial such that $\max |S_{n+1}(x)| < 2^{-n} = \max |2^{-n} T_{n+1}(x)|$. Let $q_n(x) := 2^{-n} T_{n+1}(x) - S_{n+1}(x)$. Then $q_n(x) \in P_n$ since the coefficient of $x^{n+1}$ in $T_{n+1}(x)$ and $S_{n+1}(x)$ are both $1$ and so cancel out.

	Then $q_n(y_j) = 2^{-n} T_{n+1}(y_j) - S_{n+1}(y_j)$ ($y_j$ are the extrema of $T_n(x)$). $|S_{n+1}(y_j)| < 1$ by hypothesis. Therefore $q_n(y_j) > 0$ if $j$ is odd and $< 0$ otherwise.

	Since we have $n+2$ of $y_j$, $q_n$ has at least $n+1$ zeros. But since $q_n \in P_n$, we must have $q_n(x)=0$. Therefore $S_{n+1}(x) = 2^{-n} T_{n+1}(x)$. 
\end{proof}

\begin{remark}
	To use this in $[a, b]$ instead of $[-1, 1]$, one simply maps $x_j \rightarrow a + (x_j + 1) \frac{b - a}{2}$.
\end{remark}

\begin{remark}
	Putting the above into Cauchy's error formula,
	we have \[\sup |f(x)-p(x)| \le 2^{-n} {(\frac{b - a}{2})}^{n + 1} \frac{1}{(n + 1)!}\]
\end{remark}

\begin{remark}
	We have by the above theorem, $\max |w_n(x)| = \max |(x - x_0)\dots(x - x_n) \ge 2^{-n}$ for any choice of ${x_1, \dots, x_n}$, $x_j \subset [-1, 1]$. So $2^{-n}$ is a lower bound for $|w_n(x)|$.

	The upper bound is given by $\max |w_n(x)| \le \epsilon |b-a|^n$. 
\end{remark}

\maketitle
\section{Root Finding}

\subsection{Bracketing: Bisection}

Given $f \in C^0 ([a, b])$ with $f(a) f(b) < 0$, repeat:
\begin{itemize}
	\item let $(a_0, b_0) = (a, b)$
	\item let $m_n = \frac{1}{2} (a_n + b_n)$
	\item if $f(m_n)f(a_n) \ge 0$, set $(a_{n + 1}, b_{n + 1}) = (m_n, b_n)$
	\item otherwise, set $(a_{n + 1}, b_{n + 1}) = (a_n, m_n)$
\end{itemize}

$b_{n + 1} - a_{n + 1} = \frac{1}{2} (b_n - a_n)$. By the Intermediate Value Theorem, if $f(m_n) \ne 0$, for some $p \in (a_n, b_n)$, $f(p) = 0$.

$|p - m_n| \le 2^{-(n + 1)} (b - a)$

\begin{remark}
	Each time, the width of the interval halves. In principle, we could get an approximation to any desired accuracy, but there are some caveats (e.g. with floating points).
\end{remark}

\subsection{Bracketing: False Position}

Suppose we have $|f(b)| \ll |f(a)|$, then we would expect $p$ to be closer to $b$ than to $a$. Instead of $m_n = \frac{1}{2} (a_n + b_n)$, set

\[m_n = b_n - f(b_n) \frac{b_n - a_n}{f(b_n) - f(a_n)}\]

i.e. $m_n$ is the x-intercept of the line from $(a_n, f(a_n))$ to $(b_n, f(b_n))$. This should sometimes give much faster approximation than bisection, but not always.

\subsection{Aside: Continuity and Convergence}

\begin{definition}
	$f: I \rightarrow \mathbb{R}$ is continuous at $x \in I$ if for every $\epsilon > 0$, for some $\delta(x, \epsilon)$, $|y - x| < \delta \Rightarrow |f(x) - f(y)| < \epsilon$ for every $y \in B(x)$ ($B(x)$ is an open interval containing $x$).
\end{definition}

\begin{remark}
	In general, $\delta$ depends on $\epsilon$ and $x$. When $\delta$ is independent of $x$, $f$ is uniformly continuous.
\end{remark}

\begin{definition}
	$f: I \rightarrow \mathbb{R}$ is Lipschitz continuous in $I$ if for some $L > 0$, $|f(y) - f(x)| \le L|y - x|$ for every $x \in I, y \in I$. In this case, $\delta = \epsilon / L$.
\end{definition}

\begin{remark}
	$L$ (like $\delta$ above) is not unique. The smallest such $L$ is called the Lipschitz constant of $f$ in $I$.
\end{remark}

\begin{lemma}
	\hfill
	\begin{enumerate}
		\item If $f$ is differentiable and $I$ is compact, $f$ is Lipschitz in $I$.
		\item If $f$ is Lipschitz, $f$ is continuous.
	\end{enumerate}
\end{lemma}

\begin{proof}
	\[f(y) - f(x) = \int_x^y f'(x) ds\]
	\[|f(y) - f(x)| = |\int_x^y f'(x) ds| \le \int_x^y |f'(x)| ds\]
	\[\le \max_{s \in I} |f'(s)| \int_x^y ds = \max_{s \in I} |f'(s)| |y - x|\]
	We can take $L = \max_{s \in I} |f'(s)|$
\end{proof}

\begin{remark}
	The converses of 1. and 2. are false.
\end{remark}

\begin{remark}
	\begin{itemize}
		\item When $f$ is continuous in $I$, we write $f \in C^0 (I)$.
		\item When $f$ is differentiable in $I$, we write $f \in C^1 (I)$.
		\item When $f$ is Lipschitz in $I$, we write $f \in C^{0, 1} (I)$.
		\item We can then write $C^1 (I) \subsetneq C^{0, 1} (I) \subsetneq C^0 (I)$.
	\end{itemize}
\end{remark}

\begin{definition}
	A sequence $(x_n)$ in $\mathbb{R}^d$ converges to $x$ if for every $\epsilon > 0$, for some $N(\epsilon)$, for every $n \ge N(\epsilon)$, $|x_n - x| < \epsilon$.

	This relies on us knowing $x$ in the first place.
\end{definition}

\begin{definition}
	A sequence $(x_n)$ is a Cauchy sequence if for every $\epsilon > 0$, for some $N(\epsilon)$, for every $m \ge N, n \ge N$, $|x_n - x_m| < \epsilon$.
\end{definition}


\begin{theorem}
	Let $(x_n)$ be a Cauchy sequence in $\mathbb{R}^d$. Then $(x_n)$ converges.
\end{theorem}

This is useful as it allows us to prove convergence without knowing $x$.

\subsection{Fixed Point Iterations}

We seek $x$ such that $f(x) = 0$ for a function $f$. We rewrite this as \[x = g(x)\]

We then seek to solve this equation by iterations:

\begin{enumerate}
	\item pick some $x_0$
	\item set $x_{n + 1} = g(x_n)$
\end{enumerate}

\begin{theorem}
	(1d local convergence theorem): Let $g \in C'([a, b])$ have a fixed point $x_{\star} \in [a, b]$ ($g(x_{\star}) = x_{\star}$) with $|g'(x_{\star}) < 1$. Then for $x_0$ sufficiently close to $x_{\star}$, the iteeration $x_{n + 1} = g(x_n)$ converges to $x_{\star}$.
\end{theorem}

\begin{proof}
	Let $g'(x_{\star}) = L \in (0, 1)$ ($g'(x_{\star}) < 0$ is analogous). Since $g'$ is continuous at $x_{\star}$, for every $L' \in (L, 1)$, for some $\delta(L') > 0$, $g'(x) \le L' < 1$ for every $x \in (x_{\star} - \delta, x_{\star} + \delta) = B_{\delta}$, therefore for every $x \in B_{\delta}$, $y \in B_{\delta}$, $|g(x) - g(y)| \le \sup_{s \in B_{\delta}} |g'(s)| |x - y| = L' |x - y|$ with $L' < 1$.

	Let $x_{\star} \in B_{\delta}$, then $|g(x) - x_{\star}| = |g(x) - g(x_{\star})| \le L' |x - x_{\star}|$ since $x_{\star} = g(x_{\star})$. So $x - x_{\star} \delta$ as $x \in B_{\delta}$, so $|g(x) - x_{\star}| \le L'\delta \le \delta$, therefore $g(B_\delta) \subseteq B_{\delta}$.
\end{proof}

\begin{remark}
	We do not need to know $x_{\star}$ to apply the 1d local convergence theorem, we just need to know that $|g'(x)| < 1$ for every $x \in I$ for some interval $I$.
\end{remark}

\subsection{Order of convergence}

Order of convergence is a rough measure of how quickly $x_n \rightarrow x$. We mainly look at sequences arising from iterations with a nice RHS (so not bisection).

\begin{definition}
	Let $x_n \rightarrow x_*$ and assume that $x_n \ne x_*$ for every $n \ge 0$. $x_n \rightarrow x_*$ with order at least $\alpha > 1$ if

	\[\lim_{n \rightarrow \infty} \frac{|x_{n + 1} - x_*|}{|x_n - x_*|^{\alpha}} = \lambda < todo\]

	and with order $\alpha = 1$ if also $\lambda < 1$.
\end{definition}

\begin{example}
	$x_n = n^{-\beta}$, $\beta > 0$

	\[\frac{|x_{n + 1} - x_*|}{|x_n - x_*|} = {(\frac{n}{n + 1})}^{\beta} \rightarrow 1\]
\end{example}

\begin{example}
	$x_n = e^{-n}$

	\[\frac{|x_{n + 1} - x_*|}{|x_n - x_*|} = 1/e < 1\]
\end{example}

\begin{example}
	$x_n = \frac{1}{n!}$

	\[\frac{|x_{n + 1} - x_*|}{|x_n - x_*|} = 1/(n + 1) \rightarrow 0\]
\end{example}

The order of convergence of $x_n \rightarrow x_*$ is

\[\alpha = \sup \{\beta: \lim_{n \rightarrow \infty} \frac{|x_{n + 1} - x_*|}{|x_n - x_*|^{\beta}} < todo\}\]

for $\alpha > 1$.

For $\alpha = 1$, we also require that the limit $< 1$.

The convergence is linear if $\alpha = 1$, superlinear if $\alpha > 1$ and sublinear otherwise.

\begin{remark}
	Order of convergence need not be an integer.
\end{remark}

\begin{remark}
	We need to know $x_*$ in order to determine order of convergence.
\end{remark}

\begin{remark}
	Our definition is not comprehensive for general sequences.
\end{remark}

Applying this to iterations:

$x_{n + 1} - x_* = g(x_n) - g(x_*) = (x_n - x_*) g'(c_n)$ for some $c \in \text{conv}\{x_n, x_*\}$ by the Mean Value Theorem.

Therefore

\[{|x_{n + 1} - x_*|}{|x_n - x_*|} = |g'(c_n)| \rightarrow g'(x_*)\]

We conclude that for $g \in C^2 (I)$, the iteration $x_{n + 1} = g(x_n)$ converges linearly if $g'(x_*) \ne 0$ and $|g'(x_*)| < 1$, and superlinearly otherwise.

\begin{proposition}
	Let $g \in C^{N + 1} (D)$ for some $D \subseteq \mathbb{R}$ and let $g(x_*) = x_*$, with $x_*$ in the interior of $D$.

	Then the iteration $x_{n + 1} = g(x_n)$ converges to $x_*$ for $x_0$ sufficiently close to $x_*$ with order $N + 1$ iff $g'(x_*) = g''(x_*) = \cdots = g^{(N)} (x_*) = 0$ and $g^{(N + 1)}(x_*) \ne 0$.
\end{proposition}

\begin{proof}
	$x_{n + 1} - x_* = g(x_n) - g(x_*) = g(x_*) + (x_n - x_*)g'(x_*) + \cdots + \frac{{(x_n - x_*)}^N}{N!} g^{(N)}(x_*) + \frac{{(x_n - x_*)}^{N + 1}}{(N + 1)!} g^{(N + 1)}(c_n) - g(x_*) = \frac{{(x_n - x_*)}^{N + 1}}{(N + 1)!} g^{(N + 1)}(c_n)$. Thus

	\[{|x_{n + 1} - x_*|}{|x_n - x_*|^{N + 1}} = \frac{|g^{(N + 1)}(c_n)|}{(N + 1)!} \rightarrow \frac{|g^{(N + 1)}(x_*)|}{(N + 1)!} < todo\]
\end{proof}

\subsection{Higher order iterative methods}

We want to rearrange $f(x) = 0$ to get faster convergence.

$x_{n + 1} = g(x_n) = x_n + \phi (x_n) f(x_n)$ for some $\phi$.

Using the above proposition, we need $g'(x_*) = 0$.

$g'(x_*) = 1 + \phi'(x_*)f(x_*) + \phi(x_*)f'(x_*) = 0$

So if $f'(x_*) \ne 0$, we take $\phi(x) = -\frac{1}{f'(x)}$.

This is the Newton-Raphson method:

$x_{n + 1} = x_n - \frac{f(x_n)}{f'(x_n)}$.

Multiplication of a computer is parallelisable while division is not. So we can use Newton-Raphson to divide numbers with multiplication.

To compute $x_* = 1 / b$ so $f(x_*) = \frac{1}{x_*} - b = 0$, so using Newton-Raphson, $x_{n + 1} = x_n - \frac{{x_n}^{-1} - b}{-{x_n}^{-2}} = x_n (2 - b x_n)$ which involes only multiplication and subtraction. When taking out the exponent, $x_0 \in \left[ \frac{1}{2}, 1 \right)$, and this converges quickly.

\begin{remark}
	This only works with floating points, not integers. Floating point division is 5 times faster than integer division.
\end{remark}

\begin{remark}
	It can be difficult in practice to determine the interval/domain of convergence for Newton-Raphson. We should always perform a ``sanity check'' when using it.
\end{remark}

\begin{example}
	One advantage of iterative methods is that they also work (in principle) in higher dimensions.

	Suppose $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ has a root at $\underline{p} = (p_1, p_2)$. Using Taylor expansion at the current point $\underline{x_n}$, derive Newton-Raphson (2D):

	\[\underline{x_{n + 1}} = \underline{x_n} - {(Df)}^{-1} \underline{f}(\underline{x_n})\]

	We write $\underline{x_*} = \underline{p}$ such that $f_1(p_1, p_2) = f_2(p_1, p_2) = 0$. Taylor-expanding at $\underline{x}$:

	\[0 = f_1(p_1, p_2) = f_1(x_1, x_2) + (p_1 - x_1) \diffp{f_1}{x_1} (x_1, x_2) + (p_2 - x_2) \diffp{f_1}{x_2} (x_1, x_2) + O(|\underline{p} - \underline{x}|^2)\]

	\[0 = f_2(p_1, p_2) = f_2(x_1, x_2) + (p_1 - x_1) \diffp{f_2}{x_1} (x_1, x_2) + (p_2 - x_2) \diffp{f_2}{x_2} (x_1, x_2) + O(|\underline{p} - \underline{x}|^2)\]

	In matrix form:

	\[(0, 0) = (f_1(\underline{x}), f_2(\underline{x})) = (Df)(x_1, x_2) \cdot (x_1 - p_1, x_2 - p_2) + O(|\underline{p} - \underline{x}|^2)\]

	Assuming that $Df$ is invertible (equivalently, $f'(\underline{x}) \ne 0$), we can multiply the equation by ${(Df)}^{-1}$ to get

	\[ (p_1, p_2) = (x_1, x_2) - (({(Df)}^{-1})(x_1, x_2)) \cdot (f_1(\underline{x}), f_2(\underline{x})) + O(|\underline{p} - \underline{x}|^2)\] So
	
	\[\underline{p} = \underline{x} - (({(Df)}^{-1})(x_1, x_2)) \underline{f}(\underline{x}) + O(|\underline{p} - \underline{x}|^2) \] We can use this to construct our iteration by replacing $\underline{x}$ with $\underline{x_n}$ and $\underline{p}$ with $\underline{x_{n + 1}}$, and removing the $O(|\underline{p} - \underline{x}|^2)$.
\end{example}

\subsection{Secant method}

One disadvantage of Newton-Raphson is that we need the derivative, $f'$. If $f$ is complicated or is itself computed numerically, we need to approximate $f'$. An alternative method is the secant method.

The secant method approximates $f'$ with:

\[ f'(x_n) \approx \frac{f(x_n) - f(x_{n - 1})}{x_n - x_{n - 1}} \]
so the iteration becomes

\[ x_{n + 1} = x_n - \frac{x_n - x_{n - 1}}{f(x_n) - f(x_{n - 1})} f(x_n) \]

\begin{remark}
	This is a (scalar) two-step method: $x_{n + 1} = g(x_n, x_{n - 1})$, where $x_n$ and $x_{n - 1}$ are needed to calculate $x_{n + 1}$.
\end{remark}

\begin{theorem}
	Let $f \in C^2$ with $f(x_*) = 0$ and $f'(x_*) \ne 0$. Then the secant method is convergent with order $\alpha = \frac{1 + \sqrt{5}}{2}$ for every $x_0 \ne x_1$ sufficiently close to $x_*$.
\end{theorem}

\begin{proof}
	TODO: See video on Panopto
\end{proof}

\begin{remark}
	\hfill
	\begin{enumerate}
		\item When implementing the secant method, one must be careful with floating point effects:

		\[ \frac{x_n - x_{n - 1}}{f(x_n) - f(x_{n - 1})} \]
		becomes very inaccurate as $x_n - x_{n - 1} \rightarrow 0$.
		\item $e_n := x_n - x_*$ alternates in sign if $g'(x_k) < 0$ and has the same sign if $g'(x_*) > 0$. From the proof of the method,
		
		\[ e_{n + 1} = e_n e_{n - 1} \frac{f''(\theta_n)}{f'(\phi_n)} \]
		where $\theta_n, \phi_n \in \text{conv} \{x_{n - 1}, x_n, x_{n + 1} \}$.

		For $e_0 e_1 < 0$ and $n$ sufficiently large, the error $e_n$ follows the pattern $+,+,-$ or $-,-,+$.
	\end{enumerate}
\end{remark}

\clearpage
\section{Numerical differentiation}

\subsection{Forward/backward difference}

$f'(x) = \lim_{h \rightarrow 0} \frac{f(x + h) - f(x)}{h} \approx \frac{f(x + h) - f(x)}{h}$ for some small $h \ne 0$.

More rigorously, we can use Taylor series:

\[ f(x + h) = f(x) + h f'(x) + \frac{h^2}{2} f''(\xi) \]
where $\xi \in \text{conv} \{x, x + h \}$. So

\[ f'(x) = \frac{f(x + h) - f(x)}{h} - \frac{h}{2} f''(\xi) \]
The error $\frac{h}{2} f''(\xi)$ is of order $O(h)$.

For $h > 0$ this is called forward difference.

For $h < 0$, this is called backward difference.

\subsection{Centred difference}

\[ f(x \pm h) = f(x) \pm h f'(x) + \frac{h^2}{2} f''(x) \pm \frac{h^3}{6} f^{(3)}(\xi_{\pm}) \]
Then

\[ f'(x) = \frac{f(x + h) - f(x - h)}{2h} + \frac{h^2}{12} \left( f^{(3)}(\xi_+) + f^{(3)}(\xi_-) \right) \]
The error $\frac{h^2}{12} \left( f^{(3)}(\xi_+) + f^{(3)}(\xi_-) \right)$ is of order $O(h^2)$.

\begin{remark}
	It is important to \textbf{not take $h$ too small} when computing numerically. There is always a tradeoff between formal analytical accuracy and floating point errors. Formal analytical accuracy is better for smaller $|h|$, floating point errors are worse for smaller $|h|$.
\end{remark}

\begin{remark}
	Sometimes, we can only take $h < 0$ or $h > 0$ (not both), e.g. when solving differential equations numerically. If we have an ODE

	\[ \diff{u}{t} = F(u) \]
	Given $u(0)$ we want to solve for $u(t), t \ge 0$. We approximate $u(t)$ by a $u(t_n)$ with $t_n = n \delta t, n \in \mathbb{N}$, with $\delta t > 0$ small. Then

	\[ \diff{u}{t} \approx \frac{u(t_n + \delta t) - u(t_n)}{\delta t} \approx F(u(t_n)) \]
\end{remark}

\subsection{Richardson extrapolation}

Let $f'(x) - \frac{f(x + h) - f(x)}{h} = f'(x) - R_h^{(1)} (x) = c_1(x) h + c_2(x) h^2 + c_3(x) h^3 + \cdots$ but suppose we cannot compute the $c_k$.

$R_{h / 2}^{(1)} (x) = f'(x) - c_1 \frac{h}{2} - c_2 \frac{h^2}{4} - c_3 \frac{h^3}{6} - \cdots$. We can use this to eleminate $c_1$:

\[ 2 R_{h / 2}^{(1)} (x) - R_h^{(1)} (x) = f'(x) - c_2' h^2 - c_3' h^3 - \cdots \]
So the error is of order $O(h^2)$. Then

\[ f'(x) = R_h^{(2)} (x) + O(h^2) \]
where $R_h^{(2)} = 2 R_{h / 2}^{(1)} (x) - R_h^{(1)} (x)$.

Now, $R_{h / 2}^{(2)} (x) = f'(x) - c_2' \frac{h^2}{4} - c_3' \frac{h^3}{8}$ and we use this to eliminate $c_2'$:

\[ 4 R_{h / 2}^{(2)} (x) - R_h^{(2)} (x) = 3 f'(x) + O(h^3) \]

So we set

\[ R_h^{(3)} := \frac{2^2 R_{h / 2}^{(2)} (x) - R_h^{(2)}(x)}{2^2 - 1} = f'(x) + O(h^3) \]

\begin{remark}
	\hfill
	\begin{enumerate}
		\item We only need to specify the powers of $h$, not the coefficients $c_k$ as long as they are \textbf{non-zero}. So when using centred difference to calculate $R_h^{(1)}$ then $R_h^{(2)}$, $R_h^{(3)}$ will be different.
		\item Richardson extrapolation works with many other approximation methods involving a small parameter (not just differentiation).
		\item There is no standard notation for this method.
		\item Some series expansions have irregular/non-integer powers, e.g. the Airy function $\text{Ai}(x)$ which is a solution of $\diff[2]{f}{x} = x f(x)$.
	\end{enumerate}
\end{remark}

\section{Linear systems}

Given $A \in M_n(\mathbb{R})$ and $\underline{x} \in \mathbb{R}^n$, we want to solve for $\underline{x}$:
\[
	A \underline{x} = \underline{b}
\]
We want to minimise the error when computing this with floating points, and minimise the amount of computation for: large $n$, for different $\underline{b}$ (with the same $A$) and when $A$ has particular forms.

\begin{definition}
	The \textbf{transpose} of a square matrix $A$, $A^T$ is defined as
	\[
		{(A^T)}_{i, j} = A_{j, i}
	\]
\end{definition}

\begin{definition}
	$A$ is \textbf{symmetric} if $A = A^T$.
\end{definition}

\begin{definition}
	$A$ is \textbf{skew-symmetric} if $A = -A^T$.
\end{definition}

\begin{definition}
	$A$ is \textbf{non-singular} if for every $\underline{b}$, for some $\underline{x}$, $A\underline{x} = \underline{b}$.
\end{definition}

\begin{definition}
	$A$ is \textbf{positive definite} if $(A \underline{x}) \cdot \underline{x} > 0 \quad \underline{x} \ne \underline{0}$.
\end{definition}

\begin{definition}
	$A$ is \textbf{positive semi-definite} if $(A \underline{x}) \cdot \underline{x} \ge 0 \quad \forall x$.
\end{definition}

\begin{lemma}
	If $A$ is positive definite, then $A$ is non-singular.
\end{lemma}

\begin{proof}
	Omitted.
\end{proof}

\begin{lemma}
	The converse of the above lemma is false.
\end{lemma}

\begin{proof}
	$A$ is non-singular $\Rightarrow$ $-A$ is non-singular, but $A$ is positive-definite $\Rightarrow$ $-A$ is negative definite.
\end{proof}

\begin{definition}
	A matrix $A$ is \textbf{lower triangular} if $A_{i, j} = 0 \quad \forall j > i$.
\end{definition}

\begin{definition}
	$A$ is \textbf{upper triangular} if $A_{i, j} = 0 \quad \forall j < i$.
\end{definition}

\begin{remark}
	We often write $U$ for an upper triangular matrix and $L$ for a lower triangular matrix.
\end{remark}

\begin{definition}
	To solve $Ux = b$, we can use \textbf{backward substitution}. Starting from the last equation,
	\[
		\begin{aligned}
			x_n & = \frac{1}{U_{n, n}} b_n \\
			x_{n - 1} & = \frac{1}{U_{n - 1, n - 1}} (b_{n - 1} - U_{n - 1, n} x_n) \\
			\vdots & \\
			x_j & = \frac{1}{U_j} \left( b_j - \sum_{i = j + 1}^{n} U_{j, i} x_i \right)
		\end{aligned}
	\]
\end{definition}

\begin{definition}
	Similarly, we can solve $L \underline{x} = \underline{b}$ for a lower triangular matrix $L$ with forward substitution. Starting from the first equation,
	\[
		\begin{aligned}
			x_1 & = \frac{b_1}{L_{1, 1}} \\
			\vdots & \\
			x_j & = \frac{1}{L_{j, j}} \left( b_j - \sum_{i = 1}^{j - 1} L_{j, i} x_k \right)
		\end{aligned}
	\]
\end{definition}

\begin{remark}
	If $U_{j, j} = 0$ or $L_{j, j} = 0$ for some $j$, then this method doesn't work. This is expected, because in this case $\det U = 0$ (or $\det L = 0$).
\end{remark}

\begin{definition}
	If $A$ is neither upper nor lower triangular, then we can transform $A$ into a (usually) upper triangular matrix, by \textbf{Gaussian elimination}.

	The following operations leave the solution $\underline{x}$ unchanged:
	\begin{enumerate}
		\item Swapping two rows
		\item Adding a scalar multiple of a row to another row.
	\end{enumerate}
\end{definition}

\subsection{Computational complexity}

\begin{definition}
	For simplicity, we assume that each elementary floating point operation takes 1 unit of time, called 1 \textbf{flop}.
\end{definition}

\begin{remark}
	In practice, binary64 multiplication takes roughly 3 times as long as addition or multiplication, and binary64 division takes roughly 10 times as long as addition or subtraction.
\end{remark}

\begin{definition}
	Let $f(n) > 0$ and $g(n) > 0$ for large $n$. We write
	\[
		f(n) \sim o(g(n)) \quad \text{ if } \lim_{n \rightarrow \infty} \frac{f(n)}{g(n)} = 0
	\]
\end{definition}

\begin{definition}
	Let $f(n) > 0$ and $g(n) > 0$ for large $n$. We write
	\[
		f(n) \sim O(g(n)) \quad \text{ if } \limsup_{n \rightarrow \infty} \frac{f(n)}{g(n)} < \infty
	\]
	Equivalently,
	\[
		f(n) \sim O(g(n)) \quad \text{ if } \exists C, \exists N, \forall n \le N, f(n) \le C g(n)
	\]
\end{definition}

\begin{remark}
	From these definitions, we have
	\[
		f(n) \sim o(g(n)) \Longrightarrow f(n) \sim O(g(n))
	\]
\end{remark}

\begin{example}
	\hfill
	\begin{itemize}
		\item $100n^3 + 10^6 n^2 \sim O(n^3)$
		\item $n! + 10^{100} n^{100} \sim O(n!)$
		\item $n! \sim O(n^{n + \frac{1}{2}} e^{-n})$
	\end{itemize}
\end{example}

\begin{remark}
	In this module, we will be calculate the computational complexity to \textbf{leading order}, e.g. we distinguish $3n^2 + 5n$ form $30n^2$ but not from $3n^2 - 2n$.
\end{remark}

\begin{proposition}
	Backwards substitution on $U$ where $U$ is an $n \times n$ matrix is an $O(n^2)$ operation.
\end{proposition}

\begin{proof}
	\hfill
	\begin{itemize}
		\item Computing $x_n = b_n / U_{n, n}$ takes 1 flop.
		\item Computing $x_{n - 1} = \frac{(b_{n - 1} - U_{n - 1, n} x_n)}{U_{n - 1, n - 1}}$ takes 3 flops.
		\item $\vdots$
		\item Computing $x_1 = \frac{1}{U_1} \left( b_1 - \sum_{i = j + 2}^{n} U_{1, i} x_i \right)$ takes $2n - 1$ flops.
	\end{itemize}
	So in total, backward substitution takes $1 + 3 + \cdots + 2n - 1 = n^2$ flops.
\end{proof}

\begin{proposition}
	The number of flops needed for solving $Ax = b$ where $A$ is an $n \times n$ matrix is
	\[
		\frac{2}{3} n^3	+ \frac{3}{2} n^2 - \frac{7}{6} n
	\]
\end{proposition}

\begin{proof}
	To zero the first column for rows $i \in \{ 2, \dots, n \}$:
	\begin{itemize}
		\item $\alpha_{i, 1} = -A_{i, 1} / A_{1, 1}$ ($1$ flop).
		\item $A_{i, j} \rightarrow A_{i, j} + \alpha_{i, 1} A_{1, j}$ for $j \in \{ 2, \dots, n \}$ and $A_{i, 1} = 0$ ($2(n - 1)$ flops).
		\item $b_i \rightarrow b_i + \alpha{i, 1} b_1$ ($2$ flops).
	\end{itemize}
	This is $2n + 1$ flops in total for each row, and this is done for $n - 1$ rows, so in total there are $(2n + 1)(n - 1)$ flops. For row $2$, there are $(2n - 1)(n - 2)$ flops.

	In general, to zero column $k$ for rows $i \in \{ k + 1, \dots, n \}$:
	\begin{itemize}
		\item $\alpha_{i, k} = -A_{i, k} / A_{k, k}$ ($1$ flop).
		\item $A_{i, j} \rightarrow A_{i, j} + \alpha_{i, k} A_{k, j}$ for $j \in \{ k + 1, \dots, n \}$ ($2(n - k)$ flops).
		\item $b_i \rightarrow b_i + \alpha_{i, k} b_k$ ($2$ flops).
	\end{itemize}
	This is $2n - 2k + 3$ flops in total for each row, so for the $n - k$ rows, in total there are $(2n - 2k + 3)(n - k)$ flops.

	So the total number of flops for Gaussian elimination is
	\[
		\begin{aligned}
			\sum_{k = 1}^{n - 1} (2n - 2k + 3)(n - k)
				& = \sum_{k = 1}^{n - 1} 2n^2 - 2kn + 3n - 2nk + 2k^2 - 3k \\
				& = (n - 1)(2n^2 + 3n) - (4n + 3) \sum_{k = 1}^{n - 1} k + 2 \sum_{k = 1}^{n - 1} k^2 \\
				& = (n - 1)(2n^2 + 3n) - (4n + 3) \frac{n(n - 1)}{2} \\
				&\quad + \frac{1}{3} (n - 1) n (2n - 1) \\
				& = \frac{2}{3} n^3 + \frac{1}{2} n^2 - \frac{7}{6} n
		\end{aligned}
	\]
	Adding the $n^2$ flops from back substitution, in total to solve $Ax = b$, the number of flops needed is
	\[
		\frac{2}{3} n^3	+ \frac{3}{2} n^2 - \frac{7}{6} n
	\]
\end{proof}

\begin{remark}
	Gaussian elimination with (row) pivotting can transfomr every non-singular matrix into an upper-triangular matrix.
\end{remark}

\subsection{Pivoting and roundoffs}

\begin{definition}
	To reduce round-off errors, we define \textbf{row/partial pivoting}:
	\begin{itemize}
		\item When zeroing column $k$, look for $A_{j, k}$ with $j \in \{ k, \dots, n \}$ with the largest absolute value, and swap row $j$ with row $k$.
	\end{itemize}
\end{definition}

\begin{remark}
	Multiplying a pivot row by a large constant does not improve reduction of round-off errors. The \textbf{ratio} of the pivot element to the other elements in the row is more important. 
\end{remark}

\begin{remark}
	For better results, we can also perform column/full pivoting (so we swap rows and columns), but this is much more complicated.
\end{remark}

\subsection{LU decomposition}

\begin{lemma}
	Steps of the Gaussian elimination process can be written as matrix multiplication. During Gaussian elimination, let $A^{(k)}$ be the matrix during Gaussian elimination which has zeros below the diagonal in the first $k$ columns.
	\[
		A^{(s)} = F^{(s)} A^{(s - 1)}
	\]
	where
	\[
		\begin{aligned}
			F_{i, j}^(s) & = \delta_{i, j} - f_i^{(s)} e_j^{(s)} \\
			f_i^{(s)} & = (0, \dots, 0, A_{s+1, s}^{s-1} / A_{s,s}^{(s - 1)}, \dots, A_{n, s}^{(s - 1)} / A_{s, s}^{(s - 1)}) \\
			e_j^{(s)} = \delta_{s, j}
		\end{aligned}
	\]
\end{lemma}

\begin{proof}
	To zero column $s$ below the leading diagonal, we do
	\[
		A_{i, j}^{(s)} = A_{i, j}^{(s - 1)} - \frac{A_{i, s}^{(s - 1)}}{A_{s, s}^{(s - 1)}} A_{s, j}^{(s - 1)}
	\]
	for $i \in \{ s + 1, \dots, n \}$ and $j \in \{ s, \dots, n \}$.
	\[
		\begin{aligned}
			A_{i, j}^{(s)}
				& = \sum_k \left( \delta_{i, k} A_{k, j}^{(s - 1)} - \frac{A_{i, s}^{(s - 1)}}{A_{s, s}^{(s - 1)}} \delta_{s, k} A_{k, j}^{(s - 1)} \right) \\
				& = \sum_k \left( \delta_{i, k} - \frac{A_{i, s}^{(s - 1)}}{A_{s, s}^{(s - 1)}} \cdot \delta_{s, k} \right) A_{k, j}^{(s - 1)}
		\end{aligned}
	\]
\end{proof}

\begin{definition}
	For vectors $u$ and $v$, the \textbf{tensor product} of $u$ and $v$ is a matrix defined by
	\[
		{(u \otimes v)}_{i, j} = u_i v_j
	\]
\end{definition}

\begin{definition}
	A \textbf{Frobenius matrix} of index $s$ is a unit lower triangular matrix whose only (possibly) non-zero elements are subdiagonal elements in column $s$ (other than the leading diagonal, which is all $1$s).
\end{definition}

\begin{lemma}\label{lem:GFinverse}
	Let $F^{(s)} = I - f^{(s)} \otimes e^{(s)}$. Then its inverse is
	\[
		G^{(s)} = I + f^{(s)} \otimes e^{(s)}
	\]
\end{lemma}

\begin{proof}
	\[
		\begin{aligned}
			\sum_k F_{i, k}^{(s)} G_{k, j}^{(s)}
				& = \sum_k \left( \delta_{i, k} - f_i^{(s)} e_k^{(s)} \right) \left( \delta_{k, j} + f_k^{(s)} e_j^{(s)} \right) \\
				& = \sum_k \left( \delta_{i, k} - f_i^{(s)} \delta_{s, k} \right) \left( \delta_{k, j} + f_k^{(s)} \delta_{s, j} \right) \\
				& = \sum_k \left( \delta_{i, j} + f_i^{(s)} \delta_{s, j} - f_i^{(s)} \delta_{s, j} - f_i^{(s)} f_k^{(s)} \delta_{s, k} \delta_{s, j} \right) \\
				& = \delta_{i, j} - f_i^{(s)} f_s^{(s)} \delta_{s, j}
		\end{aligned}
	\]
	But $f_s^{(s)} = 0$ which completes the proof.
\end{proof}

\begin{lemma}
	Let $G^{(s)} = I + f^{(s)} \otimes e^{(s)}$ as above. Then for every $s \in \{ 1, \dots, n - 1 \}$,
	\[
		G^{(1)} G^{(2)} \cdots G^{(s)} = I + \sum_{r = 1}^{s} f^{(r)} \otimes e^{(r)}
	\]
\end{lemma}

\begin{proof}
	Use induction on $s$.
	\begin{itemize}
		\item For $s = 1$, $G^{(1)} = I + f^{(1)} \otimes e^{(1)}$ trivially.
		\item Assume the statement is true for some $s$.
		\item For $s + 1$,
		\[
			\begin{aligned}
				{(G^{(1)} \cdots G^{(s)} G^{(s + 1)})}_{i, j}
					& = \sum_k {(G^{(1)} \cdots G^{(s)})}_{i, k} G_{k, j}^{(s + 1)} \\
					& = \left( \sum_k \left( \delta_{i, k} \sum_{r = 1}^{s} f_i^{(r)} \delta_{r, k} \right) \right) \left( \delta_{k, j} + f_k^{(s + 1)} \delta_{s + 1, j} \right) \\
					& = \delta_{i, j} + \sum_{r = 1}^{s} f_i^{(r)} \delta_{r, j} + f_i^{(s + 1)} \delta_{s + 1, j} \\
					& \quad + \sum_k \left( f_k^{(s + 1)} \delta_{s + 1, j} \sum_{r = 1}^{s} f_i^{(r)} \delta_{r, k} \right) \\
					& = \delta_{i, j} + \sum_{r = 1}^{s + 1} f_i^{(r)} \delta_{r, j}
			\end{aligned}
		\]
		which completes the induction.
	\end{itemize}
\end{proof}

\begin{theorem}
	Suppose that $A$ can be reduced to an upper-triangular matrix $U$ by Gaussian elimination without pivoting. During Gaussian elimination, let $A^{(k)}$ be the matrix during Gaussian elimination which has zeros below the diagonal in the first $k$ columns. Then there is a \textbf{unit} lower triangular matrix $L$ such that
	\[
		A = LU
	\]
	Also, the subdiagonal elements of $L$ are the coefficients used in the reduction of $A$:
	\[
		\forall i > j, L_{i, j} = {A^{(j - 1)}}_{i, j} / {A^{(j - 1)}}_{j, j}
	\]
\end{theorem}

\begin{proof}
	We write the Gaussian elimination process as
	\[
		\begin{aligned}
			U
				& = A^{(n - 1)} = F^{(n - 1)} A^{(n - 2)} \\
				& = F^{(n - 1)} F^{(n - 2)} A^{(n - 3)} \\
				& = F^{(n - 1)} F^{(n - 2)} \cdots F^{(1)} A^{(0)}
		\end{aligned}
	\]
	Now left-multiplying by $L = G^{(1)} \cdots G^{(n - 1)}$:
	\[
		LU = G^{(1)} \cdots G^{(n - 1)} F^{(n - 1)} F^{(n - 2)} \cdots F^{(1)} A^{(0)}
	\]
	But $G^{(k)} F^{(k)} = I$ by Lemma~\ref{lem:GFinverse} so this simplifies to $LU = A$.
\end{proof}

\begin{theorem}
	Every non-singular square matrix $A$ can be written as
	\[
		PA = LU
	\]
	where $L$ and $U$ are lower and upper triangular matrices and $P$ is a permutation matrix (exactly one non-zero element in each row and column).
\end{theorem}

\begin{proof}
	Too difficult.batyter
\end{proof}

\begin{remark}
	Once we have $L$ and $U$, we can solve $Ax = LUx = b$ with
	\begin{enumerate}
		\item Solve $Ly = b$ for $y$ using forward substitution.
		\item Solve $Ux = y$ for $x$ using backward substitution.
	\end{enumerate}
\end{remark}

\begin{example}
	$A = [[1, 2, 3, 6], [2, 8, 6, 5], [-4, -8, 0, 0], [0, 12, 9, -6]]$. Then $A^{(1)} = [[1, 2, 3, 6], [0, 4, 0, -7], [0, 0, 12, 24], [0, 12, 9, -6]]$, $A^{(2)} = [[1, 2, 3, 6], [0, 4, 0, -7], [0, 0, 12, 24], [0, 0, 9, 15]], A^{(3)} = [[1, 2, 3, 6], [0, 40, 0, -7], [0, 0, 12, 24], [0, 0, 0, -3]]$.

	Then $L = [[1, 0, 0, 0], [L_{2, 1}, 1, 0, 0], [L_{3, 1}, L_{3, 2}, 1, 0], [L_{4, 1}, L_{4, 2}, L_{4, 3}, 1]] = [[1, 0, 0, 0], [2, 1, 0, 0], [-4, 0, 1, 0], [0, 3, \frac{3}{4}, 1]]$.

	Then if $b = [1, 2, 4, 4]$, solving $Ly = b$ with forward substitution gives $y = [1, 0, 8, -2]$.

	Then solving $Ux = y$ for $x$ gives $x = [-\frac{10}{3}, \frac{7}{6}, -\frac{2}{3}, \frac{2}{3}]$.
\end{example}

\begin{remark}
	Comparing LU decomposition with Gaussian elimination and back substitution, LU decomposition involves less work when solving for different values of $b$ (same values of $A$). Gaussian elimination and back substitution is $O(n^3)$ for each value of $b$. LU decomposition is $O(n^3)$ for the first value of $b$, but then solving for different values of $b$ afterwards is $O(n^2)$.
\end{remark}

\begin{remark}
	LU decomposition is less prone to round off errors than computing $A^{-1}$ and is less computationally expensive for some values of $A$:
	\begin{itemize}
		\item When $A$ is tridiagonal, $L$ and $U$ are as well, but $A^{-1}$ generally isn't.
		\item When $A$ is sparse (most elements are $0$), $L$ and $U$ are also sparse, but $A^{-1}$ generally isn't.
	\end{itemize}
\end{remark}

\begin{proposition}
	If $L$ is an $n \times n$ lower triangular, invertible matrix, then $L^{-1}$ is also lower triangular.
\end{proposition}

\begin{proof}
	Let $M = L^{-1}$, then $LM = I$. Since $L^{-1}$ exists, $L_{i, i} \ne 0 \text{ and } M_{i, i} \ne 0 \ \forall 1 \le i \le n$.

	We have $I_{i, j} = \delta_{i, j}$ for every $i, j$. Then $0 = I_{1, j} = \delta_{1, j} = L_{1, 1} M_{1, j} \Rightarrow M_{1, j} = 0 \ \forall j > 1$ (since $L_{1, 1} \ne 0$).

	Now for row 2, $1 = \delta_{2, 2} = L_{2, 1} M_{1, 2} + L_{2, 2} M_{2, 2} + L_{2, 3} M_{3, 2} + \cdots$ but $L_{2, j} = 0 \forall j > 2$ since $L$ is lower triangular so $1 = L_{2, 1} M_{1, 2} + L_{2, 2} M_{2, 2}$. Similarly, $0 = \delta_{2, 3} = L_{2, 1} M_{1, 3} + L_{2, 2} M_{2, 3} + L_{2, 3} M_{3, 3} + \cdots = L_{2, 1} M_{1, 3} + L_{2, 2} M_{2, 3} = L_{2, 2} M_{2, 3} \Rightarrow M_{2, 3} = 0$. Generally,
	\[
		\delta_{2, j} = \sum_{k = 1}^{n} L_{2, k} M_{k, j} = L_{2, 1} M_{1, j} + L_{2, 2} M_{2, j} + L_{2, 3} M_{3, j} + \cdots = L_{2, 1} M_{1, j} + L_{2, 2} M_{2, j}
	\]
	But for $j > 2$, $M_{1, 2}, \dots, M_{1, n} = 0$. Since $L_{2, 2} \ne 0$, $M_{2, j} = 0 \ \forall j > 2$.

	This process continues for the rest of the rows by induction: assume that $M_{i, j} = 0 \ \forall i \in \{ 1, \dots, s - 1 \}, j > i$. Now we show that this holds for $i \in \{ 1, \dots, s \}$.
	\[
		\begin{aligned}
			\delta_{s, j}
				& = \sum_{k = 1}^{n} L_{s, k} M_{k, j} \\
				& = \sum_{k = 1}^{s} L_{s, k} M_{k, j}
		\end{aligned}
	\]
	For $j > s$,
	\[
		\begin{aligned}
			\delta_{s, j}
				& = \sum_{k = 1}^{s} L_{s, k} M_{k, j} \\
				& = L_{s, 1} M_{1, j} + \cdots + L_{s, s - 1} M_{s - 1, j} + L_{s, s} M_{s, j}
		\end{aligned}
	\]
	But by assumption, $L_{s, 1} M_{1, j} + \cdots + L_{s, s - 1} M_{s - 1, j} = 0$, so $\delta_{s, j} = L_{s, s} M_{s, j} = 0 \Rightarrow M_{s, j} = 0$ as $L_{s, s} \ne 0$.
\end{proof}

\subsection{Normed vector spaces}

\begin{definition}
	Given a vector space $V$, a \textbf{norm} $||.||: V \rightarrow \mathbb{R}_+$ satisfies the following properties:
	\begin{enumerate}
		\item $||x + y|| \le ||x|| + ||y|| \quad \forall x, y \in V$.
		\item $||c \cdot x|| = |c| ||x|| \quad \forall x \in V, c \in \mathbb{C}$.
		\item $||x|| = 0 \Longleftrightarrow x = 0$ (if this is not satisfied, the norm is called a \textbf{seminorm}).
	\end{enumerate}
\end{definition}

\begin{example}
	Some examples of norms:
	\begin{enumerate}
		\item The $L_2$ norm,
		\[
			{||x||}_2 := {\left( \sum_{k = 1}^{n} |x_k|^2 \right)}^{1/2}
		\]
		\item The $L_1$ norm,
		\[
			{||x||}_1 := \sum_{k = 1}^{n} |x_k|
		\]
		\item ${||x||}_{\infty} := \max_{k \in \{ 1, \dots, n \}} |x_k|$
		\item For $p \in [1, \infty)$,
		\[
			{||x||}_p := {\left( \sum_{k = 1}^{n} |x_k|^{p} \right)}^{1/p}
		\]
	\end{enumerate}
\end{example}

\begin{definition}
	Let $\tilde{V}$ be the set of $n \times n$ matrices over $\mathbb{R}$, then
	\[
		\begin{aligned}
			{||A||}_{\text{row}} & := \max_i \sum_{j = 1}^{n} |A_{i, j}| \\
			{||A||}_{\text{col}} & := \max_j \sum_{i = 1}^{n} |A_{i, j}| \\
			{||A||}_{\text{Frob}} & := {\left( \sum_{i, j} |A_{i, j}|^2 \right)}^{1/2}
		\end{aligned}
	\]
	are the \textbf{row-sum norm}, the \textbf{column-sum norm}, and the \textbf{Frobenius matrix norm}, respectively.
\end{definition}

\begin{definition}
	A matrix norm $||.||$ on $\tilde{V}$ is \textbf{submultiplicative} if
	\[
		\forall A, B \in \tilde{V}, \quad ||AB|| \le ||A|| \cdot ||B||
	\]
\end{definition}

\begin{definition}
	Given a vector norm $||.||_{\text{vec}}: V \rightarrow \mathbb{R}_+$, the \textbf{matrix norm} induced by $||.||_{\text{vec}}$ is defined as
	\[
		||A||_{\text{mat}} = \sup_{x \ne 0} \frac{||A x||_{\text{vec}}}{||x||_\text{vec}} = \max_{||x||_\text{vec} = 1} ||A x||_{\text{vec}}
	\]
\end{definition}

\begin{remark}
	Often, we omit $\text{vec}$ or $\text{mat}$ when the meaning of the norm is clear from context.
\end{remark}

\begin{example}	
	By linearity of vector norms,
	\[
		||A x||_{\star} \le ||A||_{\star} ||x||_{\star}
		\]
		where $|||A||_{\star}$ is the matrix norm induced by the vector norm $||.||_{\star}$.
\end{example}

\begin{example}
	Let $V = \mathbb{R}^2$ and $||.|| = ||.||_2$. Compute $||A||_2$ where
	\[
		A = \begin{pmatrix}
			3 & 1 \\
			-5 & -1
		\end{pmatrix}
	\]
	By definition,
	\[
		\begin{aligned}
			||A||_2
				& = \max_{||x||_2 = 1} ||A x||_2 \\
				& = \max_{\theta \in [0, 2 \pi]} ||A \cdot (\cos(\theta), \sin(\theta))||_2 \\
				& = \max_{\theta \in [0, 2 \pi]} || (3 \cos(\theta) + \sin(\theta), -5 \cos(\theta) - \sin(\theta)) ||_2
		\end{aligned}
	\]
	Let
	\[
		\begin{aligned}
			f(\theta) 
				& = {(3 \cos(\theta) + \sin(\theta))}^2 + {(5 \cos(\theta)) + \sin(\theta))}^2 \\
				& = 34 {\cos(\theta)}^2 + 2 {\sin(\theta)}^2 + 16 \cos(\theta) \sin(\theta) \\
				& = 16 \cos(2 \theta) + 8 \sin(2 \theta) + 18
		\end{aligned}
	\]
	so $f'(\theta) = -32 \sin(2 \theta) + 16 \cos(2 \theta)$, then $f'(\theta) = 0 \Rightarrow \tan(2 \theta) = 1/2$ which gives
	\[
		\max_{\theta} f(\theta) = 18 + \frac{40}{\sqrt{5}}
	\]
	which gives
	\[
		||A||_2 = \sqrt{18 + \frac{40}{\sqrt{5}}}
	\]
\end{example}

\begin{definition}
	A matrix $A$ is \textbf{normal} if $A A^T = A^T A$.
\end{definition}

\begin{theorem}
	If a matrix $A$ is normal, then $A$ is diagonalisable.
\end{theorem}

\begin{proof}
	Too long.
\end{proof}

\begin{theorem}
	If $A$ is symmetric, then its eigenvalues are real, and
	\[
		||A||_2 = \max \{ |\lambda|: \lambda \text{ is an eigenvalue of } A \}
	\]
	For non-symmetric $A$,
	\[
		||A||_2 = \max \{ |\lambda|: \lambda \text{ is an eigenvalue of } A A^T \}
	\]
\end{theorem}

\begin{proof}
	Too long.
\end{proof}

\begin{proposition}
	The vector $1$-norm induces the matrix column-sum norm.
\end{proposition}

\begin{proof}
	$||x||_1 = \sum_i |x_i|$ and $||A||_{\text{col}} = \max_j \sum_i |A_{i, j}|$.
	\[
		\begin{aligned}
			||A x||_1
				& = \sum_i \left| \sum_j A_{i, j} \right| \\
				& \le \sum_i \sum_j |A_{i, j} x_j| \\
				& = \sum_i \sum_j |A_{i, j}| |x_j| = \sum_j \left( \sum_i |A_{i, j}| \right) |x_j| \\
				& \le \left( \max_j \sum_i |A_{i, j}| \right) \sum_j |x_j| = ||A||_{\text{col}} ||x||_1
		\end{aligned}
	\]
	Hence,
	\[
		||A||_1 \le ||A||_\text{col}
	\]
	and also
	\[
		\begin{aligned}
			||A||_1
				& = \max_{||x||_1 = 1} ||A x||_1 \\
				& \ge \max_k ||A e^{(k)} ||
		\end{aligned}
	\]
	where $e_j^{(k)} = \delta_{j, k}$. So
	\[
		\begin{aligned}
			||A||_1
				& \ge \max_k \sum_i \left| \sum_j A_{i, j} \delta_{j, k} \right| \\
				& = \max_k \sum_i |A_{i, k}| = ||A||_\text{col}
		\end{aligned}
	\]
	Hence $||A||_1 = ||A||_\text{col}$.
\end{proof}

\begin{example}
	Let
	\[
		A = \begin{bmatrix}
			3 & 1 & -4 \\
			1 & -5 & 9 \\
			-2 & 6 & 5
		\end{bmatrix}
	\]
	so $||A||_{\text{col}} = \max \{ 3 + 1 + 2, 1 + 5 + 6, 4 + 9 + 5 \} = 18$. The max is attained at column $3$. Let $\underline{x} = (0, 0, 1)$ then $A \underline{x} = (-4, 9, 5)$ and $||A \underline{x}||_1 = 4 + 9 + 5 = 18$.
\end{example}

\begin{proposition}
	The vector $\infty$-norm induces the matrix row-sum norm.
\end{proposition}

\begin{proof}
	We want to show that
	\[
		\max_{||x||_{\infty} = 1} ||Ax||_{\infty} = \max_i \sum_j |A_{i, j}|
	\]
	First, we show $||A||_{\infty} \le ||A||_{\text{row}}$ for every $A$ and $x$:
	\[
		\begin{aligned}
			||Ax||_{\infty}
				& = \max_i \left| \sum_j A_{i, j} x_j \right| \\
				& \le \max_i \sum_j |A_{i, j}| |x_j| \\
				& \le \max_i \left\{ \max_k |x_k| \sum_j |A_{i, j}| \right\} \\
				& = \max_k |x_k| \max_i \sum_j |A_{i, j}| \\
				& = ||x||_{\infty} ||A||_{\text{row}}
		\end{aligned}
	\]
	Assuming that $x \ne 0$, we divide by $||x||_{\infty}$ and take the maximum:
	\[
		||A||_{\infty} = \max_{||x||_{\infty}} \frac{||Ax||_{\infty}}{||x||_{\infty}} \le \max_{||x||_{\infty} \ne 0} ||A||_{\text{row}} = ||A||_{\text{row}}
	\]
	To show equality, for every $A$, we need to find an $x$ that gives equality in the inequalities above. By linearity, we can take $||x||_{\infty} = 1$. We want to find an $x$ such that
	\[
		\max_k \left| \sum_j A_{k, j} x_j \right| = \max_i \sum_j |A_{i, j}|
	\]
	Let $i^*$ be an $i$ that realises the maximum. Then let
	\[
		x_j = \text{sgn} (A_{i^*, j})
	\]
	Let $k = i^*$, then
	\[
		\sum_j A_{i^*, j} x_j = \sum_j A_{i^*, j} \cdot \text{sgn} (A_{i^*, j}) = \sum_j |A_{i^*, j}| = \max_i \sum_j |A_{i, j}|
	\]
\end{proof}

\begin{remark}
	\[
		||A|| = \sup_{y \ne 0} \frac{||Ay||}{||x||} \quad \forall x \ne 0
	\]
	Therefore
	\[
		||Ax|| \le ||A|| \cdot ||x|| \quad \forall x
	\]
\end{remark}

\begin{example}
	(Problems class) Show the equivalence of the induced norm definitions.
	\[
		\begin{aligned}
			\sup_{x \ne 0} \frac{||Ax||}{||x||}
				& = \sup_{x \ne 0} \frac{||Ax|| / ||x||}{||x|| / ||x||} \\
				& = \sup_{x \ne 0} \frac{||A (x / ||x||)||}{||(x / ||x||)||} \\
				& = \sup_{x \ne 0} ||A(x / ||x||)|| \\
				& = \max_{||y|| = 1} ||Ay||
		\end{aligned}
	\]
\end{example}

\begin{example}\label{exa:normOfProductLessThanProductOfNorms}
	(Problems class) Show that $||AB|| \le ||A|| \cdot ||B||$ for all matrices $A, B$ and every induced norm $||\cdot||$.
	\[
		\begin{aligned}
			\sup_{x \ne 0, Bx \ne 0} \frac{||ABx||}{||x||}
				& = \sup_{x \ne 0, Bx \ne 0} \frac{||ABx||}{||Bx||} \frac{||Bx||}{||x||} \\
				& \le \sup_{Bx \ne 0} \frac{||ABx||}{||Bx||} \cdot \sup_{x \ne 0} \frac{||Bx||}{||x||} \\
				& = ||A|| \cdot ||B||
		\end{aligned}
	\]
	When $Bx = 0$,
	\[
		0 = ||ABx|| = ||A|| \cdot ||Bx|| = 0
	\]
	so we can remove the condition $Bx \ne 0$ on the $\sup$s in the equation above.
\end{example}

\subsection{Errors and condition numbers}

In the case where we want to solve $Ax = b$ for $x$, but with some floating po0int error in $b$, we have $b + \delta b$ instead of $b$, where $\delta b$ is unknown but bounded (small). So we have
\[
	A (x + \delta x) = b + \delta b
\]
It is not always the case that $\delta x$ is also small. We want to find bounds for $\delta x$ in terms of $\delta b$. We have that $A \delta x = \delta b$. Assuming $A^{-1}$ exists, $\delta x = A^{-1} \delta b$. Using any norm $||\cdot||_*$,
\[
	||\delta x||_* = ||A^{-1} \delta b||_* \le ||A^{-1}||_* ||\delta b||_*
\]
Assuming $x \ne 0$, divide by $||x||_*$:
\[
	\frac{||\delta x||_*}{||x||_*} \le ||A^{-1}|| \frac{||\delta b||_*}{||x||_*}
\]
Now since $Ax = b$,
\[
	||b||_* = ||A x||_* \le ||A||_* ||x||_* \Longrightarrow \frac{1}{||x||_*} \le ||A||_* \frac{1}{||b||_*}
\]
Therefore,
\[
	\frac{||\delta x||_*}{||x||_*} \le ||A||_* ||A^{-1}||_* \frac{||\delta b||_*}{||b||_*}
\]
This means the relative error in $x$ is less than a constant multiplied by the relative error in $b$.

\begin{definition}
	For any norm $||\cdot||_*$, we define
	\[
		\kappa_* (A) := ||A||_* ||A^{-1}||_*
	\]
	which is called the \textbf{*-condition} number of the matrix $A$.
\end{definition}

\begin{example}
	(Problems class) Prove that $\kappa(A) \ge 1$ for every non-singular matrix $A$ and for every induced matrix norm.
	\[
		1 = ||I|| = ||A A^{-1}|| \le ||A|| \cdot ||A^{-1}||
	\]
	by Example~\ref{exa:normOfProductLessThanProductOfNorms}.
\end{example}

\begin{example}
	(Problems class) Prove that
	\[
		||x||_p = {\left( \sum_i |x_i|^p \right)}^{1 / p}
	\]
	is a norm for every $p \in [1, \infty)$.
	\[
		\begin{aligned}
			{(||x + y||_p)}^p
				& = \sum_i |x_i + y_i|^p \\
				& = \sum_i |x_i + y_i| \cdot |x_i + y_i|^{p - 1} \\
				& \le \sum_i (|x_i| + |y_i|) |x_i + y_i|^{p - 1} \\
				& = \sum_i |x_i| \cdot |x_i + y_i|^{p - 1} + \cdots \\
				& \le {\left( \sum_i |x_i|^r \right)}^{1 / r} {\left( \sum_j |x_j + y_j|^{q (p - 1)} \right)}^{1 / q} + \cdots
		\end{aligned}
	\]
	by the Holder inequality (TODO: define this inequality). Let $r = p, q = 1/(1 - 1/p) = p / (p - 1)$. Then
	\[
		\begin{aligned}
			{(||x + y||_p)}^p
				& \le {\left( \sum_i |x_i|^p \right)}^{1 / p} {\left( \sum_j |x_j + y_j|^{(p - 1) p / (p - 1)} \right)}^{(p - 1) / p} + \cdots \\
				& = ||x||_p {(||x + y||)}_p^{p - 1} + ||y||_p {(||x + y||)}_p^{p - 1}
		\end{aligned}
	\]
\end{example}

\subsection{$L^2$ norm, eigenvalues and diagonalisability}

\begin{theorem}\label{thm:smallEigenValuesEquivalentToZeroConvergence}
	Let $\{ \lambda_j \}$ be the eigenvalues of a matrix $A$. Then
	\[
		\max \{ |\lambda_j| \} < 1 \Longleftrightarrow A^k \rightarrow 0 \quad \text{ as } k \rightarrow \infty
	\]
\end{theorem}

\begin{example}
	Let
	\[
		A = \begin{bmatrix}
			0.99 & 10^6 \\
			0 & 0.99
		\end{bmatrix}
		\Longrightarrow
		A^{128} = \begin{bmatrix}
			0.2765\dots & 3.751 \cdot 10^6 \\
			0 & 0.2765\dots
		\end{bmatrix}
	\]
	This does not seem to agree with Theorem~\ref{thm:smallEigenValuesEquivalentToZeroConvergence}, however the theorem is correct, as products and powers of matrices are more complicated than those of complex numbers. For example, for every $z \in \mathbb{C}$, $|z|^k = |z^k|$ but generally $||A^k|| \ne ||A||^k$ for a matrix $A$ and a norm $||\cdot||$. $||A||^k$ could grow before it then tends to zero. This occurs especially when $A$ is not diagonalisable ($A$ is defective, meaning that its eigenvectors do not span $\mathbb{R}^n$).

	In fact, Theorem~\ref{thm:smallEigenValuesEquivalentToZeroConvergence} is easy to prove for diagonalisable $A$, but difficult for defective $A$.
\end{example}

\begin{definition}
	The \textbf{spectral radius} of a square matrix $A$ is defined as
	\[
		\rho(A) := \max \{ |\lambda_j| \}
	\]
	where the $\lambda_j$ are the eigenvalues of $A$.
\end{definition}

\begin{proposition}\label{prop:symmetricMatrixProperties}
	If $A$ is symmetric (or hermitian), then
	\begin{itemize}
		\item All its eigenvalues are real, so $B v_i = \lambda_i v_i$ where $\lambda_i \in \mathbb{R}$.
		\item Its eigenvectors $v_i$ form a basis of $\mathbb{R}^n$: every vector $x \in \mathbb{R}^n$ can be written as a linear combination of the eigenvectors of $A$, i.e. $x = \sum_i c_i v_i$ where $v_i$ are the eigenvectors of $A$ and $c_i$ are real numbers.
		\item Its eigenvectors can be chosen to be orthonormal, i.e. $v_i \cdot v_j = \delta_{ij}$.
		\item Parseval's identity holds:
		\[
			{(||x||_2)}^2 = \left( \sum_i c_i v_i, \sum_j c_j v_j \right) = \sum_{i, j} c_i c_j (v_i, v_j) = \sum_i c_i^2
		\]
	\end{itemize}
\end{proposition}

\begin{definition}
	A square matrix $A$ is \textbf{normal} if it commutes with its transpose:
	\[
		A A^T = A^T A
	\]
	($A A^* = A^* A$ for complex matrices).
\end{definition}

\begin{definition}
	A matrix $U$ is unitary if
	\[
		U^* = U^{-1}
	\]
\end{definition}

\begin{definition}
	A matrix $A$ is unitarily diagonalisable if $A = U D U^*$ for some unitary matrix $U$ and diagonal matrix $D$.
\end{definition}

\begin{theorem}
	(\textbf{The spectral theorem}) A matrix $A$ is unitarily diagonalisable if and only if it is normal.
\end{theorem}

\begin{example}
	Let $a \ne 1$ and
	\[
		A = \begin{bmatrix}
			1 & a \\
			0 & 1
		\end{bmatrix}
	\]
	is not normal, as
	\[
		A A^T = \begin{bmatrix}
			1 + a^2 & a \\
			a & 1
		\end{bmatrix}
		\ne
		A^T A = \begin{bmatrix}
			1 & a \\
			a & 1 + a^2
		\end{bmatrix}
	\]
	$\lambda = 1$ is the only eigenvalue, with eigenvector $(1, 0)$. $A$ is not unitarily diagonalisable.
\end{example}

\begin{theorem}
	The induced $2$-norm of a matrix $A$ is
	\[
		\sqrt{\rho(A^T A)} = \max \{ \sqrt{|\lambda|}: \lambda \text{ is an eigenvalue of } A^T A \}
	\]
\end{theorem}

\begin{proof}
	For every matrix $A$, $A^T A$ is symmetric, so by Proposition~\ref{prop:symmetricMatrixProperties} its eigenvalues are real and its eigenvectors, $\{ v_i \}$ for $i \in \{ 1, \dots, n \}$, are orthonormal and span $\mathbb{R}^n$. So for every $x \in \mathbb{R}^n$,
	\[
		x = \sum_i c_i v_i
	\]
	for some $c_i \in \mathbb{R}$, $A^T A v_i = c_i v_i$ and $(v_i, v_j) = \delta_{i, j}$.
	\[
		\begin{aligned}
			{(||Ax||_2)}^2
				& = (Ax, Ax) = (A^T A x, x) = \left( A^T A \sum_i c_i v_i, \sum_j c_j v_j \right) \\
				& = \left( \sum_i \lambda_i c_i v_i, \sum_k c_j v_j \right) \\
				& = \sum_{i, j} \lambda_i c_i c_j (v_i, v_j) \\
				& = \sum_i \lambda_i c_i^2
		\end{aligned}
	\]
	We claim that $\lambda_j \ge 0 \ \forall j$. Indeed, if $\lambda_s < 0$, let $x = v_s$, then
	\[
		{(||A v_s||_2)}^2 = \lambda_s < 0
	\]
	Write $0 \le \lambda_1 \le \lambda_2 \le \cdots \le \lambda_n$ (counting multiplicities). Then
	\[
		{(||Ax||_2)}^2 = \sum_i \lambda_i c_i^2 \le \lambda_n \sum_i c_i^2 = \lambda_n {(||x||_2)}^2
	\]
	which gives
	\[
		||A||_2 = \sup_{x \ne 0} \frac{||Ax||_2}{||x||_2} \le \sqrt{\lambda_n}
	\]
	Taking $x = v_n$ gives ${(||A x_n||_2)}^2 = \lambda_n {(||x_n||_2)}^2$, which gives equality in the above inequality.
\end{proof}

\begin{remark}
	$\rho(A^T A) = \rho(A A^T)$.
\end{remark}

\begin{example}
	Let
	\[
		A = \begin{bmatrix}
			1 & a \\
			0 & 1
		\end{bmatrix}
		\Longrightarrow
		A^T A = \begin{bmatrix}
			1 & 0 \\
			0 & 1 + a^2
		\end{bmatrix}
	\]
	The eigenvalues of $A^T A$ are given by $0 = (1 - \lambda) (1 + a^2 - \lambda) - a^2$, which gives $\lambda_1 \approx 2$ and $\lambda_2 \approx 2 a^2$ for $|a| >> 1$.
\end{example}

\end{document}