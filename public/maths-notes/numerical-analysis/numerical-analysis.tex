\input{../header.tex}

\title{Numerical Analysis Course Notes}
\author{Isaac Holt}

\begin{document}

\input{../titletoc.tex}

\section{Chebyshev Polynomials}

\begin{theorem}
let $w_n(x) = (x - x_0) \dots (x - x_n)$ with distinct nodes ${x_0, \dots, x_n}$, $x_j \subset [-1, 1]$. Then the maximum of $|w_n(x)|$ on $[-1, 1]$ attains its smallest value ($2^{-n}$) iff ${x_j}$ are the zeros of $T_{n + 1}(x)$.
\end{theorem}

\begin{proof}
	($\Longleftarrow$): By construction $2^{-n} T_{n+1}(x)$ is a monic polynomial (highest power of $x$ is 1) with $n + 1$ roots in $[-1, 1]$. Suppose $S_{n+1}(x) = (x - z_0)\dots(x - z_n)$ is another monic polynomial such that $\max |S_{n+1}(x)| < 2^{-n} = \max |2^{-n} T_{n+1}(x)|$. Let $q_n(x) := 2^{-n} T_{n+1}(x) - S_{n+1}(x)$. Then $q_n(x) \in P_n$ since the coefficient of $x^{n+1}$ in $T_{n+1}(x)$ and $S_{n+1}(x)$ are both $1$ and so cancel out.

	Then $q_n(y_j) = 2^{-n} T_{n+1}(y_j) - S_{n+1}(y_j)$ ($y_j$ are the extrema of $T_n(x)$). $|S_{n+1}(y_j)| < 1$ by hypothesis. Therefore $q_n(y_j) > 0$ if $j$ is odd and $< 0$ otherwise.

	Since we have $n+2$ of $y_j$, $q_n$ has at least $n+1$ zeros. But since $q_n \in P_n$, we must have $q_n(x)=0$. Therefore $S_{n+1}(x) = 2^{-n} T_{n+1}(x)$. 
\end{proof}

\begin{remark}
	To use this in $[a, b]$ instead of $[-1, 1]$, one simply maps $x_j \rightarrow a + (x_j + 1) \frac{b - a}{2}$.
\end{remark}

\begin{remark}
	Putting the above into Cauchy's error formula,
	we have \[\sup |f(x)-p(x)| \le 2^{-n} {(\frac{b - a}{2})}^{n + 1} \frac{1}{(n + 1)!}\]
\end{remark}

\begin{remark}
	We have by the above theorem, $\max |w_n(x)| = \max |(x - x_0)\dots(x - x_n) \ge 2^{-n}$ for any choice of ${x_1, \dots, x_n}$, $x_j \subset [-1, 1]$. So $2^{-n}$ is a lower bound for $|w_n(x)|$.

	The upper bound is given by $\max |w_n(x)| \le \epsilon |b-a|^n$. 
\end{remark}

\maketitle
\section{Root Finding}

\subsection{Bracketing: Bisection}

Given $f \in C^0 ([a, b])$ with $f(a) f(b) < 0$, repeat:
\begin{itemize}
	\item let $(a_0, b_0) = (a, b)$
	\item let $m_n = \frac{1}{2} (a_n + b_n)$
	\item if $f(m_n)f(a_n) \ge 0$, set $(a_{n + 1}, b_{n + 1}) = (m_n, b_n)$
	\item otherwise, set $(a_{n + 1}, b_{n + 1}) = (a_n, m_n)$
\end{itemize}

$b_{n + 1} - a_{n + 1} = \frac{1}{2} (b_n - a_n)$. By the Intermediate Value Theorem, if $f(m_n) \ne 0$, for some $p \in (a_n, b_n)$, $f(p) = 0$.

$|p - m_n| \le 2^{-(n + 1)} (b - a)$

\begin{remark}
	Each time, the width of the interval halves. In principle, we could get an approximation to any desired accuracy, but there are some caveats (e.g. with floating points).
\end{remark}

\subsection{Bracketing: False Position}

Suppose we have $|f(b)| \ll |f(a)|$, then we would expect $p$ to be closer to $b$ than to $a$. Instead of $m_n = \frac{1}{2} (a_n + b_n)$, set

\[m_n = b_n - f(b_n) \frac{b_n - a_n}{f(b_n) - f(a_n)}\]

i.e. $m_n$ is the x-intercept of the line from $(a_n, f(a_n))$ to $(b_n, f(b_n))$. This should sometimes give much faster approximation than bisection, but not always.

\subsection{Aside: Continuity and Convergence}

\begin{definition}
	$f: I \rightarrow \mathbb{R}$ is continuous at $x \in I$ if for every $\epsilon > 0$, for some $\delta(x, \epsilon)$, $|y - x| < \delta \Rightarrow |f(x) - f(y)| < \epsilon$ for every $y \in B(x)$ ($B(x)$ is an open interval containing $x$).
\end{definition}

\begin{remark}
	In general, $\delta$ depends on $\epsilon$ and $x$. When $\delta$ is independent of $x$, $f$ is uniformly continuous.
\end{remark}

\begin{definition}
	$f: I \rightarrow \mathbb{R}$ is Lipschitz continuous in $I$ if for some $L > 0$, $|f(y) - f(x)| \le L|y - x|$ for every $x \in I, y \in I$. In this case, $\delta = \epsilon / L$.
\end{definition}

\begin{remark}
	$L$ (like $\delta$ above) is not unique. The smallest such $L$ is called the Lipschitz constant of $f$ in $I$.
\end{remark}

\begin{lemma}
	\hfill
	\begin{enumerate}
		\item If $f$ is differentiable and $I$ is compact, $f$ is Lipschitz in $I$.
		\item If $f$ is Lipschitz, $f$ is continuous.
	\end{enumerate}
\end{lemma}

\begin{proof}
	\[f(y) - f(x) = \int_x^y f'(x) ds\]
	\[|f(y) - f(x)| = |\int_x^y f'(x) ds| \le \int_x^y |f'(x)| ds\]
	\[\le \max_{s \in I} |f'(s)| \int_x^y ds = \max_{s \in I} |f'(s)| |y - x|\]
	We can take $L = \max_{s \in I} |f'(s)|$
\end{proof}

\begin{remark}
	The converses of 1. and 2. are false.
\end{remark}

\begin{remark}
	\begin{itemize}
		\item When $f$ is continuous in $I$, we write $f \in C^0 (I)$.
		\item When $f$ is differentiable in $I$, we write $f \in C^1 (I)$.
		\item When $f$ is Lipschitz in $I$, we write $f \in C^{0, 1} (I)$.
		\item We can then write $C^1 (I) \subsetneq C^{0, 1} (I) \subsetneq C^0 (I)$.
	\end{itemize}
\end{remark}

\begin{definition}
	A sequence $(x_n)$ in $\mathbb{R}^d$ converges to $x$ if for every $\epsilon > 0$, for some $N(\epsilon)$, for every $n \ge N(\epsilon)$, $|x_n - x| < \epsilon$.

	This relies on us knowing $x$ in the first place.
\end{definition}

\begin{definition}
	A sequence $(x_n)$ is a Cauchy sequence if for every $\epsilon > 0$, for some $N(\epsilon)$, for every $m \ge N, n \ge N$, $|x_n - x_m| < \epsilon$.
\end{definition}


\begin{theorem}
	Let $(x_n)$ be a Cauchy sequence in $\mathbb{R}^d$. Then $(x_n)$ converges.
\end{theorem}

This is useful as it allows us to prove convergence without knowing $x$.

\subsection{Fixed Point Iterations}

We seek $x$ such that $f(x) = 0$ for a function $f$. We rewrite this as \[x = g(x)\]

We then seek to solve this equation by iterations:

\begin{enumerate}
	\item pick some $x_0$
	\item set $x_{n + 1} = g(x_n)$
\end{enumerate}

\begin{theorem}
	(1d local convergence theorem): Let $g \in C'([a, b])$ have a fixed point $x_{\star} \in [a, b]$ ($g(x_{\star}) = x_{\star}$) with $|g'(x_{\star}) < 1$. Then for $x_0$ sufficiently close to $x_{\star}$, the iteeration $x_{n + 1} = g(x_n)$ converges to $x_{\star}$.
\end{theorem}

\begin{proof}
	Let $g'(x_{\star}) = L \in (0, 1)$ ($g'(x_{\star}) < 0$ is analogous). Since $g'$ is continuous at $x_{\star}$, for every $L' \in (L, 1)$, for some $\delta(L') > 0$, $g'(x) \le L' < 1$ for every $x \in (x_{\star} - \delta, x_{\star} + \delta) = B_{\delta}$, therefore for every $x \in B_{\delta}$, $y \in B_{\delta}$, $|g(x) - g(y)| \le \sup_{s \in B_{\delta}} |g'(s)| |x - y| = L' |x - y|$ with $L' < 1$.

	Let $x_{\star} \in B_{\delta}$, then $|g(x) - x_{\star}| = |g(x) - g(x_{\star})| \le L' |x - x_{\star}|$ since $x_{\star} = g(x_{\star})$. So $x - x_{\star} \delta$ as $x \in B_{\delta}$, so $|g(x) - x_{\star}| \le L'\delta \le \delta$, therefore $g(B_\delta) \subseteq B_{\delta}$.
\end{proof}

\begin{remark}
	We do not need to know $x_{\star}$ to apply the 1d local convergence theorem, we just need to know that $|g'(x)| < 1$ for every $x \in I$ for some interval $I$.
\end{remark}

\subsection{Order of convergence}

Order of convergence is a rough measure of how quickly $x_n \rightarrow x$. We mainly look at sequences arising from iterations with a nice RHS (so not bisection).

\begin{definition}
	Let $x_n \rightarrow x_*$ and assume that $x_n \ne x_*$ for every $n \ge 0$. $x_n \rightarrow x_*$ with order at least $\alpha > 1$ if

	\[\lim_{n \rightarrow \infty} \frac{|x_{n + 1} - x_*|}{|x_n - x_*|^{\alpha}} = \lambda < todo\]

	and with order $\alpha = 1$ if also $\lambda < 1$.
\end{definition}

\begin{example}
	$x_n = n^{-\beta}$, $\beta > 0$

	\[\frac{|x_{n + 1} - x_*|}{|x_n - x_*|} = {(\frac{n}{n + 1})}^{\beta} \rightarrow 1\]
\end{example}

\begin{example}
	$x_n = e^{-n}$

	\[\frac{|x_{n + 1} - x_*|}{|x_n - x_*|} = 1/e < 1\]
\end{example}

\begin{example}
	$x_n = \frac{1}{n!}$

	\[\frac{|x_{n + 1} - x_*|}{|x_n - x_*|} = 1/(n + 1) \rightarrow 0\]
\end{example}

The order of convergence of $x_n \rightarrow x_*$ is

\[\alpha = \sup \{\beta: \lim_{n \rightarrow \infty} \frac{|x_{n + 1} - x_*|}{|x_n - x_*|^{\beta}} < todo\}\]

for $\alpha > 1$.

For $\alpha = 1$, we also require that the limit $< 1$.

The convergence is linear if $\alpha = 1$, superlinear if $\alpha > 1$ and sublinear otherwise.

\begin{remark}
	Order of convergence need not be an integer.
\end{remark}

\begin{remark}
	We need to know $x_*$ in order to determine order of convergence.
\end{remark}

\begin{remark}
	Our definition is not comprehensive for general sequences.
\end{remark}

Applying this to iterations:

$x_{n + 1} - x_* = g(x_n) - g(x_*) = (x_n - x_*) g'(c_n)$ for some $c \in \text{conv}\{x_n, x_*\}$ by the Mean Value Theorem.

Therefore

\[{|x_{n + 1} - x_*|}{|x_n - x_*|} = |g'(c_n)| \rightarrow g'(x_*)\]

We conclude that for $g \in C^2 (I)$, the iteration $x_{n + 1} = g(x_n)$ converges linearly if $g'(x_*) \ne 0$ and $|g'(x_*)| < 1$, and superlinearly otherwise.

\begin{proposition}
	Let $g \in C^{N + 1} (D)$ for some $D \subseteq \mathbb{R}$ and let $g(x_*) = x_*$, with $x_*$ in the interior of $D$.

	Then the iteration $x_{n + 1} = g(x_n)$ converges to $x_*$ for $x_0$ sufficiently close to $x_*$ with order $N + 1$ iff $g'(x_*) = g''(x_*) = \cdots = g^{(N)} (x_*) = 0$ and $g^{(N + 1)}(x_*) \ne 0$.
\end{proposition}

\begin{proof}
	$x_{n + 1} - x_* = g(x_n) - g(x_*) = g(x_*) + (x_n - x_*)g'(x_*) + \cdots + \frac{{(x_n - x_*)}^N}{N!} g^{(N)}(x_*) + \frac{{(x_n - x_*)}^{N + 1}}{(N + 1)!} g^{(N + 1)}(c_n) - g(x_*) = \frac{{(x_n - x_*)}^{N + 1}}{(N + 1)!} g^{(N + 1)}(c_n)$. Thus

	\[{|x_{n + 1} - x_*|}{|x_n - x_*|^{N + 1}} = \frac{|g^{(N + 1)}(c_n)|}{(N + 1)!} \rightarrow \frac{|g^{(N + 1)}(x_*)|}{(N + 1)!} < todo\]
\end{proof}

\subsection{Higher order iterative methods}

We want to rearrange $f(x) = 0$ to get faster convergence.

$x_{n + 1} = g(x_n) = x_n + \phi (x_n) f(x_n)$ for some $\phi$.

Using the above proposition, we need $g'(x_*) = 0$.

$g'(x_*) = 1 + \phi'(x_*)f(x_*) + \phi(x_*)f'(x_*) = 0$

So if $f'(x_*) \ne 0$, we take $\phi(x) = -\frac{1}{f'(x)}$.

This is the Newton-Raphson method:

$x_{n + 1} = x_n - \frac{f(x_n)}{f'(x_n)}$.

Multiplication of a computer is parallelisable while division is not. So we can use Newton-Raphson to divide numbers with multiplication.

To compute $x_* = 1 / b$ so $f(x_*) = \frac{1}{x_*} - b = 0$, so using Newton-Raphson, $x_{n + 1} = x_n - \frac{{x_n}^{-1} - b}{-{x_n}^{-2}} = x_n (2 - b x_n)$ which involes only multiplication and subtraction. When taking out the exponent, $x_0 \in \left[ \frac{1}{2}, 1 \right)$, and this converges quickly.

\begin{remark}
	This only works with floating points, not integers. Floating point division is 5 times faster than integer division.
\end{remark}

\begin{remark}
	It can be difficult in practice to determine the interval/domain of convergence for Newton-Raphson. We should always perform a ``sanity check'' when using it.
\end{remark}

\begin{example}
	One advantage of iterative methods is that they also work (in principle) in higher dimensions.

	Suppose $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ has a root at $\underline{p} = (p_1, p_2)$. Using Taylor expansion at the current point $\underline{x_n}$, derive Newton-Raphson (2D):

	\[\underline{x_{n + 1}} = \underline{x_n} - {(Df)}^{-1} \underline{f}(\underline{x_n})\]

	We write $\underline{x_*} = \underline{p}$ such that $f_1(p_1, p_2) = f_2(p_1, p_2) = 0$. Taylor-expanding at $\underline{x}$:

	\[0 = f_1(p_1, p_2) = f_1(x_1, x_2) + (p_1 - x_1) \diffp{f_1}{x_1} (x_1, x_2) + (p_2 - x_2) \diffp{f_1}{x_2} (x_1, x_2) + O(|\underline{p} - \underline{x}|^2)\]

	\[0 = f_2(p_1, p_2) = f_2(x_1, x_2) + (p_1 - x_1) \diffp{f_2}{x_1} (x_1, x_2) + (p_2 - x_2) \diffp{f_2}{x_2} (x_1, x_2) + O(|\underline{p} - \underline{x}|^2)\]

	In matrix form:

	\[(0, 0) = (f_1(\underline{x}), f_2(\underline{x})) = (Df)(x_1, x_2) \cdot (x_1 - p_1, x_2 - p_2) + O(|\underline{p} - \underline{x}|^2)\]

	Assuming that $Df$ is invertible (equivalently, $f'(\underline{x}) \ne 0$), we can multiply the equation by ${(Df)}^{-1}$ to get

	\[ (p_1, p_2) = (x_1, x_2) - (({(Df)}^{-1})(x_1, x_2)) \cdot (f_1(\underline{x}), f_2(\underline{x})) + O(|\underline{p} - \underline{x}|^2)\] So
	
	\[\underline{p} = \underline{x} - (({(Df)}^{-1})(x_1, x_2)) \underline{f}(\underline{x}) + O(|\underline{p} - \underline{x}|^2) \] We can use this to construct our iteration by replacing $\underline{x}$ with $\underline{x_n}$ and $\underline{p}$ with $\underline{x_{n + 1}}$, and removing the $O(|\underline{p} - \underline{x}|^2)$.
\end{example}

\subsection{Secant method}

One disadvantage of Newton-Raphson is that we need the derivative, $f'$. If $f$ is complicated or is itself computed numerically, we need to approximate $f'$. An alternative method is the secant method.

The secant method approximates $f'$ with:

\[ f'(x_n) \approx \frac{f(x_n) - f(x_{n - 1})}{x_n - x_{n - 1}} \]
so the iteration becomes

\[ x_{n + 1} = x_n - \frac{x_n - x_{n - 1}}{f(x_n) - f(x_{n - 1})} f(x_n) \]

\begin{remark}
	This is a (scalar) two-step method: $x_{n + 1} = g(x_n, x_{n - 1})$, where $x_n$ and $x_{n - 1}$ are needed to calculate $x_{n + 1}$.
\end{remark}

\begin{theorem}
	Let $f \in C^2$ with $f(x_*) = 0$ and $f'(x_*) \ne 0$. Then the secant method is convergent with order $\alpha = \frac{1 + \sqrt{5}}{2}$ for every $x_0 \ne x_1$ sufficiently close to $x_*$.
\end{theorem}

\begin{proof}
	TODO: See video on Panopto
\end{proof}

\begin{remark}
	\hfill
	\begin{enumerate}
		\item When implementing the secant method, one must be careful with floating point effects:

		\[ \frac{x_n - x_{n - 1}}{f(x_n) - f(x_{n - 1})} \]
		becomes very inaccurate as $x_n - x_{n - 1} \rightarrow 0$.
		\item $e_n := x_n - x_*$ alternates in sign if $g'(x_k) < 0$ and has the same sign if $g'(x_*) > 0$. From the proof of the method,
		
		\[ e_{n + 1} = e_n e_{n - 1} \frac{f''(\theta_n)}{f'(\phi_n)} \]
		where $\theta_n, \phi_n \in \text{conv} \{x_{n - 1}, x_n, x_{n + 1} \}$.

		For $e_0 e_1 < 0$ and $n$ sufficiently large, the error $e_n$ follows the pattern $+,+,-$ or $-,-,+$.
	\end{enumerate}
\end{remark}

\clearpage
\section{Numerical differentiation}

\subsection{Forward/backward difference}

$f'(x) = \lim_{h \rightarrow 0} \frac{f(x + h) - f(x)}{h} \approx \frac{f(x + h) - f(x)}{h}$ for some small $h \ne 0$.

More rigorously, we can use Taylor series:

\[ f(x + h) = f(x) + h f'(x) + \frac{h^2}{2} f''(\xi) \]
where $\xi \in \text{conv} \{x, x + h \}$. So

\[ f'(x) = \frac{f(x + h) - f(x)}{h} - \frac{h}{2} f''(\xi) \]
The error $\frac{h}{2} f''(\xi)$ is of order $O(h)$.

For $h > 0$ this is called forward difference.

For $h < 0$, this is called backward difference.

\subsection{Centred difference}

\[ f(x \pm h) = f(x) \pm h f'(x) + \frac{h^2}{2} f''(x) \pm \frac{h^3}{6} f^{(3)}(\xi_{\pm}) \]
Then

\[ f'(x) = \frac{f(x + h) - f(x - h)}{2h} + \frac{h^2}{12} \left( f^{(3)}(\xi_+) + f^{(3)}(\xi_-) \right) \]
The error $\frac{h^2}{12} \left( f^{(3)}(\xi_+) + f^{(3)}(\xi_-) \right)$ is of order $O(h^2)$.

\begin{remark}
	It is important to \textbf{not take $h$ too small} when computing numerically. There is always a tradeoff between formal analytical accuracy and floating point errors. Formal analytical accuracy is better for smaller $|h|$, floating point errors are worse for smaller $|h|$.
\end{remark}

\begin{remark}
	Sometimes, we can only take $h < 0$ or $h > 0$ (not both), e.g. when solving differential equations numerically. If we have an ODE

	\[ \diff{u}{t} = F(u) \]
	Given $u(0)$ we want to solve for $u(t), t \ge 0$. We approximate $u(t)$ by a $u(t_n)$ with $t_n = n \delta t, n \in \mathbb{N}$, with $\delta t > 0$ small. Then

	\[ \diff{u}{t} \approx \frac{u(t_n + \delta t) - u(t_n)}{\delta t} \approx F(u(t_n)) \]
\end{remark}

\subsection{Richardson extrapolation}

Let $f'(x) - \frac{f(x + h) - f(x)}{h} = f'(x) - R_h^{(1)} (x) = c_1(x) h + c_2(x) h^2 + c_3(x) h^3 + \cdots$ but suppose we cannot compute the $c_k$.

$R_{h / 2}^{(1)} (x) = f'(x) - c_1 \frac{h}{2} - c_2 \frac{h^2}{4} - c_3 \frac{h^3}{6} - \cdots$. We can use this to eleminate $c_1$:

\[ 2 R_{h / 2}^{(1)} (x) - R_h^{(1)} (x) = f'(x) - c_2' h^2 - c_3' h^3 - \cdots \]
So the error is of order $O(h^2)$. Then

\[ f'(x) = R_h^{(2)} (x) + O(h^2) \]
where $R_h^{(2)} = 2 R_{h / 2}^{(1)} (x) - R_h^{(1)} (x)$.

Now, $R_{h / 2}^{(2)} (x) = f'(x) - c_2' \frac{h^2}{4} - c_3' \frac{h^3}{8}$ and we use this to eliminate $c_2'$:

\[ 4 R_{h / 2}^{(2)} (x) - R_h^{(2)} (x) = 3 f'(x) + O(h^3) \]

So we set

\[ R_h^{(3)} := \frac{2^2 R_{h / 2}^{(2)} (x) - R_h^{(2)}(x)}{2^2 - 1} = f'(x) + O(h^3) \]

\begin{remark}
	\hfill
	\begin{enumerate}
		\item We only need to specify the powers of $h$, not the coefficients $c_k$ as long as they are \textbf{non-zero}. So when using centred difference to calculate $R_h^{(1)}$ then $R_h^{(2)}$, $R_h^{(3)}$ will be different.
		\item Richardson extrapolation works with many other approximation methods involving a small parameter (not just differentiation).
		\item There is no standard notation for this method.
		\item Some series expansions have irregular/non-integer powers, e.g. the Airy function $\text{Ai}(x)$ which is a solution of $\diff[2]{f}{x} = x f(x)$.
	\end{enumerate}
\end{remark}

\section{Linear systems}

Given $A \in M_n(\mathbb{R})$ and $\underline{x} \in \mathbb{R}^n$, we want to solve for $\underline{x}$:
\[
	A \underline{x} = \underline{b}
\]
We want to minimise the error when computing this with floating points, and minimise the amount of computation for: large $n$, for different $\underline{b}$ (with the same $A$) and when $A$ has particular forms.

\begin{definition}
	The \textbf{transpose} of a square matrix $A$, $A^T$ is defined as
	\[
		{(A^T)}_{i, j} = A_{j, i}
	\]
\end{definition}

\begin{definition}
	$A$ is \textbf{symmetric} if $A = A^T$.
\end{definition}

\begin{definition}
	$A$ is \textbf{skew-symmetric} if $A = -A^T$.
\end{definition}

\begin{definition}
	$A$ is \textbf{non-singular} if for every $\underline{b}$, for some $\underline{x}$, $A\underline{x} = \underline{b}$.
\end{definition}

\begin{definition}
	$A$ is \textbf{positive definite} if $(A \underline{x}) \cdot \underline{x} > 0 \quad \underline{x} \ne \underline{0}$.
\end{definition}

\begin{definition}
	$A$ is \textbf{positive semi-definite} if $(A \underline{x}) \cdot \underline{x} \ge 0 \quad \forall x$.
\end{definition}

\begin{lemma}
	If $A$ is positive definite, then $A$ is non-singular.
\end{lemma}

\begin{proof}
	Omitted.
\end{proof}

\begin{lemma}
	The converse of the above lemma is false.
\end{lemma}

\begin{proof}
	$A$ is non-singular $\Rightarrow$ $-A$ is non-singular, but $A$ is positive-definite $\Rightarrow$ $-A$ is negative definite.
\end{proof}

\begin{definition}
	A matrix $A$ is \textbf{lower triangular} if $A_{i, j} = 0 \quad \forall j > i$.
\end{definition}

\begin{definition}
	$A$ is \textbf{upper triangular} if $A_{i, j} = 0 \quad \forall j < i$.
\end{definition}

\begin{remark}
	We often write $U$ for an upper triangular matrix and $L$ for a lower triangular matrix.
\end{remark}

\begin{definition}
	To solve $Ux = b$, we can use \textbf{backward substitution}. Starting from the last equation,
	\[
		\begin{aligned}
			x_n & = \frac{1}{U_{n, n}} b_n \\
			x_{n - 1} & = \frac{1}{U_{n - 1, n - 1}} (b_{n - 1} - U_{n - 1, n} x_n) \\
			\vdots & \\
			x_j & = \frac{1}{U_j} \left( b_j - \sum_{i = j + 1}^{n} U_{j, i} x_i \right)
		\end{aligned}
	\]
\end{definition}

\begin{definition}
	Similarly, we can solve $L \underline{x} = \underline{b}$ for a lower triangular matrix $L$ with forward substitution. Starting from the first equation,
	\[
		\begin{aligned}
			x_1 & = \frac{b_1}{L_{1, 1}} \\
			\vdots & \\
			x_j & = \frac{1}{L_{j, j}} \left( b_j - \sum_{i = 1}^{j - 1} L_{j, i} x_k \right)
		\end{aligned}
	\]
\end{definition}

\begin{remark}
	If $U_{j, j} = 0$ or $L_{j, j} = 0$ for some $j$, then this method doesn't work. This is expected, because in this case $\det U = 0$ (or $\det L = 0$).
\end{remark}

\begin{definition}
	If $A$ is neither upper nor lower triangular, then we can transform $A$ into a (usually) upper triangular matrix, by \textbf{Gaussian elimination}.

	The following operations leave the solution $\underline{x}$ unchanged:
	\begin{enumerate}
		\item Swapping two rows
		\item Adding a scalar multiple of a row to another row.
	\end{enumerate}
\end{definition}

\subsection{Computational complexity}

\begin{definition}
	For simplicity, we assume that each elementary floating point operation takes 1 unit of time, called 1 \textbf{flop}.
\end{definition}

\begin{remark}
	In practice, binary64 multiplication takes roughly 3 times as long as addition or multiplication, and binary64 division takes roughly 10 times as long as addition or subtraction.
\end{remark}

\begin{definition}
	Let $f(n) > 0$ and $g(n) > 0$ for large $n$. We write
	\[
		f(n) \sim o(g(n)) \quad \text{ if } \lim_{n \rightarrow \infty} \frac{f(n)}{g(n)} = 0
	\]
\end{definition}

\begin{definition}
	Let $f(n) > 0$ and $g(n) > 0$ for large $n$. We write
	\[
		f(n) \sim O(g(n)) \quad \text{ if } \limsup_{n \rightarrow \infty} \frac{f(n)}{g(n)} < \infty
	\]
	Equivalently,
	\[
		f(n) \sim O(g(n)) \quad \text{ if } \exists C, \exists N, \forall n \le N, f(n) \le C g(n)
	\]
\end{definition}

\begin{remark}
	From these definitions, we have
	\[
		f(n) \sim o(g(n)) \Longrightarrow f(n) \sim O(g(n))
	\]
\end{remark}

\begin{example}
	\hfill
	\begin{itemize}
		\item $100n^3 + 10^6 n^2 \sim O(n^3)$
		\item $n! + 10^{100} n^{100} \sim O(n!)$
		\item $n! \sim O(n^{n + \frac{1}{2}} e^{-n})$
	\end{itemize}
\end{example}

\begin{remark}
	In this module, we will be calculate the computational complexity to \textbf{leading order}, e.g. we distinguish $3n^2 + 5n$ form $30n^2$ but not from $3n^2 - 2n$.
\end{remark}

\begin{proposition}
	Backwards substitution on $U$ where $U$ is an $n \times n$ matrix is an $O(n^2)$ operation.
\end{proposition}

\begin{proof}
	\hfill
	\begin{itemize}
		\item Computing $x_n = b_n / U_{n, n}$ takes 1 flop.
		\item Computing $x_{n - 1} = \frac{(b_{n - 1} - U_{n - 1, n} x_n)}{U_{n - 1, n - 1}}$ takes 3 flops.
		\item $\vdots$
		\item Computing $x_1 = \frac{1}{U_1} \left( b_1 - \sum_{i = j + 2}^{n} U_{1, i} x_i \right)$ takes $2n - 1$ flops.
	\end{itemize}
	So in total, backward substitution takes $1 + 3 + \cdots + 2n - 1 = n^2$ flops.
\end{proof}

\begin{proposition}
	The number of flops needed for solving $Ax = b$ where $A$ is an $n \times n$ matrix is
	\[
		\frac{2}{3} n^3	+ \frac{3}{2} n^2 - \frac{7}{6} n
	\]
\end{proposition}

\begin{proof}
	To zero the first column for rows $i \in \{ 2, \dots, n \}$:
	\begin{itemize}
		\item $\alpha_{i, 1} = -A_{i, 1} / A_{1, 1}$ ($1$ flop).
		\item $A_{i, j} \rightarrow A_{i, j} + \alpha_{i, 1} A_{1, j}$ for $j \in \{ 2, \dots, n \}$ and $A_{i, 1} = 0$ ($2(n - 1)$ flops).
		\item $b_i \rightarrow b_i + \alpha{i, 1} b_1$ ($2$ flops).
	\end{itemize}
	This is $2n + 1$ flops in total for each row, and this is done for $n - 1$ rows, so in total there are $(2n + 1)(n - 1)$ flops. For row $2$, there are $(2n - 1)(n - 2)$ flops.

	In general, to zero column $k$ for rows $i \in \{ k + 1, \dots, n \}$:
	\begin{itemize}
		\item $\alpha_{i, k} = -A_{i, k} / A_{k, k}$ ($1$ flop).
		\item $A_{i, j} \rightarrow A_{i, j} + \alpha_{i, k} A_{k, j}$ for $j \in \{ k + 1, \dots, n \}$ ($2(n - k)$ flops).
		\item $b_i \rightarrow b_i + \alpha_{i, k} b_k$ ($2$ flops).
	\end{itemize}
	This is $2n - 2k + 3$ flops in total for each row, so for the $n - k$ rows, in total there are $(2n - 2k + 3)(n - k)$ flops.

	So the total number of flops for Gaussian elimination is
	\[
		\begin{aligned}
			\sum_{k = 1}^{n - 1} (2n - 2k + 3)(n - k)
				& = \sum_{k = 1}^{n - 1} 2n^2 - 2kn + 3n - 2nk + 2k^2 - 3k \\
				& = (n - 1)(2n^2 + 3n) - (4n + 3) \sum_{k = 1}^{n - 1} k + 2 \sum_{k = 1}^{n - 1} k^2 \\
				& = (n - 1)(2n^2 + 3n) - (4n + 3) \frac{n(n - 1)}{2} \\
				&\quad + \frac{1}{3} (n - 1) n (2n - 1) \\
				& = \frac{2}{3} n^3 + \frac{1}{2} n^2 - \frac{7}{6} n
		\end{aligned}
	\]
	Adding the $n^2$ flops from back substitution, in total to solve $Ax = b$, the number of flops needed is
	\[
		\frac{2}{3} n^3	+ \frac{3}{2} n^2 - \frac{7}{6} n
	\]
\end{proof}

\begin{remark}
	Gaussian elimination with (row) pivotting can transfomr every non-singular matrix into an upper-triangular matrix.
\end{remark}

\subsection{Pivoting and roundoffs}

\begin{definition}
	To reduce round-off errors, we define \textbf{row/partial pivoting}:
	\begin{itemize}
		\item When zeroing column $k$, look for $A_{j, k}$ with $j \in \{ k, \dots, n \}$ with the largest absolute value, and swap row $j$ with row $k$.
	\end{itemize}
\end{definition}

\begin{remark}
	Multiplying a pivot row by a large constant does not improve reduction of round-off errors. The \textbf{ratio} of the pivot element to the other elements in the row is more important. 
\end{remark}

\begin{remark}
	For better results, we can also perform column/full pivoting (so we swap rows and columns), but this is much more complicated.
\end{remark}

\subsection{LU decomposition}

\begin{lemma}
	Steps of the Gaussian elimination process can be written as matrix multiplication. During Gaussian elimination, let $A^{(k)}$ be the matrix during Gaussian elimination which has zeros below the diagonal in the first $k$ columns.
	\[
		A^{(s)} = F^{(s)} A^{(s - 1)}
	\]
	where
	\[
		\begin{aligned}
			F_{i, j}^(s) & = \delta_{i, j} - f_i^{(s)} e_j^{(s)} \\
			f_i^{(s)} & = (0, \dots, 0, A_{s+1, s}^{s-1} / A_{s,s}^{(s - 1)}, \dots, A_{n, s}^{(s - 1)} / A_{s, s}^{(s - 1)}) \\
			e_j^{(s)} = \delta_{s, j}
		\end{aligned}
	\]
\end{lemma}

\begin{proof}
	To zero column $s$ below the leading diagonal, we do
	\[
		A_{i, j}^{(s)} = A_{i, j}^{(s - 1)} - \frac{A_{i, s}^{(s - 1)}}{A_{s, s}^{(s - 1)}} A_{s, j}^{(s - 1)}
	\]
	for $i \in \{ s + 1, \dots, n \}$ and $j \in \{ s, \dots, n \}$.
	\[
		\begin{aligned}
			A_{i, j}^{(s)}
				& = \sum_k \left( \delta_{i, k} A_{k, j}^{(s - 1)} - \frac{A_{i, s}^{(s - 1)}}{A_{s, s}^{(s - 1)}} \delta_{s, k} A_{k, j}^{(s - 1)} \right) \\
				& = \sum_k \left( \delta_{i, k} - \frac{A_{i, s}^{(s - 1)}}{A_{s, s}^{(s - 1)}} \cdot \delta_{s, k} \right) A_{k, j}^{(s - 1)}
		\end{aligned}
	\]
\end{proof}

\begin{definition}
	For vectors $u$ and $v$, the \textbf{tensor product} of $u$ and $v$ is a matrix defined by
	\[
		{(u \otimes v)}_{i, j} = u_i v_j
	\]
\end{definition}

\begin{definition}
	A \textbf{Frobenius matrix} of index $s$ is a unit lower triangular matrix whose only (possibly) non-zero elements are subdiagonal elements in column $s$ (other than the leading diagonal, which is all $1$s).
\end{definition}

\begin{lemma}\label{lem:GFinverse}
	Let $F^{(s)} = I - f^{(s)} \otimes e^{(s)}$. Then its inverse is
	\[
		G^{(s)} = I + f^{(s)} \otimes e^{(s)}
	\]
\end{lemma}

\begin{proof}
	\[
		\begin{aligned}
			\sum_k F_{i, k}^{(s)} G_{k, j}^{(s)}
				& = \sum_k \left( \delta_{i, k} - f_i^{(s)} e_k^{(s)} \right) \left( \delta_{k, j} + f_k^{(s)} e_j^{(s)} \right) \\
				& = \sum_k \left( \delta_{i, k} - f_i^{(s)} \delta_{s, k} \right) \left( \delta_{k, j} + f_k^{(s)} \delta_{s, j} \right) \\
				& = \sum_k \left( \delta_{i, j} + f_i^{(s)} \delta_{s, j} - f_i^{(s)} \delta_{s, j} - f_i^{(s)} f_k^{(s)} \delta_{s, k} \delta_{s, j} \right) \\
				& = \delta_{i, j} - f_i^{(s)} f_s^{(s)} \delta_{s, j}
		\end{aligned}
	\]
	But $f_s^{(s)} = 0$ which completes the proof.
\end{proof}

\begin{lemma}
	Let $G^{(s)} = I + f^{(s)} \otimes e^{(s)}$ as above. Then for every $s \in \{ 1, \dots, n - 1 \}$,
	\[
		G^{(1)} G^{(2)} \cdots G^{(s)} = I + \sum_{r = 1}^{s} f^{(r)} \otimes e^{(r)}
	\]
\end{lemma}

\begin{proof}
	Use induction on $s$.
	\begin{itemize}
		\item For $s = 1$, $G^{(1)} = I + f^{(1)} \otimes e^{(1)}$ trivially.
		\item Assume the statement is true for some $s$.
		\item For $s + 1$,
		\[
			\begin{aligned}
				{(G^{(1)} \cdots G^{(s)} G^{(s + 1)})}_{i, j}
					& = \sum_k {(G^{(1)} \cdots G^{(s)})}_{i, k} G_{k, j}^{(s + 1)} \\
					& = \left( \sum_k \left( \delta_{i, k} \sum_{r = 1}^{s} f_i^{(r)} \delta_{r, k} \right) \right) \left( \delta_{k, j} + f_k^{(s + 1)} \delta_{s + 1, j} \right) \\
					& = \delta_{i, j} + \sum_{r = 1}^{s} f_i^{(r)} \delta_{r, j} + f_i^{(s + 1)} \delta_{s + 1, j} \\
					& \quad + \sum_k \left( f_k^{(s + 1)} \delta_{s + 1, j} \sum_{r = 1}^{s} f_i^{(r)} \delta_{r, k} \right) \\
					& = \delta_{i, j} + \sum_{r = 1}^{s + 1} f_i^{(r)} \delta_{r, j}
			\end{aligned}
		\]
		which completes the induction.
	\end{itemize}
\end{proof}

\begin{theorem}
	Suppose that $A$ can be reduced to an upper-triangular matrix $U$ by Gaussian elimination without pivoting. During Gaussian elimination, let $A^{(k)}$ be the matrix during Gaussian elimination which has zeros below the diagonal in the first $k$ columns. Then there is a \textbf{unit} lower triangular matrix $L$ such that
	\[
		A = LU
	\]
	Also, the subdiagonal elements of $L$ are the coefficients used in the reduction of $A$:
	\[
		\forall i > j, L_{i, j} = {A^{(j - 1)}}_{i, j} / {A^{(j - 1)}}_{j, j}
	\]
\end{theorem}

\begin{proof}
	We write the Gaussian elimination process as
	\[
		\begin{aligned}
			U
				& = A^{(n - 1)} = F^{(n - 1)} A^{(n - 2)} \\
				& = F^{(n - 1)} F^{(n - 2)} A^{(n - 3)} \\
				& = F^{(n - 1)} F^{(n - 2)} \cdots F^{(1)} A^{(0)}
		\end{aligned}
	\]
	Now left-multiplying by $L = G^{(1)} \cdots G^{(n - 1)}$:
	\[
		LU = G^{(1)} \cdots G^{(n - 1)} F^{(n - 1)} F^{(n - 2)} \cdots F^{(1)} A^{(0)}
	\]
	But $G^{(k)} F^{(k)} = I$ by Lemma~\ref{lem:GFinverse} so this simplifies to $LU = A$.
\end{proof}

\begin{theorem}
	Every non-singular square matrix $A$ can be written as
	\[
		PA = LU
	\]
	where $L$ and $U$ are lower and upper triangular matrices and $P$ is a permutation matrix (exactly one non-zero element in each row and column).
\end{theorem}

\begin{proof}
	Too difficult.batyter
\end{proof}

\begin{remark}
	Once we have $L$ and $U$, we can solve $Ax = LUx = b$ with
	\begin{enumerate}
		\item Solve $Ly = b$ for $y$ using forward substitution.
		\item Solve $Ux = y$ for $x$ using backward substitution.
	\end{enumerate}
\end{remark}

\begin{example}
	$A = [[1, 2, 3, 6], [2, 8, 6, 5], [-4, -8, 0, 0], [0, 12, 9, -6]]$. Then $A^{(1)} = [[1, 2, 3, 6], [0, 4, 0, -7], [0, 0, 12, 24], [0, 12, 9, -6]]$, $A^{(2)} = [[1, 2, 3, 6], [0, 4, 0, -7], [0, 0, 12, 24], [0, 0, 9, 15]], A^{(3)} = [[1, 2, 3, 6], [0, 40, 0, -7], [0, 0, 12, 24], [0, 0, 0, -3]]$.

	Then $L = [[1, 0, 0, 0], [L_{2, 1}, 1, 0, 0], [L_{3, 1}, L_{3, 2}, 1, 0], [L_{4, 1}, L_{4, 2}, L_{4, 3}, 1]] = [[1, 0, 0, 0], [2, 1, 0, 0], [-4, 0, 1, 0], [0, 3, \frac{3}{4}, 1]]$.

	Then if $b = [1, 2, 4, 4]$, solving $Ly = b$ with forward substitution gives $y = [1, 0, 8, -2]$.

	Then solving $Ux = y$ for $x$ gives $x = [-\frac{10}{3}, \frac{7}{6}, -\frac{2}{3}, \frac{2}{3}]$.
\end{example}

\begin{remark}
	Comparing LU decomposition with Gaussian elimination and back substitution, LU decomposition involves less work when solving for different values of $b$ (same values of $A$). Gaussian elimination and back substitution is $O(n^3)$ for each value of $b$. LU decomposition is $O(n^3)$ for the first value of $b$, but then solving for different values of $b$ afterwards is $O(n^2)$.
\end{remark}

\begin{remark}
	LU decomposition is less prone to round off errors than computing $A^{-1}$ and is less computationally expensive for some values of $A$:
	\begin{itemize}
		\item When $A$ is tridiagonal, $L$ and $U$ are as well, but $A^{-1}$ generally isn't.
		\item When $A$ is sparse (most elements are $0$), $L$ and $U$ are also sparse, but $A^{-1}$ generally isn't.
	\end{itemize}
\end{remark}

\begin{proposition}
	If $L$ is an $n \times n$ lower triangular, invertible matrix, then $L^{-1}$ is also lower triangular.
\end{proposition}

\begin{proof}
	Let $M = L^{-1}$, then $LM = I$. Since $L^{-1}$ exists, $L_{i, i} \ne 0 \text{ and } M_{i, i} \ne 0 \ \forall 1 \le i \le n$.

	We have $I_{i, j} = \delta_{i, j}$ for every $i, j$. Then $0 = I_{1, j} = \delta_{1, j} = L_{1, 1} M_{1, j} \Rightarrow M_{1, j} = 0 \ \forall j > 1$ (since $L_{1, 1} \ne 0$).

	Now for row 2, $1 = \delta_{2, 2} = L_{2, 1} M_{1, 2} + L_{2, 2} M_{2, 2} + L_{2, 3} M_{3, 2} + \cdots$ but $L_{2, j} = 0 \forall j > 2$ since $L$ is lower triangular so $1 = L_{2, 1} M_{1, 2} + L_{2, 2} M_{2, 2}$. Similarly, $0 = \delta_{2, 3} = L_{2, 1} M_{1, 3} + L_{2, 2} M_{2, 3} + L_{2, 3} M_{3, 3} + \cdots = L_{2, 1} M_{1, 3} + L_{2, 2} M_{2, 3} = L_{2, 2} M_{2, 3} \Rightarrow M_{2, 3} = 0$. Generally,
	\[
		\delta_{2, j} = \sum_{k = 1}^{n} L_{2, k} M_{k, j} = L_{2, 1} M_{1, j} + L_{2, 2} M_{2, j} + L_{2, 3} M_{3, j} + \cdots = L_{2, 1} M_{1, j} + L_{2, 2} M_{2, j}
	\]
	But for $j > 2$, $M_{1, 2}, \dots, M_{1, n} = 0$. Since $L_{2, 2} \ne 0$, $M_{2, j} = 0 \ \forall j > 2$.

	This process continues for the rest of the rows by induction: assume that $M_{i, j} = 0 \ \forall i \in \{ 1, \dots, s - 1 \}, j > i$. Now we show that this holds for $i \in \{ 1, \dots, s \}$.
	\[
		\begin{aligned}
			\delta_{s, j}
				& = \sum_{k = 1}^{n} L_{s, k} M_{k, j} \\
				& = \sum_{k = 1}^{s} L_{s, k} M_{k, j}
		\end{aligned}
	\]
	For $j > s$,
	\[
		\begin{aligned}
			\delta_{s, j}
				& = \sum_{k = 1}^{s} L_{s, k} M_{k, j} \\
				& = L_{s, 1} M_{1, j} + \cdots + L_{s, s - 1} M_{s - 1, j} + L_{s, s} M_{s, j}
		\end{aligned}
	\]
	But by assumption, $L_{s, 1} M_{1, j} + \cdots + L_{s, s - 1} M_{s - 1, j} = 0$, so $\delta_{s, j} = L_{s, s} M_{s, j} = 0 \Rightarrow M_{s, j} = 0$ as $L_{s, s} \ne 0$.
\end{proof}

\subsection{Normed vector spaces}

\begin{definition}
	Given a vector space $V$, a \textbf{norm} $||.||: V \rightarrow \mathbb{R}_+$ satisfies the following properties:
	\begin{enumerate}
		\item $||x + y|| \le ||x|| + ||y|| \quad \forall x, y \in V$.
		\item $||c \cdot x|| = |c| ||x|| \quad \forall x \in V, c \in \mathbb{C}$.
		\item $||x|| = 0 \Longleftrightarrow x = 0$ (if this is not satisfied, the norm is called a \textbf{seminorm}).
	\end{enumerate}
\end{definition}

\begin{example}
	Some examples of norms:
	\begin{enumerate}
		\item The $L_2$ norm,
		\[
			{||x||}_2 := {\left( \sum_{k = 1}^{n} |x_k|^2 \right)}^{1/2}
		\]
		\item The $L_1$ norm,
		\[
			{||x||}_1 := \sum_{k = 1}^{n} |x_k|
		\]
		\item ${||x||}_{\infty} := \max_{k \in \{ 1, \dots, n \}} |x_k|$
		\item For $p \in [1, \infty)$,
		\[
			{||x||}_p := {\left( \sum_{k = 1}^{n} |x_k|^{p} \right)}^{1/p}
		\]
	\end{enumerate}
\end{example}

\begin{definition}
	Let $\tilde{V}$ be the set of $n \times n$ matrices over $\mathbb{R}$, then
	\[
		\begin{aligned}
			{||A||}_{\text{row}} & := \max_i \sum_{j = 1}^{n} |A_{i, j}| \\
			{||A||}_{\text{col}} & := \max_j \sum_{i = 1}^{n} |A_{i, j}| \\
			{||A||}_{\text{Frob}} & := {\left( \sum_{i, j} |A_{i, j}|^2 \right)}^{1/2}
		\end{aligned}
	\]
	are the \textbf{row-sum norm}, the \textbf{column-sum norm}, and the \textbf{Frobenius matrix norm}, respectively.
\end{definition}

\begin{definition}
	A matrix norm $||.||$ on $\tilde{V}$ is \textbf{submultiplicative} if
	\[
		\forall A, B \in \tilde{V}, \quad ||AB|| \le ||A|| \cdot ||B||
	\]
\end{definition}

\begin{definition}
	Given a vector norm $||.||_{\text{vec}}: V \rightarrow \mathbb{R}_+$, the \textbf{matrix norm} induced by $||.||_{\text{vec}}$ is defined as
	\[
		||A||_{\text{mat}} = \sup_{x \ne 0} \frac{||A x||_{\text{vec}}}{||x||_\text{vec}} = \max_{||x||_\text{vec} = 1} ||A x||_{\text{vec}}
	\]
\end{definition}

\begin{remark}
	Often, we omit $\text{vec}$ or $\text{mat}$ when the meaning of the norm is clear from context.
\end{remark}

\begin{example}	
	By linearity of vector norms,
	\[
		||A x||_{\star} \le ||A||_{\star} ||x||_{\star}
		\]
		where $|||A||_{\star}$ is the matrix norm induced by the vector norm $||.||_{\star}$.
\end{example}

\begin{example}
	Let $V = \mathbb{R}^2$ and $||.|| = ||.||_2$. Compute $||A||_2$ where
	\[
		A = \begin{pmatrix}
			3 & 1 \\
			-5 & -1
		\end{pmatrix}
	\]
	By definition,
	\[
		\begin{aligned}
			||A||_2
				& = \max_{||x||_2 = 1} ||A x||_2 \\
				& = \max_{\theta \in [0, 2 \pi]} ||A \cdot (\cos(\theta), \sin(\theta))||_2 \\
				& = \max_{\theta \in [0, 2 \pi]} || (3 \cos(\theta) + \sin(\theta), -5 \cos(\theta) - \sin(\theta)) ||_2
		\end{aligned}
	\]
	Let
	\[
		\begin{aligned}
			f(\theta) 
				& = {(3 \cos(\theta) + \sin(\theta))}^2 + {(5 \cos(\theta)) + \sin(\theta))}^2 \\
				& = 34 {\cos(\theta)}^2 + 2 {\sin(\theta)}^2 + 16 \cos(\theta) \sin(\theta) \\
				& = 16 \cos(2 \theta) + 8 \sin(2 \theta) + 18
		\end{aligned}
	\]
	so $f'(\theta) = -32 \sin(2 \theta) + 16 \cos(2 \theta)$, then $f'(\theta) = 0 \Rightarrow \tan(2 \theta) = 1/2$ which gives
	\[
		\max_{\theta} f(\theta) = 18 + \frac{40}{\sqrt{5}}
	\]
	which gives
	\[
		||A||_2 = \sqrt{18 + \frac{40}{\sqrt{5}}}
	\]
\end{example}

\begin{definition}
	A matrix $A$ is \textbf{normal} if $A A^T = A^T A$.
\end{definition}

\begin{theorem}
	If a matrix $A$ is normal, then $A$ is diagonalisable.
\end{theorem}

\begin{proof}
	Too long.
\end{proof}

\begin{theorem}
	If $A$ is symmetric, then its eigenvalues are real, and
	\[
		||A||_2 = \max \{ |\lambda|: \lambda \text{ is an eigenvalue of } A \}
	\]
	For non-symmetric $A$,
	\[
		||A||_2 = \max \{ |\lambda|: \lambda \text{ is an eigenvalue of } A A^T \}
	\]
\end{theorem}

\begin{proof}
	Too long.
\end{proof}

\begin{proposition}
	The vector $1$-norm induces the matrix column-sum norm.
\end{proposition}

\begin{proof}
	$||x||_1 = \sum_i |x_i|$ and $||A||_{\text{col}} = \max_j \sum_i |A_{i, j}|$.
	\[
		\begin{aligned}
			||A x||_1
				& = \sum_i \left| \sum_j A_{i, j} \right| \\
				& \le \sum_i \sum_j |A_{i, j} x_j| \\
				& = \sum_i \sum_j |A_{i, j}| |x_j| = \sum_j \left( \sum_i |A_{i, j}| \right) |x_j| \\
				& \le \left( \max_j \sum_i |A_{i, j}| \right) \sum_j |x_j| = ||A||_{\text{col}} ||x||_1
		\end{aligned}
	\]
	Hence,
	\[
		||A||_1 \le ||A||_\text{col}
	\]
	and also
	\[
		\begin{aligned}
			||A||_1
				& = \max_{||x||_1 = 1} ||A x||_1 \\
				& \ge \max_k ||A e^{(k)} ||
		\end{aligned}
	\]
	where $e_j^{(k)} = \delta_{j, k}$. So
	\[
		\begin{aligned}
			||A||_1
				& \ge \max_k \sum_i \left| \sum_j A_{i, j} \delta_{j, k} \right| \\
				& = \max_k \sum_i |A_{i, k}| = ||A||_\text{col}
		\end{aligned}
	\]
	Hence $||A||_1 = ||A||_\text{col}$.
\end{proof}

\begin{example}
	Let
	\[
		A = \begin{bmatrix}
			3 & 1 & -4 \\
			1 & -5 & 9 \\
			-2 & 6 & 5
		\end{bmatrix}
	\]
	so $||A||_{\text{col}} = \max \{ 3 + 1 + 2, 1 + 5 + 6, 4 + 9 + 5 \} = 18$. The max is attained at column $3$. Let $\underline{x} = (0, 0, 1)$ then $A \underline{x} = (-4, 9, 5)$ and $||A \underline{x}||_1 = 4 + 9 + 5 = 18$.
\end{example}

\begin{proposition}
	The vector $\infty$-norm induces the matrix row-sum norm.
\end{proposition}

\begin{proof}
	We want to show that
	\[
		\max_{||x||_{\infty} = 1} ||Ax||_{\infty} = \max_i \sum_j |A_{i, j}|
	\]
	First, we show $||A||_{\infty} \le ||A||_{\text{row}}$ for every $A$ and $x$:
	\[
		\begin{aligned}
			||Ax||_{\infty}
				& = \max_i \left| \sum_j A_{i, j} x_j \right| \\
				& \le \max_i \sum_j |A_{i, j}| |x_j| \\
				& \le \max_i \left\{ \max_k |x_k| \sum_j |A_{i, j}| \right\} \\
				& = \max_k |x_k| \max_i \sum_j |A_{i, j}| \\
				& = ||x||_{\infty} ||A||_{\text{row}}
		\end{aligned}
	\]
	Assuming that $x \ne 0$, we divide by $||x||_{\infty}$ and take the maximum:
	\[
		||A||_{\infty} = \max_{||x||_{\infty}} \frac{||Ax||_{\infty}}{||x||_{\infty}} \le \max_{||x||_{\infty} \ne 0} ||A||_{\text{row}} = ||A||_{\text{row}}
	\]
	To show equality, for every $A$, we need to find an $x$ that gives equality in the inequalities above. By linearity, we can take $||x||_{\infty} = 1$. We want to find an $x$ such that
	\[
		\max_k \left| \sum_j A_{k, j} x_j \right| = \max_i \sum_j |A_{i, j}|
	\]
	Let $i^*$ be an $i$ that realises the maximum. Then let
	\[
		x_j = \sgn (A_{i^*, j})
	\]
	Let $k = i^*$, then
	\[
		\sum_j A_{i^*, j} x_j = \sum_j A_{i^*, j} \cdot \sgn (A_{i^*, j}) = \sum_j |A_{i^*, j}| = \max_i \sum_j |A_{i, j}|
	\]
\end{proof}

\begin{remark}
	\[
		||A|| = \sup_{y \ne 0} \frac{||Ay||}{||x||} \quad \forall x \ne 0
	\]
	Therefore
	\[
		||Ax|| \le ||A|| \cdot ||x|| \quad \forall x
	\]
\end{remark}

\begin{example}
	(Problems class) Show the equivalence of the induced norm definitions.
	\[
		\begin{aligned}
			\sup_{x \ne 0} \frac{||Ax||}{||x||}
				& = \sup_{x \ne 0} \frac{||Ax|| / ||x||}{||x|| / ||x||} \\
				& = \sup_{x \ne 0} \frac{||A (x / ||x||)||}{||(x / ||x||)||} \\
				& = \sup_{x \ne 0} ||A(x / ||x||)|| \\
				& = \max_{||y|| = 1} ||Ay||
		\end{aligned}
	\]
\end{example}

\begin{example}\label{exa:normOfProductLessThanProductOfNorms}
	(Problems class) Show that $||AB|| \le ||A|| \cdot ||B||$ for all matrices $A, B$ and every induced norm $||\cdot||$.
	\[
		\begin{aligned}
			\sup_{x \ne 0, Bx \ne 0} \frac{||ABx||}{||x||}
				& = \sup_{x \ne 0, Bx \ne 0} \frac{||ABx||}{||Bx||} \frac{||Bx||}{||x||} \\
				& \le \sup_{Bx \ne 0} \frac{||ABx||}{||Bx||} \cdot \sup_{x \ne 0} \frac{||Bx||}{||x||} \\
				& = ||A|| \cdot ||B||
		\end{aligned}
	\]
	When $Bx = 0$,
	\[
		0 = ||ABx|| = ||A|| \cdot ||Bx|| = 0
	\]
	so we can remove the condition $Bx \ne 0$ on the $\sup$s in the equation above.
\end{example}

\subsection{Errors and condition numbers}

In the case where we want to solve $Ax = b$ for $x$, but with some floating po0int error in $b$, we have $b + \delta b$ instead of $b$, where $\delta b$ is unknown but bounded (small). So we have
\[
	A (x + \delta x) = b + \delta b
\]
It is not always the case that $\delta x$ is also small. We want to find bounds for $\delta x$ in terms of $\delta b$. We have that $A \delta x = \delta b$. Assuming $A^{-1}$ exists, $\delta x = A^{-1} \delta b$. Using any norm $||\cdot||_*$,
\[
	||\delta x||_* = ||A^{-1} \delta b||_* \le ||A^{-1}||_* ||\delta b||_*
\]
Assuming $x \ne 0$, divide by $||x||_*$:
\[
	\frac{||\delta x||_*}{||x||_*} \le ||A^{-1}|| \frac{||\delta b||_*}{||x||_*}
\]
Now since $Ax = b$,
\[
	||b||_* = ||A x||_* \le ||A||_* ||x||_* \Longrightarrow \frac{1}{||x||_*} \le ||A||_* \frac{1}{||b||_*}
\]
Therefore,
\[
	\frac{||\delta x||_*}{||x||_*} \le ||A||_* ||A^{-1}||_* \frac{||\delta b||_*}{||b||_*}
\]
This means the relative error in $x$ is less than a constant multiplied by the relative error in $b$.

\begin{definition}
	For any norm $||\cdot||_*$, we define
	\[
		\kappa_* (A) := ||A||_* ||A^{-1}||_*
	\]
	which is called the \textbf{*-condition} number of the matrix $A$.
\end{definition}

\begin{example}
	(Problems class) Prove that $\kappa(A) \ge 1$ for every non-singular matrix $A$ and for every induced matrix norm.
	\[
		1 = ||I|| = ||A A^{-1}|| \le ||A|| \cdot ||A^{-1}||
	\]
	by Example~\ref{exa:normOfProductLessThanProductOfNorms}.
\end{example}

\begin{example}
	(Problems class) Prove that
	\[
		||x||_p = {\left( \sum_i |x_i|^p \right)}^{1 / p}
	\]
	is a norm for every $p \in [1, \infty)$.
	\[
		\begin{aligned}
			{(||x + y||_p)}^p
				& = \sum_i |x_i + y_i|^p \\
				& = \sum_i |x_i + y_i| \cdot |x_i + y_i|^{p - 1} \\
				& \le \sum_i (|x_i| + |y_i|) |x_i + y_i|^{p - 1} \\
				& = \sum_i |x_i| \cdot |x_i + y_i|^{p - 1} + \cdots \\
				& \le {\left( \sum_i |x_i|^r \right)}^{1 / r} {\left( \sum_j |x_j + y_j|^{q (p - 1)} \right)}^{1 / q} + \cdots
		\end{aligned}
	\]
	by the Holder inequality (TODO: define this inequality). Let $r = p, q = 1/(1 - 1/p) = p / (p - 1)$. Then
	\[
		\begin{aligned}
			{(||x + y||_p)}^p
				& \le {\left( \sum_i |x_i|^p \right)}^{1 / p} {\left( \sum_j |x_j + y_j|^{(p - 1) p / (p - 1)} \right)}^{(p - 1) / p} + \cdots \\
				& = ||x||_p {(||x + y||)}_p^{p - 1} + ||y||_p {(||x + y||)}_p^{p - 1}
		\end{aligned}
	\]
\end{example}

\subsection{$L^2$ norm, eigenvalues and diagonalisability}

\begin{theorem}\label{thm:smallEigenValuesEquivalentToZeroConvergence}
	Let $\{ \lambda_j \}$ be the eigenvalues of a matrix $A$. Then
	\[
		\max \{ |\lambda_j| \} < 1 \Longleftrightarrow A^k \rightarrow 0 \quad \text{ as } k \rightarrow \infty
	\]
\end{theorem}

\begin{example}
	Let
	\[
		A = \begin{bmatrix}
			0.99 & 10^6 \\
			0 & 0.99
		\end{bmatrix}
		\Longrightarrow
		A^{128} = \begin{bmatrix}
			0.2765\dots & 3.751 \cdot 10^6 \\
			0 & 0.2765\dots
		\end{bmatrix}
	\]
	This does not seem to agree with Theorem~\ref{thm:smallEigenValuesEquivalentToZeroConvergence}, however the theorem is correct, as products and powers of matrices are more complicated than those of complex numbers. For example, for every $z \in \mathbb{C}$, $|z|^k = |z^k|$ but generally $||A^k|| \ne ||A||^k$ for a matrix $A$ and a norm $||\cdot||$. $||A||^k$ could grow before it then tends to zero. This occurs especially when $A$ is not diagonalisable ($A$ is defective, meaning that its eigenvectors do not span $\mathbb{R}^n$).

	In fact, Theorem~\ref{thm:smallEigenValuesEquivalentToZeroConvergence} is easy to prove for diagonalisable $A$, but difficult for defective $A$.
\end{example}

\begin{definition}
	The \textbf{spectral radius} of a square matrix $A$ is defined as
	\[
		\rho(A) := \max \{ |\lambda_j| \}
	\]
	where the $\lambda_j$ are the eigenvalues of $A$.
\end{definition}

\begin{proposition}\label{prop:symmetricMatrixProperties}
	If $A$ is symmetric (or hermitian), then
	\begin{itemize}
		\item All its eigenvalues are real, so $B v_i = \lambda_i v_i$ where $\lambda_i \in \mathbb{R}$.
		\item Its eigenvectors $v_i$ form a basis of $\mathbb{R}^n$: every vector $x \in \mathbb{R}^n$ can be written as a linear combination of the eigenvectors of $A$, i.e. $x = \sum_i c_i v_i$ where $v_i$ are the eigenvectors of $A$ and $c_i$ are real numbers.
		\item Its eigenvectors can be chosen to be orthonormal, i.e. $v_i \cdot v_j = \delta_{ij}$.
		\item Parseval's identity holds:
		\[
			{(||x||_2)}^2 = \left( \sum_i c_i v_i, \sum_j c_j v_j \right) = \sum_{i, j} c_i c_j (v_i, v_j) = \sum_i c_i^2
		\]
	\end{itemize}
\end{proposition}

\begin{definition}
	A square matrix $A$ is \textbf{normal} if it commutes with its transpose:
	\[
		A A^T = A^T A
	\]
	($A A^* = A^* A$ for complex matrices).
\end{definition}

\begin{definition}
	A matrix $U$ is unitary if
	\[
		U^* = U^{-1}
	\]
\end{definition}

\begin{definition}
	A matrix $A$ is unitarily diagonalisable if $A = U D U^*$ for some unitary matrix $U$ and diagonal matrix $D$.
\end{definition}

\begin{theorem}
	(\textbf{The spectral theorem}) A matrix $A$ is unitarily diagonalisable if and only if it is normal.
\end{theorem}

\begin{example}
	Let $a \ne 1$ and
	\[
		A = \begin{bmatrix}
			1 & a \\
			0 & 1
		\end{bmatrix}
	\]
	is not normal, as
	\[
		A A^T = \begin{bmatrix}
			1 + a^2 & a \\
			a & 1
		\end{bmatrix}
		\ne
		A^T A = \begin{bmatrix}
			1 & a \\
			a & 1 + a^2
		\end{bmatrix}
	\]
	$\lambda = 1$ is the only eigenvalue, with eigenvector $(1, 0)$. $A$ is not unitarily diagonalisable.
\end{example}

\begin{theorem}
	The induced $2$-norm of a matrix $A$ is
	\[
		\sqrt{\rho(A^T A)} = \max \{ \sqrt{|\lambda|}: \lambda \text{ is an eigenvalue of } A^T A \}
	\]
\end{theorem}

\begin{proof}
	For every matrix $A$, $A^T A$ is symmetric, so by Proposition~\ref{prop:symmetricMatrixProperties} its eigenvalues are real and its eigenvectors, $\{ v_i \}$ for $i \in \{ 1, \dots, n \}$, are orthonormal and span $\mathbb{R}^n$. So for every $x \in \mathbb{R}^n$,
	\[
		x = \sum_i c_i v_i
	\]
	for some $c_i \in \mathbb{R}$, $A^T A v_i = c_i v_i$ and $(v_i, v_j) = \delta_{i, j}$.
	\[
		\begin{aligned}
			{(||Ax||_2)}^2
				& = (Ax, Ax) = (A^T A x, x) = \left( A^T A \sum_i c_i v_i, \sum_j c_j v_j \right) \\
				& = \left( \sum_i \lambda_i c_i v_i, \sum_k c_j v_j \right) \\
				& = \sum_{i, j} \lambda_i c_i c_j (v_i, v_j) \\
				& = \sum_i \lambda_i c_i^2
		\end{aligned}
	\]
	We claim that $\lambda_j \ge 0 \ \forall j$. Indeed, if $\lambda_s < 0$, let $x = v_s$, then
	\[
		{(||A v_s||_2)}^2 = \lambda_s < 0
	\]
	Write $0 \le \lambda_1 \le \lambda_2 \le \cdots \le \lambda_n$ (counting multiplicities). Then
	\[
		{(||Ax||_2)}^2 = \sum_i \lambda_i c_i^2 \le \lambda_n \sum_i c_i^2 = \lambda_n {(||x||_2)}^2
	\]
	which gives
	\[
		||A||_2 = \sup_{x \ne 0} \frac{||Ax||_2}{||x||_2} \le \sqrt{\lambda_n}
	\]
	Taking $x = v_n$ gives ${(||A x_n||_2)}^2 = \lambda_n {(||x_n||_2)}^2$, which gives equality in the above inequality.
\end{proof}

\begin{remark}
	$\rho(A^T A) = \rho(A A^T)$.
\end{remark}

\begin{example}
	Let
	\[
		A = \begin{bmatrix}
			1 & a \\
			0 & 1
		\end{bmatrix}
		\Longrightarrow
		A^T A = \begin{bmatrix}
			1 & 0 \\
			0 & 1 + a^2
		\end{bmatrix}
	\]
	The eigenvalues of $A^T A$ are given by $0 = (1 - \lambda) (1 + a^2 - \lambda) - a^2$, which gives $\lambda_1 \approx 2$ and $\lambda_2 \approx 2 a^2$ for $|a| >> 1$.
\end{example}

\subsection{Iterative methods for linear systems}

This section is concerned with solving the equation $Ax = b$ with $A$ a matrix and $x$ and $b$ vectors, for very large sizes of $A$.

\begin{definition}
	We define \textbf{Richardson's method} as the following: for some $\omega > 0$, let
	\[
		x^{(k+1)} = x^{(k)} + \omega (b - Ax^{(k)})
	\]
	Clearly $x^{(k)} = x^{(k + 1)} = x$, the exact solution, satisfies this. The \textbf{residual} is defined as $r^{(k)} := x^{(k)} - x$. Then
	\[
		\begin{aligned}
			r^{(k + 1)}
				& = x^{(k + 1)} - x \\
				& = x^{(k)} - x + \omega (b - Ax^{(k)}) \\
				& = r^{(k)} + \omega (b - Ax^{(k)} - Ax) \\
				& = (I - \omega A) r^{(k)}
		\end{aligned}
	\]
	So $r^{(k)} = {(I - \omega A)}^k r^{(0)}$. Consider the case where $A$ has real eigenvalues which satisfy
	\[
		0 < \lambda_1 \le \lambda_2 \le \cdot \le \lambda_m
	\]
	So the eigenvalues of $I - \omega A$ are also real and satisfy
	\[
		1 - \omega \lambda_m \le \cdots \le 1 - \omega \lambda_1 < 1
	\]
	By the spectral radius theorem (TODO: add this to notes), for convergence, we need
	\[
		-1 < 1 - \omega \lambda_m \Longleftrightarrow \omega < 2 / \lambda_m = 2 / \rho(A)
	\]
	So the iteration converges iff the above equation holds.
\end{definition}

\begin{remark}
	If $\rho(A)$ is large, then $\omega$ must be small. This is a problem, because if $\omega$ is small, then the iteration takes a long time to converge.
\end{remark}

\begin{definition}
	$A$ is called \textbf{strictly upper triangular} if $A$ is upper triangular and $A_{i, i} 0 0$ for every $i$. $A$ is called \textbf{unit upper triangular triangular} if $A$ is upper triangular and $A_{i, i} = 1$ for every $i$.
\end{definition}

\begin{definition}
	We define \textbf{Jacobi's method} as follows: assuming $A_{i, i} \ne 0$ for every $i$, we rewrite $Ax = b$ as
	\[
		Dx = (E + F)x + b
	\]
	where $D$ is diagonal, $E$ is strictly lower triangular, $F$ is strictly upper triangular, and we use the iteration
	\[
		D x^{(k + 1)} = (E + F) x^{(k)} + b \Longleftrightarrow x^{(k + 1)} = D^{-1} ( (E + F) x^{(k)} + b) =: M_J^{-1} (N_J x^{(k)} + b)
	\]
	Consider the residual $r^{(k)} := x^{(k)} - x$:
	\[
		\begin{aligned}
			r^{(k + 1)}
				& = x^{(k + 1)} - x \\
				& = M^{-1} N x^{(k)} + M^{-1} b - x \\
				& = M^{-1} N r^{(k)} + M^{-1} N x + M^{-1} b - x \\
				& = M^{-1} N r^{(k)}
		\end{aligned}
	\]
	Arguing as before, this iteration converges iff
	\[
		{(M_J^{-1} N_J)}^k \to 0 \quad \text{as } k \to \infty
	\]
	The implementation of this method is as follows:
	\[
		x_i^{(k + 1)} = \left( \sum_{j \ne i} A_{i, j} x_j^{(k)} + b_i \right) / A_{i, i}
	\]
\end{definition}

\begin{remark}
	Computing eigenvalues is even harder than solving $Ax = b$.
\end{remark}

\begin{definition}
	A square matrix $B$ is called \textbf{strictly diagonally dominant} if
	\[
		\forall i, \quad |B_{i, i}| < \sum_{j \ne i} |B_{i, j}|
	\]
	$B$ is called \textbf{diagonally dominant} if
	\[
		\forall i, \quad |B_{i, i}| \le \sum_{j \ne i} |B_{i, j}|
	\]
\end{definition}

\begin{proposition}
	The Jacobi method converges if $A$ is strictly diagonal dominant.
\end{proposition}

\begin{proof}
	\[
		\begin{aligned}
			{(M^{-1} N)}_{i, j}
				& = \sum_k D_{i, k}^{-1} {(A - D)}_{i, j} \\
				& = D_{i, i}^{-1} {(A - D)}_{i, j} \\
				& = \begin{cases}
					A_{i, i}^{-1} A_{i, j} & \text{if } i \ne j \\
					0 & \text{ otherwise}
				\end{cases}
		\end{aligned}
	\]
	Then consider
	\[
		||M^{-1} N||_{\infty} = \max_i |M^{-1} N_{i, j}| = \max_i \sum_{j \ne i} |A_{i, j}| / |A_{i, i}| < 1
	\]
	if $A$ is strictly diagonal dominant, henc ${(M^{-1} N)}^k \to 0$ as $k \to \infty$.
\end{proof}

\begin{definition}
	The \textbf{Gauss-Seidel method} is defined as the iteration
	\[
		\begin{aligned}
			x_i^{(k + 1)}
				& = \left( b_i - \sum_{j < i} A_{i, j} x_j^{(k + 1)} - \sum_{j > i} A_{i, j} x_j^{(k)} \right) \\
				& = x_i^{(k)} + \left( b_i - \sum_{j < i} A_{i, j} x_j^{(k + 1)} - \sum_{j \ge i} A_{i, j} x_j^{(k)} \right)
		\end{aligned}
	\]
	which only requires one vector for $x$: when computing $x_i^{(k + 1)}$, we use the updated values of $x_j$ for $j < i$ and the old values of $x_j$ for $j \ge i$.

	Splitting $A$ into $A = D - E - F$ where $D$ is diagonal, $E$ is strictly lower triangular, and $F$ is strictly upper triangular, we can write
	\[
		(D - E) x^{(k + 1)} = F x^{(k)} + b \Longleftrightarrow M x^{(k + 1)} = N x^{(k)} + b
	\]
	where $M = D - E$ and $N = F$. Consider $r^{(k)} := x^{(k)} - x$, then
	\[
		r^{(k + 1)} = M^{-1} (N x^{(k)} + b) - x = M^{-1} (N r^{(k)} + Nx + b) - x = M^{-1} N r^{(k)}
	\]
\end{definition}

\begin{proposition}
	If $A$ is strictly diagonally dominant, then the Gauss-Seidel method converges.
\end{proposition}

\begin{remark}
	To accelerate convergence, we can sometimes use a parameter $w > 1$ and use
	\[
		x_i^{(k)} + w \left( b_i - \sum_{j < i} A_{i, j} x_j^{(k + 1)} - \sum_{j \ge i} A_{i, j} x_j^{(k)} \right)
	\]
\end{remark}

\begin{example}
	Let $W$ be any non-singular matrix and $||\cdot||$ be any vector norm. Prove that
	\[
		||x||_W := ||Wx||
	\]
	is a vector norm.

	Verify the properties:
	\begin{itemize}
		\item $||cx||_W = ||W(cx)|| = ||c(Wx)|| = |c| ||Wx|| = |c| ||x||_W$.
		\item $||x + y||_W = ||W(x + y)|| = ||Wx + Wy|| \le ||Wx|| + ||Wy|| = ||x||_W + ||y||_W$.
		\item If $x \ne 0$, then $Wx \ne 0$ as $W$ is non-singular. Hence $||x||_W = 0 \Longrightarrow x = 0$.
	\end{itemize}
\end{example}

\begin{example}
	Prove that for symmetric $A$, $\rho(A) = ||A||_2$.

	For any matrix $B$, $||B||_2 = \rho{(B B^T)}^1/2$. For symmetric $A = A^T$, we have a full set of eigenvectors with eigenvalues $\lambda_1 \le \cdots \le \lambda_n$. Now if $Av = \lambda v$ then $A A^T v = A^2 v = A(\lambda v) = \lambda^2 v$. So if $\lambda^2$ is an eigenvalue of $A A^T$ then $|\lambda|$ is an eigenvalue of $A$ with a largest modulus.
\end{example}

\section{$L^2$ Approximations}

\subsection{$L^2$ approximations of functions}

\begin{definition}
	Let $f$ be a function and $w$ be a weight function, with $w(x) > 0$ except at a finite number of points. The \textbf{weighted $L^2$ norm} of $f$ is defined as
	\[
		||f||_{L^2_w (a, b)} := \left( \int_{a}^{b} |f(x)|^2 w(x) dx \right)
	\]
\end{definition}

\begin{remark}
	The weighted $L^2$ norm is a seminorm for every function $f$ but is a norm for every continuous $f$.
\end{remark}

\begin{example}
	To solve the problem, we can write $p(x) = p_0 \phi(x) + \cdots + p_N \phi_N(x)$ for some undetermined coefficients $p_0, \ldots, p_N$. Then
	\[
		\begin{aligned}
			E(p) & = \int_a^b |p_0 \phi_0(x) + \cdots + p_N \phi_N(x) - f(x)|^2 w(x) dx \\
			& = \int_{a}^{b} \left| \sum_{i = 0}^N p_i \phi_i(x) - f(x) \right|^2 w(x) dx \\
			& = \int_{a}^{b} \left( \sum_{j, k}^N p_j p_k \phi_j(x) \phi_k(x) - 2 f(x) \sum_k p_k \phi_k(x) + {f(x)}^2 \right) w(x) dx \\
			& = ||f||^2 - 2 \sum_k p_k \int_{a}^{b} \phi_k(x) f(x) w(x) dx + \sum_{j, k} p_j p_k \int_{a}^{b} \phi_j(x) \phi_k(x) w(x) dx \\
			& =: ||f||^2 - 2 \sum_k p_k q_k + \sum_{j, k} p_j p_k A_{j, k}
		\end{aligned}
	\]
	To find the $p$ which minimises this, we vary the coefficients $p_j$ until we find a stationary point. For $i \in \{0, \ldots, N\}$,
	\[
		\diffp{E}{p_i} = -2 q_i + 2 \sum_k A_{i, k} p_k + \sum_j p_j A_{j, i} \Longleftrightarrow \sum_j A_{i, j} p_j = q_i
	\]
	If we can solve this linear system, we have our stationary point. We show that $A$ is strictly positive definite, so there is a unique $p$ which minimises the error. For every vector $(z_0, \ldots, z_N)$,
	\[
		\begin{aligned}
			\sum_{j, k} A_{j, k} z_j z_k & = \int_{a}^{b} \left( \sum_j z_j \phi_j(x) \right) \left( \sum_k z_k \phi_k(x) \right) w(x) dx \\
			& = \int_{a}^{b} \left( \sum_j z_j \phi_j(x) \right)^2 w(x) dx \\
			& > 0 \text{ iff the } \phi_j \text{ are linearly independent}
		\end{aligned}
	\]
	so assuming that $\phi_j$ are linearly independent, $A$ is strictly positive definite. So $E(p)$ has a unique minimum and no maximum.
\end{example}

\begin{example}
	Let $(a, b) = (-1, 1)$ and $w(x) = {(1 - x^2)}^{-1/2}$. Find a quadratic approximation to $f(x) = \arcsin(x)$. We use the above example to compute
	\[
		q_0 = \int_{-1}^{1} x^0 \arcsin(x) {(1 - x^2)}^{-1/2} dx = 0
	\]
	since the integrand is an odd function. Similarly,
	\[
		q_2 = \int_{-1}^{1} x^2 \arcsin(x) {(1 - x^2)}^{-1/2} dx = 0
	\]
	Now
	\[
		q_1 = \int_{-1}^{1} x^1 \arcsin(x) {(1 - x^2)}^{-1/2} dx = \int_{0}^{\pi} \cos(\theta) (\pi / 2 - \theta) d\theta = 2
	\]
	So
	\[
		\begin{aligned}
			A_{1, 1} & = \int_{-1}^{1} x^0 x^0 {(1 - x^2)}^{-1/2} dx = \pi \\
			A_{1, 2} = A_{2, 1} & = \int_{-1}^{1} x^0 x^1 {(1 - x^2)}^{-1/2} dx = 0 \\
			A_{2, 3} = A_{3, 2} & = \int_{-1}^{1} x^1 x^2 {(1 - x^2)}^{-1/2} dx = 0 \\
			A_{3, 1} = A_{2, 2} = A_{1, 3} & = \int_{-1}^{1} x^2 x^0 {(1 - x^2)}^{-1/2} dx = \frac{\pi}{2} \\
			A_{3, 3} & = \int_{-1}^{1} x^2 x^2 {(1 - x^2)}^{-1/2} dx = \frac{3 \pi}{8}
		\end{aligned}
	\]
	So
	\[
		A = \pi \begin{pmatrix}
			1 & 0 & 1/2 \\
			0 & 1/2 & 0 \\
			1/2 & 0 & 3/8
		\end{pmatrix},
		\quad q = (0, 2, 0)
	\]
	So $p = \frac{1}{\pi} (0, 4, 0)$ so $p(x) = \frac{4}{\pi} x$.
\end{example}

\begin{definition}
	A map $(\cdot, \cdot): V \times V \rightarrow \mathbb{C}$ is called an \textbf{inner product} if $\forall u, v \in V$ and $\alpha, \beta \in \mathbb{C}$,
	\begin{enumerate}
		\item $(\alpha u + \beta u', v) = \alpha (u, v) + \beta (u', v)$.
		\item $(u, v) = \overline{(v, u)}$.
		\item $(u, u) \geq 0$ and $(u, u) = 0$ iff $u = 0$.
	\end{enumerate}
\end{definition}

\begin{remark}
	From properties 1. and 2. of an inner product, $(u, \alpha u) = \overline{\alpha} (u, v)$.
\end{remark}

\begin{theorem}
	Given an inner product $(\cdot, \cdot)$ on $V$, the map $||\cdot||: V \rightarrow \mathbb{R}_+$ defined as
	\[
		||u|| := {(u, u)}^{1/2}
	\]
	is a norm.
\end{theorem}

\begin{proof}
	\hfill
	\begin{itemize}
		\item Linearity: use properties 1. and 2.
		\item Positivity for $u \ne 0$ follows from property 3.
		\item The triangle inequality follows from Cauchy-Schwarz:
		\[
			\begin{aligned}
				0 \le ||u + \alpha v||^2 & = (u + \alpha v, u + \alpha v) \\
				& = ||u||^2 + |\alpha|^2 ||v||^2 + \alpha (v, u) + \overline{\alpha} (u, v)
			\end{aligned}
		\]
		Then set $\alpha = -(u, v) / ||v||^2$ so $\overline{\alpha} = -(v, u) / ||v||^2$ by property 2. Then
		\[
			\begin{aligned}
				0 & \le ||u||^2 + |(u, v)|^2 / ||v||^2 - (u, v) (v, u) / ||v||^2 - (v, u) (u, v) / ||v||^2 \\
				& = ||u||^2 - |(u, v)|^2 / ||v||^2 \\
				& \Longleftrightarrow |(u, v)|^2 \le ||u||^2 ||v||^2
			\end{aligned}
		\]
		This proves the Cauchy-Schwarz inequality. Now
		\[
			\begin{aligned}
				||u + v||^2 = (u + v, u + v) & = ||u||^2 + ||v||^2 + (u, v) + (v, u) \\
				& = ||u||^2 + ||v||^2 + 2 |(u, v)| \\
				& \le ||u||^2 + ||v||^2 + 2 ||u|| ||v|| \\
				& = {(||u|| + ||v||)}^2
			\end{aligned}
		\]
		so
	\end{itemize}
\end{proof}

\begin{example}\label{exa:innerProdRN}
	Let $V = \mathbb{R}^n$ and let $u = (u_1, \ldots, u_n) = u_1 \underline{e_1} + \cdots + u_n \underline{e_n}$. An inner product is defined as
	\[
		(u, v) = \sum_{i = 1}^{n} u_i v_i
	\]
	Then a norm is
	\[
		||u|| = \sqrt{\sum_{i = 1}^{n} u_i^2}
	\]
\end{example}

\begin{example}
	Let $V = C^0 ([a, b])$. An inner product is defined as
	\[
		{(u, v)}_{L^2 w (a, b)} := \int_{a}^{b} u(x) v(x) w(x) dx
	\]
	Then a norm is
	\[
		{(u, u)}_{L^2 w (a, b)} = ||u||^2_{L^2 w (a, b)}
	\]
	i.e. the $L^2$ norm as before.
\end{example}

\begin{theorem}
	Let $V$ be an inner product space and $X$ be a linear subspace of $V$. Let $f \in V$ be given and suppose $\tilde{p} \in X$ minimises $||f - p||$:
	\[
		E(\tilde{p}) = ||f - \tilde{p}||^2 \le ||f - p||^2 \quad \forall p \in X
	\]
	Then $\tilde{p}$ satisfies
	\[
		(f - \tilde{p}, p) = 0 \quad \forall p \in X
	\]
\end{theorem}

\begin{proof}
	(Assume $V$ is a real inner product space). Assume that $(f - \tilde{p}, \hat{p}) = a \ne 0$ for some $\hat{p} \in X$. Let
	\[
		q = \hat{p} + \frac{a}{||\hat{p}||^2} \hat{p} \in X
	\]
	Then
	\[
		\begin{aligned}
			||f - q||^2 & = ||f - \hat{p} - \frac{a}{||\hat{p}||^2} \hat{p}||^2 \\
			& = ||f - \hat{p}||^2 - \frac{2a}{||\hat{p}||^2} (f - \tilde{p}, \hat{p}) + \frac{a^2}{||\hat{p}||^4} ||\hat{p}||^2 \\
			& = ||f - \tilde{p}||^2 - \frac{a^2}{||\hat{p}||^2} \\
			& < ||f - \tilde{p}||^2
		\end{aligned}
	\]
	But this is a contradiction as then $||f - p||$ is not minimised by $\tilde{p}$. So $a = 0$.
\end{proof}

\begin{definition}
	(\textbf{Gram-Schmidt}) Given a basis $\{ \phi_k \}$ which is not orthogonal, the algorithm is as follows:
	\begin{itemize}
		\item $\hat{\phi}_0 = \phi_0$.
		\item \[
			\hat{\phi}_k = \phi_k - \sum_{j = 0}^{k - 1} \frac{(\hat{\phi}_j, \phi_k)}{||\hat{\phi}_j||^2} \hat{\phi}_j
		\]
	\end{itemize}
\end{definition}

\begin{proposition}
	Let $V = C^0 (a, b; \mathbb{R})$ with the inner product
	\[
		{(u, v)}_{L^2 w(a, b)} := \int_{a}^{b} u(x) v(x) w(x) dx
	\]
	and $X = P_n$ (set of polynomials of degree $n$). let $P_n = \text{span}\{ \phi_0, \dots, \phi_n \}$ with $\phi_k \in P_k$. The following properties hold:
	\begin{enumerate}
		\item $\{ \phi_k \}$ is unique up to normalisation: if $\{ \hat{\phi}_k \}$ is another orthogonal set with respect to the same inner product, then $\hat{\phi}_k = c_k \phi_k$ for some $c_k \in \mathbb{R}$.
		\item $\phi_k$ has exactly $k$ real roots in $(a, b)$.
		\item $\{ \phi_k \}$ satisfy the recurrence relation $\phi_{-1} (x) := 0, \phi_0(x) = 1$,
		\[
			\phi_{k + 1} (x) = \frac{1}{||\phi_k||} x \phi_k(x) - \frac{(x \phi_k, \phi_k)}{||\phi_k||^3} \phi_k(x) - \frac{||\phi_k||}{||\phi_{k - 1}||} \phi_{k - 1}(x)
		\]
	\end{enumerate}
\end{proposition}

\begin{proof}
	\hfill
	\begin{enumerate}
		\item Let $\phi_k(x) = a_k x^k + \cdots + a_0$, $\tilde{\phi}_k(x) = \tilde{a}_k x^k + \cdots + \tilde{a}_0$. Then $\tilde{\phi}_k(x) - \frac{\tilde{a}_k}{a_k} \phi_k(x) =: b_{k - 1} x^{k - 1} + \cdots + b_0 =: \delta \phi(x) \in P_{n - 1}$. Now,
		\[
			(\delta \phi, \delta \phi) = \left( \tilde{\phi}_k - \frac{\tilde{a}_k}{a_k} \phi_k, \delta \phi \right) = (\tilde{\phi}_k, \delta \phi) - \frac{\tilde{a}_k}{a_k} (\phi_k, \delta \phi) = 0
		\]
		since $(\tilde{\phi}_k, \phi_m) = 0$ for $k \ne m$ since they are orthogonal. Hence $\delta \phi = 0$.
		\item We have $\forall p \in P_{k - 1}, (\phi_k, p) = 0$. $\phi_k$ can have at most $k$ sign changes in $(a, b)$. We will show that it has exactly $k$ sign changes. Assume that $\phi_k$ has $m < k$ sign changes at $\{ x_j \}_{j = 1}^m$. Define
		\[
			s(x) := (x - x_1) \cdots (x - x_m) \in P_m
		\]
		Then
		\[
			(\phi_k, s) = \int_{a}^{b} \phi_k(x) s(x) w(x) dx = 0
		\]
		since $s \in P_m$ with $m < k$. So $m = k$ and each sign change corresponds to a simple root.
		\item Omitted.
	\end{enumerate}
\end{proof}

\begin{remark}
	Properties 1 and 2 imply that the roots of $\phi_k$ are unique. The recurrence relation defined above should be used instead of Gram-Schmidt where possible.
\end{remark}

\subsection{Convergence of $L^2$ approximations}

\begin{theorem}
	(\textbf{Weierstrass}) For every $f \in C^0 (a, b; \mathbb{R})$ and every $\epsilon > 0$, for some $N \in \mathbb{N}$ and $p \in P_N$,
	\[
		\max_{x \in [a, b]} |f(x) - p(x)| < \epsilon
	\]
\end{theorem}

\begin{proof}
	Omitted.
\end{proof}

\section{Numerical Integration}

We want to numerically approximate the integral
\[
	I[f] = \int_{a}^{b} f(x) w(x) dx
\]
for $w(x)$ a weight function.

\begin{definition}
	For a function $f$, a \textbf{quadrature formlula} is defined as
	\[
		Q_n(f) = (b - a) \sum_{k = 0}^n \hat{\sigma}_k f(x_k)
	\]
	where $\hat{\sigma}_k$ are are the \textbf{coefficients} and $x_k \in (a, b)$ are the \textbf{nodes}.
\end{definition}

\begin{example}
	The trapezium rule is
	\[
		Q_1(f) = (b - a) \frac{f(a) + f(b)}{2}
	\]
	Here $x_0 = a$, $x_1 = b$, $\hat{\sigma}_0 = \hat{\sigma}_1 = \frac{1}{2}$.
\end{example}

\begin{definition}
	A quadrature formula $Q_n$ has \textbf{degree of exactness} $r$ if
	\begin{itemize}
		\item $Q_n(x^m) = I(x^m)$ for $m \in \{ 0, \dots, r \}$ and
		\item $Q_n(x^{r + 1}) \ne I(x^{r + 1})$.
	\end{itemize}
\end{definition}

\begin{proposition}
	\hfill
	\begin{enumerate}
		\item $Q_n$ is linear: $Q_n(f + g) = Q_n(f) + Q_n(g)$.
		\item If $Q_n$ has degree of exactness $r$, then $Q_n(p) = I(p)$ for every $p \in P_r$.
	\end{enumerate}
\end{proposition}

\begin{definition}
	(\textbf{Interpolatory quadrature}) Let $w(x) = 1$ and let $\{ x_k \}_{k = 0}^n$ be such that $x_k < x_{k + 1}$. We interpolate $f$ by $p \in P_n$ at the nodes: $p(x_k) = f(x_k)$. Then we compute
	\[
		I_n(f) = \int_{a}^{b} p(x) dx
	\]
	Now using the Lagrange interpolation formula,
	\[
		p(x) = \sum_{k = 0}^{n} f(x_k) L_k(x)
	\]
	where
	\[
		L_k(x) = \prod_{j = 0, j \ne k}^{n} \frac{x - x_j}{x_k - x_j}
	\]
	Therefore
	\[
		I_n(f) = \int_{a}^{b} \sum_k f(x_k) L_k(x) dx = \sum_k f(x_k) \int_{a}^{b} L_k(x) dx
	\]
	Let $t = \frac{x - a}{b - a} \in (0, 1)$, $t_k = \frac{x_k - a}{b - a} \in (0, 1)$. Then
	\[
		\int_{a}^{b} L_k(x) = \int_{a}^{b} \prod_{j \ne k} \frac{x - x_j}{x_k - x_j} dx = (b - a) \int_{0}^{1} \prod_{j \ne k} \frac{t - t_j}{t_k - t_j} dt =: \sigma_k
	\]
\end{definition}

\begin{definition}
	If the nodes $x_k$ are equidistant, $x_k = x_{k - 1} - x_{k + 1} - x_k$, the resulting quadrature is called a \textbf{Newton-Cotes formula}. If $x_0 = a$ and $x_n = b$, it is called a \textbf{closed Newton-Cotes formula}.
\end{definition}

\begin{example}
	For $n = 0$, $L_0(x) = 1$ and $\sigma_0 = 1$. So $I_0(f) = (b - a) F(\xi)$ for any $\xi \in [a, b]$.

	If $\xi = \frac{b + a}{2}$, this is called the \textbf{rectangle method}.
\end{example}

\begin{example}
	For $n = 1$, for the closed Newton-Cotes formula, $t_0 = 0$ and $t_1 = 1$. $L_0(t) = \frac{t - t_1}{t_0 - t_1} = 1 - t$ and $L_1(t) = \frac{t - t_0}{t_1 - t_0} = t$, so $\sigma_0 = \sigma_1 = \frac{1}{2}$. Hence
	\[
		I_1(f) = (b - a) \frac{f(a) + f(b)}{2}
	\]
\end{example}

\begin{example}
	For $n = 2$, for the closed Newton-Cotes formula, $t_0 = 0$, $t_1 = \frac{1}{2}$ and $t_2 = 1$. $L_0(t) = \frac{(t - t_1)(t - t_2)}{(t_0 - t_1)(t_0 - t_2)} = \frac{1}{2} t^2 - t$ and $L_1(t) = \frac{(t - t_0)(t - t_2)}{(t_1 - t_0)(t_1 - t_2)} = -t^2 + t$ and $L_2(t) = \frac{(t - t_0)(t - t_1)}{(t_2 - t_0)(t_2 - t_1)} = \frac{1}{2} t^2 - \frac{1}{2} t$. So $\sigma_0 = \sigma_2 = \frac{1}{6}$ and $\sigma_1 = \frac{2}{3}$. Hence
	\[
		I_2(f) = (b - a) \frac{f(a) + 4f(\frac{a + b}{2}) + f(b)}{6}
	\]
	This is called \textbf{Simpson's rule}.
\end{example}

\begin{theorem}
	Let $f \in C^{n + 1}([a, b])$ and let $p \in P_n$ interpolate $f$ at $\{ x_k \}_{k = 0}^n \subset [a, b]$. Then for every $x \in [a, b]$,
	\[
		f(x) - p(x) = \frac{(x - x_0) \cdots (x - x_n)}{(n + 1)!} f^{(n + 1)}(\xi)
	\]
	where $\xi \in (a, b)$ and we denote $\omega_{n + 1}(x) = (x - x_0) \cdots (x - x_n)$.  
\end{theorem}

\begin{theorem}
	Let $f \in C^{n + 1}([a, b])$. If $I_n$ is an interpolatory quadrature on $[a, b]$, then
	\[
		|I(f) - I_n(f)| \le \frac{1}{(n + 1)!} \max_{\xi \in (a, b)} |f^{(n + 1)}(\xi)| \int_{a}^{b} |(x - x_0) \cdots (x - x_n)| dx
	\]
\end{theorem}

\begin{example}
	(Problems class) Suppose that the interpolatory quadrature formula $I_1$ has the nodes $1/4$ and $2/3$ on the integration domain $[0, 1]$. Compute the coefficients of $I_1$.
	
	$I_1(f) = \sum_{k = 0}^{n} \sigma_k f(x_k) = \sigma_0 f(1/4) + \sigma_1 f(2/3)$. We compute
	\[
		\begin{aligned}
			\sigma_0 & = (b - a) \int_{a}^{b} \prod_{j \ne 0} \frac{x - x_j}{x_0 - x_j} dx = \int_{0}^{1} \frac{x - x_1}{x_0 - x_1} dx \\
			& = \int_{0}^{1} \frac{x - 2/3}{1/4 - 2/3} dx = \frac{2}{5}
		\end{aligned}
	\]
	and
	\[
		\begin{aligned}
			\sigma_1 = \int_{0}^{1} \frac{x - x_0}{x_1 - x_0} dx = \frac{3}{5}
		\end{aligned}
	\]
\end{example}

\begin{example}
	(Problems class) Find the coefficient $\sigma_0$ in the open Newton-Cotes formula
	\[
		I_0(f) = \sigma_0 f \left( \frac{a + b}{2} \right)
	\]
	and prove that
	\[
		|I(f) - I_0(f)| \le \frac{(b - a)^3}{24} \max_{\xi \in (a, b)} |f^{(2)}(\xi)|
	\]
	We use Taylor's theorem to write
	\[
		f(x) = f(x_0) + (x - x_0) f'(x_0) + \frac{{(x - x_0)}^2}{2!} f''(\xi(x))
	\]
	where $\xi(x) \in \text{conv} \{ x, x_0 \} \subset [0, 1]$. Then, with $p_0 = f(x_0)$ a constant polynomial,
	\[
		\begin{aligned}
			I(f) - I_0(f) & = I(f) - I(p_0) = I(f - p_0) \\
			& = \int_{a}^{b} (f(x) - p_0(x)) dx \\
			& = \int_{a}^{b} ((x - x_0) f'(x_0) + \frac{{(x - x_0)}^2}{2!} f''(\xi(x))) dx \\
			& = f'(x_0) \int_{a}^{b} (x - x_0) dx + \frac{1}{2!} \int_{a}^{b} f''(\xi(x)) (x - x_0)^2 dx
		\end{aligned}
	\]
	So
	\[
		\begin{aligned}
			|I(f) - I_0(f)| & = \frac{1}{2} \left| \int_{a}^{b} f''(\xi(x)) (x - x_0)^2 dx \right| \\
			& \le \frac{1}{2} \int_{a}^{b} |f''(\xi(x))| (x - x_0)^2 dx \\
			& \le \frac{1}{2} \max_{\xi \in [0, 1]} |f''(\xi)| \int_{a}^{b} (x - x_0)^2 dx \\
			& = \frac{(b - a)^3}{24} \max_{\xi \in (a, b)} |f^{(2)}(\xi)|
		\end{aligned}
	\]
\end{example}

\begin{example}
	(Problems class) Show that the Fourier-Chebyshev expansion
	\[
		\tilde{p}(x) = \sum_{k = 0}^{\infty} \frac{(f, T_k)}{||T_k||^2} T_k(x)
	\]
	where $T_K$ is the Chebyshev polynomial and the inner product is in $(-1, 1)$ with weight $w(x) = {(1 - x^2)}^{-1/2}$, converges for every $f \in C^2 ([-1, 1])$.

	We will show that the series
	\[
		p_N(x) = \sum_{k = 0}^{N} \frac{(f, T_k)}{||T_k||^2} T_k(x)
	\]
	converges absolutely.
	\[
		\begin{aligned}
			||p_N||^2 & = (p_N, p_N) = \sum_{j, k} c_j c_k (T_k, T_j) = c_0^2 \pi + \sum_{k = 1}^{N} c_k^2 \pi / 2
		\end{aligned}
	\]
	where $c_j = (f, T_j) / ||T_j||^2$. Now
	\[
		\begin{aligned}
			(f, T_k) & = \int_{-1}^{1} f(x) T_k(x) {(1 - x^2)}^{-1/2} dx \\
			& = \int_{0}^{\pi} f(\cos(\theta)) \cos(k\theta) d\theta \\
			& = \frac{1}{k} \left[ f(\cos(\theta)) \sin(k \theta) \right]_{0}^{\pi} - \frac{1}{k} \int_{0}^{\pi} \sin(k \theta) \diff{}{\theta} f(\cos(\theta)) d\theta \\
			& = \frac{1}{k^2} \left[ \cos(k\theta) \diff{}{\theta} f(\cos(\theta)) \right] + \frac{1}{k^2} \int_{0}^{\pi} \cos(k \theta) \diff[2]{}{\theta} f(\cos(\theta)) d\theta \\
			& = -\frac{1}{k^2} \left[ \cos(k\theta) f'(\cos(\theta)) \sin(\theta) \right]_{0}^{\pi} - \frac{1}{k^2} \int_{0}^{\pi} \cos(k \theta) \diff{}{\theta} (\sin(\theta) f'(\cos(\theta))) d\theta \\
			& = -\frac{1}{k^2} \int_{0}^{\pi} \cos(k \theta) (\cos(\theta) f'(\cos(\theta) - \sin(\theta)^2 f''(\cos(\theta))) d\theta
		\end{aligned}
	\]
\end{example}

\end{document}