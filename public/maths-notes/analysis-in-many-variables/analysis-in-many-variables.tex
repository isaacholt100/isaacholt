\input{../header}

\title{Analysis in Many Variables Course Notes}
\author{Isaac Holt}

\begin{document}

\input{../titletoc.tex}

\begin{theorem}
	If $f(x): U \rightarrow \mathbb{R}$ is differentiable with U an open subset of $\mathbb{R}^n$ and if $x$ is a function of $u_1, \dots, u_m$, such that the partial derivatives $\diffp{x_i}{u_j}$ exist for all $1 \le i \le n$ and all $1 \le j \le m$, and if $F(u_1, \dots, u_m) = f(x(u_1, \dots, u_m))$ then $\diffp{F}{u_b} = \diffp{x_i}{u_b} \diffp{f}{x_i}$
\end{theorem}

\begin{proof}
	We have $f: U \subset \mathbb{R}^n \rightarrow \mathbb{R}$, $x: \mathbb{R}^m \rightarrow \mathbb{R}^n$, $F: \mathbb{R}^m \rightarrow \mathbb{R}$ with $F = f(x(u))$.

	let $a = x(u_1, \dots, u_b, \dots, u_m)$, $a + h(k) = x(u_1, \dots, u_b + k, \dots, u_m)$. then
	\[\diffp{F}{u_b} = \lim{k \rightarrow 0}{\frac{F(u_1, \dots, u_b + k, \dots, u_m) - F(u_1, \dots, u_b, \dots, u_m)}{k}} = \lim{k \rightarrow 0}{\frac{f(x(u_1, \dots, u_b + k, \dots, u_m)) - f(x(u_1, \dots, u_b, \dots, u_m))}{k}} = \lim{k \rightarrow 0}{\frac{f(a + h(k)) - f(a)}{k}}\]
\end{proof}

\subsection{The implicit function theorem}

$y = g(x)$ gives $y$ as an explicit function of $x$. $f(x, y) = 0$ gives $y$ as an implicit function of $x$.

To go from an explicit function to an implicit function, set \[f(x, y) = y - g(x) = 0\]

Suppose the level curve $f(x, y) = c$ can be written as $y = g(x)$. Then $f(x, g(x)) = c$. Differentiating this using the chain rule:

\[\diff{}{x}f(x, g(x)) = \diffp{f}{x} + \diff{g}{x}\diffp{f}{y} = \diff{}{x}c = 0\]

Hence $\diff{g}{x} = \frac{- \partial f / \partial x}{\partial f / \partial y}$

\begin{theorem}
	(Implicit Function Theorem or IFT): If $f(x, y): U \rightarrow \mathbb{R}$ with $U \subseteq \mathbb{R}^2$ open is differentiable on the level curve $f(x, y) = c$, at which $\diffp{f}{y} \ne 0$, then a differentiable function $g(x)$ exists in a neighbourhood of $x_0$, satisfying $g(x_0) = y_0$.
\end{theorem}

\begin{remark}
	At points when $\diffp{f}{y} = 0$, if $\diffp{f}{x} = 0$, we can use the IFT to find $h(y)$ such that $x = h(y)$.
\end{remark}

\begin{remark}
	If there is a point $Q$ on a level curve $f(x, y) = c$ at which $\underline{\nabla} f = 0$ (this is a critical point), then the value of $c$ is called a critical value (otherwise it is a regular value) and the level curve cannot be written either as $y = g(x)$ or as $x = h(y)$ ($g$ and $h$ are differentiable) in a neighbourhood of $Q$.
\end{remark}

\subsection{The implicit function theorem for surfaces}

The level sets of scalar fields on $\mathbb{R}^3$ generally define surfaces, and we therefore have the IFTfor surfaces.

\begin{theorem}
	Let $f(x, y, z): U \rightarrow \mathbb{R}$ for $U \subseteq \mathbb{R}^3$ open be differentiable. Let $(x_0, y_0, z_0) \in U$ be a point of the level set $f(x, y, z) = c$ so $f(x_0, y_0, z_0) = c$.

	If $\diffp{f}{z} (x_0, y_0, z_0) \ne 0$ then the equation $f(x, y, z) = c$ implicitly defines a surfaces $z = g(x, y)$ in a neighbourhood of $(x_0, y_0, z_0)$ if the following hold:
	\begin{enumerate}
		\item $f(x, y, g(x, y)) = c$ with $g(x_0, y_0) = z_0$
		\item $\diffp{g}{x} = \frac{-\diffp{f}{x} (x_0, y_0, z_0)}{\diffp{f}{z} (x_0, y_0, z_0)}$
		\item $\diffp{g}{y} = \frac{-\diffp{f}{y} (x_0, y_0, z_0)}{\diffp{f}{z} (x_0, y_0, z_0)}$
	\end{enumerate}
\end{theorem}

As in the IFT for curves, 2. and 3. must hold for $g(x, y)$ if it exists, since $f(x, y, g(x, y)) = c$ for all $(x, y)$ in some neighbourhood of $(x_0, y_0)$. If we partially differentiate with respect to $x$ and use the chain rule:

\[0 = \diffp{f}{x} (x_0, y_0, z_0) = \diffp{x}{x} \diffp{f}{x} + \diffp{y}{x}\diffp{f}{y} + \diffp{g}{x}\diffp{f}{z}\]

\[= \diffp{f}{x} + \diffp{g}{x}\diffp{f}{z}\]

Recall: $\underline{\nabla} f$ at $(x_0, y_0, z_0)$ is normal to the tangent plane of the surface $z = g(x, y)$ at $(x_0, y_0)$, so the normal line is given in parametric form as $\underline{x}(t) = x_0 + t \underline{\nabla} f$ and the tangent plane is given by $(\underline{x} - x_0) . \underline{\nabla} f = 0$.

\section{Differentiability of Vector Fields}

\subsection{Diffentiable maps from $\mathbb{R}^n$ to $\mathbb{R}^n$}

\begin{definition}
	For a vector field $\underline{F}(\underline{x}): U \rightarrow \mathbb{R}^n$, with $U \subseteq \mathbb{R}^n$ open, $F$ is differentiable at $\underline{a} \in U$ if there is a linear function $\underline{L}: \mathbb{R}^n \rightarrow \mathbb{R}^n$ such that $\underline{F}(\underline{a} + \underline{h}) - \underline{F}(\underline{a}) = \underline{L}(\underline{h}) + \underline{R}(\underline{h})$ with $\lim_{\underline{h} \rightarrow \underline{0}} \frac{\underline{R}(\underline{h})}{|\underline{h}|} = \underline{0}$.

	Now linear maps $\mathbb{R}^n \rightarrow \mathbb{R}^n$ are given by matrices. To see what matrix, use standard basis $\underline{F}(\underline{x}) = F_1(\underline{x}) \underline{e_1} + \cdots + F_n(\underline{x}) \underline{e_n}$, $\underline{L}(\underline{\underline{h}}) = L_1(h) \underline{e_1} + \cdots + L_n(\underline{h}) \underline{e_n}$, $\underline{R}(\underline{\underline{h}}) = R_1(\underline{h}) \underline{e_1} + \cdots + R_n(\underline{h}) \underline{e_n}$

	So the $j$th component of A and B is $F_j(\underline{a + h}) - F_j(\underline{a}) = L_j(\underline{h}) + R_j(\underline{h})$ with $\lim_{\underline{h} \rightarrow \underline{0}} \frac{\underline{R}(\underline{h})}{|\underline{h}|} = \underline{0}$

	So $L_j(\underline{h}) = \underline{h} \cdot \underline{\nabla} F_j(\underline{a}) = h_1 \diffp{F_j}{x_1} + \cdots + h_n \diffp{F_j}{x_n}$

	So $L$ as a column vector is the matrix product $J \underline{h}$ where $J_{i, j} = \diffp{F_i}{x_j}$. $J$ is the Jacobian matrix or the differential of $\underline{F}(\underline{x})$ at $\underline{x} = \underline{a}$. It is written as $D\underline{F}(\underline{a})$.
\end{definition}

\begin{definition}
	The determinant of the differential $\det (D\underline{V}) = |D\underline{V}|$ is called the Jacobian of $\underline{V}$, $J(\underline{V})$
\end{definition}

\subsection{Diffeomorphisms and the inverse function theorem}

We can think of a vector field $\underline{V}(\underline{x}): \mathbb{R}^n \rightarrow \mathbb{R}^n$ as a coordinate transformation on $\mathbb{R}^n$.

If we think of the components of $\underline{h}$ as coordinates of $\underline{x} = \underline{a} + \underline{h}$ relative to an origin $\underline{a}$, the components of $\underline{V}(\underline{a} + \underline{h}) - \underline{V}(\underline{a})$ are the transformed coordinates of $\underline{x}$ relative to the transformed origin $\underline{V}(\underline{a})$.

\begin{theorem}
	(Inverse function theorem) Let $\underline{v}: U \rightarrow \mathbb{R}^n$, $U \subseteq \mathbb{R}^n$ open, be differentiable with continuous partial derivatives, and let $\underline{a} \in U$. Then if $J(\underline{v}(\underline{a})) \ne 0$, for some open set $\tilde{u=U} \subseteq U$ containing $\underline{a}$:

	\begin{enumerate}
		\item $\underline{v}(\tilde{U})$ is open.
		\item the mapping $\underline{v}$ from $\tilde{U}$ to $\underline{v}(\tilde{U})$ has a differentiable inverse, i.e. there exists a differentiable $\underline{w}: \underline{v}(\tilde{u}) \rightarrow \mathbb{R}^n$ such that $\underline{w}(\underline(v)(\underline{x})) = \underline{x}$ and $\underline{v}(\underline{w}(\underline{y})) = y$.
	\end{enumerate}
\end{theorem}

\begin{definition}
	A mapping $\underline{v}: \tilde{U} \rightarrow V \subset \mathbb{R}^n$ satisfying 1. and 2. is called a diffeomorphism of $\tilde{u}$ onto $\tilde{v} = \underline{v}(\tilde{U})$. We say $\tilde{U}$ and $\tilde{V}$ are diffeomorphic.

	More generally, a mapping $\underline{v}: U \rightarrow V$ is a \textbf{local diffeomorphism} if for every point $\underline{a} \in U$, there is an open set $\tilde{U} \subseteq U$ containing $\underline{a}$ such that $\underline{v}: \tilde{U} \rightarrow \underline{v}(\tilde{U})$ is a diffeomorphism.
\end{definition}

\begin{proof}
	Let $\underline{v}: U \rightarrow V \subseteq \mathbb{R}^n$, $\underline{w}: V \rightarrow W \subseteq \mathbb{R}^n$, $u, v, w$ open in $\mathbb{R}^n$ and $\underline{v}, \underline{w}$ differentiable.

	Then $\underline{w}(\underline{v}(\underline{x}))$ is a map $U \rightarrow W \subseteq \mathbb{R}^n$ and its differential can be calculated using the chain rule: $D \underline{w}(\underline{v}(\underline{x})) = D \underline{w}(\underline(v)) D \underline{v}(\underline{x})$.

	In the particular case when $\underline{v}$ is a local diffeomorphism and $\underline{w}$ is its inverse $\underline{w}(\underline{v}(\underline{x})) = \underline{x}$, $D \underline{w} D \underline{v} = D \underline{w}(\underline{v}(\underline{x})) = D \underline{x}(\underline{x}) = I_n$.

	Similarly, $\underline{v}(\underline{w}(\underline{y})) = \underline{y}$ so $D \underline{v} D \underline{w} = D \underline{v}(\underline{w}(\underline{y})) = I_n$.

	So $D \underline{v}$ is invertible with inverse ${(D \underline{v})}^{-1} = D \underline{w}$ and by taking determinants, $J(\underline{w}) = 1 / J(\underline{v})$ and $J(\underline{v}) \ne 0$.
\end{proof}

\begin{definition}
	Such a $\underline{v}$ is \textbf{orientation-preserving} if $J(\underline{v}) > 0$ and \textbf{orientation-reversing} if $J(\underline{v}) < 0$.
\end{definition}

\section{Volume, Line and Surface Integrals}

\subsection{Fubini's theorem}

Given a scalar field on $\mathbb{R}^2$, $f(x, y): \mathbb{R}^2 \rightarrow \mathbb{R}$ which is continuous on $R \subset \mathbb{R}^2$, then the double integral $\int_R f(x, y) dA$ (or equivalently $\int \int_R f(x, y) dA$) is defined by partitioning $R$ into smaller areas $\Delta A_k$ and then defining the integral as the limit of the Riemann sum (which should be independent of the partition):

\[\int_R f(x, y) dA = \lim_{N \rightarrow \infty} \sum_{k = 1}^N f(x_k^*, y_k^*) \Delta A_k\] where $(x_k^*, y_k^*)$ lies in the base of the $k$th region.

If we choose the small areas $\Delta A_k$ to be rectangles on a rectangular grid, then $\Delta A_k = \Delta x_i \Delta y_j$ where $\Delta x_i = x_{i + 1} - x_i$ and $\Delta y_i = y_{i + 1} - y_i$ and $x$ and $y$ are partitioned as $x_0 < x_1 < \cdots < x_n$ and $x_0 < x_1 < \cdots < x_n$.

We then get

\[\int_R f(x, y) dA = \lim_{n \rightarrow \infty, m \rightarrow \infty} \sum_{i = 0}^n \sum_{j = 0}^m f(x_i^*, y_j^*) \Delta x_i \Delta y_j\]

where $x_i^* \in [x_i, x_{i + 1}]$ and $y_i^* \in [y_i, y_{i + 1}]$.

If we assume we can take the limit as $m \rightarrow \infty$ first and the limit as $n \rightarrow \infty$ afterwards, we get

\[\lim_{n \rightarrow \infty, m \rightarrow \infty} \sum_{i = 0}^n \sum_{j = 0}^m f(x_i^*, y_j^*) \Delta x_i \Delta y_j = \lim_{n \rightarrow \infty} \sum_{i = 0}^n (\lim_{m \rightarrow \infty} \sum_{j = 0}^m f(x_i^*, y_j^*) \Delta y_j) \Delta y_j\]

\[= \lim_{n \rightarrow \infty} \sum_{i = 0}^n (\int_y f(x_i^*, y) dy) \Delta x_i = \int_x \int_y f(x, y) dy dx\]

If we take $n \rightarrow \infty$ first, we exchange the order of integration:

\[\int_R f(x, y) dA = \int_y \int_x f(x, y) dx dy\]

\begin{theorem}
	(Fubini's theorem): If $f(x, y)$ is continuous on a closed and bounded region of $\mathbb{R}^2$ (a region of integration $R$), then the double integral $\int_R f(x, y) dA$ can be written as an iterated integral, with the integrals in either order:

	\[\int_R f(x, y) dA = \int_y \int_x f(x, y) dx dy = \int_x \int_y f(x, y) dy dx\]
\end{theorem}

\begin{remark}
	If the region and/or function is unbounded, Fubini's theorem still holds if the double integral is absolutely convergent, i.e. if the integral of the $|f(x, y)|$ is finite. If this is not the case, Fubini's theorem doesn't necessarily hold.
\end{remark}

\begin{remark}
	If the region $R$ is not rectangular, it is more complicated.
	
	e.g. if $R = \{(x, y) \in \mathbb{R}^2: a \le x \le b, y_0(x) \le y \le y_1(x)\}$, then
	
	\[\int_R f(x, y) dA = \int_a^b \int_{y_0(x)}^{y_1(x)} f(x, y) dy dx\]

	If $f(x, y)$ is continuous then by Fubini's theorem we can change the order of integration. To calculate the integral over $x$ first in this case, we need to split $R$ into sub-regions where I can write the $x$-limits as functions of $y$.
\end{remark}

\subsection{Line integrals}

\begin{definition}
	A \textbf{regular arc} $C \subset \mathbb{R}^n$ is a parameterised curve $\underline{x}(t)$ whose cartesian components $x_a(t)$, $a \in \{1, \dots, n\}$ are continuous with continuous first derivative, where $t$ lies in some (possibly infinite) interval.	
\end{definition}

\begin{definition}
	A \textbf{regular curve} consists of a finite number of regular arcs joined end-to-end.
\end{definition}

Given $\underline{v}(\underline{x}): \mathbb{R}^n \rightarrow \mathbb{R}^n$ its restriction to a regular arc $\underline{v}(\underline{x}(t))$ is a vector function of $t$ and its scalar product with the tangent vector $\diff{\underline{x}(t)}{t}$ is a scalar function of $t$.

We can therefore integrate it along the arc to get a real number, this is called the \textbf{line integral} of $\underline{v}$ along the arc $C: t \rightarrow \underline{x}(t)$ between $t = \alpha$ and $t = \beta$.

\[\int_C \underline{v} d\underline{x} = \int_{\alpha}^{\beta} \underline{v}(\underline{x}(t)) \cdot \diff{\underline{x}}{t} dt\]

This is independent of the choice of parameterisation. This can be proven using the chain rule:

let $t = t(u)$, then $dt = \diff{t}{u} du$.

\[\int_{t^{-1}(\alpha)}^{t^{-1}(\beta)} \underline{v}(\underline{x}(t(u))) \cdot \diff{\underline{x(t(u))}}{u} du = \int_{t^{-1}(\alpha)}^{t^{-1}(\beta)} \underline{v}(\underline{x}(t(u))) \cdot \diff{\underline{x}}{t} \diff{t}{u} du = \int_{\alpha}^{\beta} \underline{v}(\underline{x}(t)) \cdot \diff{\underline{x}}{t} dt\]

If $C$ is a regular curve, made up of regular arcs, the line integral $\int_C \underline{v} d\underline{x}$ is the sum of the line integral over the arcs.

If the integral is calculated over a regular closed curve (the endpoints join), then it is often written as

\[\oint_C \underline{v} d\underline{x}\]

\subsection{Surface integrals I: defining a surface}


Given a 2D surface $S \subset \mathbb{R}^3$, a 3D vector field can be integrated over the surface $S$ to give a double integral analogue of the line integral.

There are two methods for specifying the surface:

\begin{enumerate}
	\item Give the surface in parametric form $\underline{x}(u, v)$ where the real parameters $u, v$ lie in some region $U \subseteq \mathbb{R}^2$ called the parameter domain.
	
	In general, $\diffp{\underline{x}}{u} \times \diffp{\underline{x}}{v}$ is a normal vector to $S$ at $\underline{x}(u, v)$, and so
	
	\[ \hat{\underline{n}} = \left( \diffp{\underline{x}}{u} \times \diffp{\underline{x}}{v} \right) / \left| \diffp{\underline{x}}{u} \times \diffp{\underline{x}}{v} \right| \] is a unit normal at $\underline{x}(u, v)$.

	Note: if $u$ and $v$ are swapped we get a unit normal to $S$, but one pointing in the opposite direction.
	\item Express the surface as (part of) a level surface of a scalar field $f$, i.e. given implicity as $f(x, y, z) = c$. Then $\underline{\nabla}f$ is normal to the level surface $S$, and $\hat{\underline{n}} = \frac{\underline{\nabla}f}{|\underline{\nabla}f|}$ is a unit normal.
	
	Note: as with method 1, the negative of the unit normal found is also a valid unit normal.
\end{enumerate}

\subsection{Surface integrals II: evaluating the integral}

\begin{definition}
	We define the surface integral as a Riemann sum. Let $\underline{F}(\underline{x}): \mathbb{R}^3 \rightarrow \mathbb{R}^3$ be a vector field. Let $S \subset \mathbb{R}^3$ be a surface with parameterised position vector $\underline{x}(u, v)$ with $u, v \in U$, $U$ is the parameter domain.

	Assume that the partial derivatives of $\underline{x}$ exist and are continuous and the unit normal $\hat{\underline{n}}(u, v)$ is continuous (so $S$ is orientable).

	The surface integral is defined as

	\[ \int_S \underline{F} d\underline{A} = \lim_{\Delta A_k \rightarrow 0} \sum_k \underline{F} (\underline{x}_{k}^*) \cdot \underline{\hat{n}}_k \Delta A_k \]
	Methods for computing:

	\begin{enumerate}
		\item The surface $S$ given parametrically as $\underline{x}(u, v)$, construct $\Delta A_k$ by approximating as parallelograms, by partioning $S$ using lines of constant $u$ and $v$.
		
		Let $A_k$ be the area element with vertices $\underline(u_i, v_j), \underline{x}(u_i + \Delta u_i, v_j), \underline{x}(u_i, v_j + \Delta v_j), \underline{x}(u_i + \Delta u_i, v_j + \Delta v_j)$. Then $\underline{\hat{n}}_k \Delta A_k = (\underline{x}(u_i + \Delta u_i, v_j)) - \underline{x}(u_i, v_j)) \times (\underline{x}(u_i, v_j + \Delta v_j) - \underline{x}(u_i, v_j) \approx \Delta u_i \diffp{\underline{x}}{u} \times \Delta v_j \diffp{\underline{x}}{v}$.

		Substituting this into the surface integral,

		\[ \int_S \underline{F} d\underline{A} = \lim_{\Delta u_i, \Delta v_j \rightarrow 0} \sum_{i, j} \underline{F}(\underline{x}_{i, j}^*) \cdot \left( \diffp{\underline{x}}{u} \times \diffp{\underline{x}}{v} \right) \Delta u_i \Delta v_j \]
		Taking the limit, this becomes

		\[ \int_S \underline{F} d\underline{A} = \int_u \underline{F}(\underline{x}(u, v)) \cdot \left( \diffp{\underline{x}}{u} \times \diffp{\underline{x}}{v} \right) du dv \]

		\item Let $S$ be given as (part of) a level set of a scalar field $f(x, y, z)$ and assume $\diffp{f}{z} \ne 0$ on $S$. Then by the implicit function theorem, the points of $S$ can be written as $(x, y, g(x, y))$ for some differentiable function $g$, where $(x, y)$ range over some region $A$ of the $x, y$ plane ($A$ is the projection of $S$ onto the $x, y$ plane).
		
		We can then apply method 1, with $u = x$ and $v = y$. So $\underline{x}(x, y) = x \underline{e_1} + y \underline{e_2} + g(x, y) \underline{e_3}$, and $\diffp{\underline{x}}{x} = \underline{e_1} + \diffp{g}{x} \underline{e_3}$, $\diffp{\underline{x}}{y} = \underline{e_2} + \diffp{g}{y} \underline{e_3}$.

		Using the implicit function theorem, noting that $f(x, y, g(x, y))$ is constant, $0 = \diffp{F}{x} = \diffp{f}{x} + \diffp{f}{z} \diffp{g}{x} \Rightarrow \diffp{g}{x} = \frac{-\partial f / \partial x}{\partial f / \partial z}$ and similarly, $\diffp{g}{y} = \frac{-\partial f / \partial y}{\partial f / \partial z}$.

		So $\diffp{\underline{x}}{x} \times \diffp{\underline{x}}{y} = \frac{\underline{\nabla}f}{\underline{e_3} \cdot \underline{\nabla}f}$. Then

		\[ \int_S \underline{F} d\underline{A} = \int_A \frac{\underline{F} \cdot \underline{\nabla}f}{\underline{e_3} \cdot \underline{\nabla}f} dx dy \]
	\end{enumerate}
\end{definition}

\begin{remark}
	In the formula above, the $z$ component of the normal $= 1$ so this corresponds to the upwards (positive $z$ component) choice of normal. If we wanted the downward direction instead, we simply negate the formula.

	If $\diffp{f}{x} \ne 0$ we can project onto the $y, z$ plane. Similarly, if $\diffp{f}{y} \ne 0$ we can project onto the $x, y$ plane.
\end{remark}

\section{Green's, Stokes' and divergence theorems}

\subsection{The Big 3 theorems}

\begin{theorem}
	(\textbf{Green's theorem}) Let $P(x, y): \mathbb{R}^2 \rightarrow \mathbb{R}$ and $Q(x, y): \mathbb{R}^2 \rightarrow \mathbb{R}$ be continuously differentiable scalar fields on $\mathbb{R}^2$, and let $c$ be a simple closed curve traversed in anti-clockwise direction (the positive direction) which is the boundary of a region $A$. Then
	
	\[\oint_C (P(x, y) dx + Q(x, y) dy) = \int_A \left( \diffp{Q}{x} - \diffp{P}{y} \right) dx dy \]
	We can also write this in vector form by embedding the $xy$-plane into $\mathbb{R}^3$ as the $z = 0$ plane and setting

	\[ \underline{F}(x, y, z) = (P(x, y), Q(x, y), R) \]
	the Green's theorem can be written as

	\[ \oint_C \underline{F} \cdot d\underline{x} = \int_A \left( \underline{\nabla} \times \underline{F} \right) \cdot \underline{e}_3 dA \]
\end{theorem}

\begin{proof}
	Not examinable
\end{proof}

\begin{theorem}
	(\textbf{Stokes' theorem} - this generalises the vector form of Green's theorem to arbitrary surfaces in $\mathbb{R}^3$) Let $\underline{F}(x, y, z): \mathbb{R}^3 \rightarrow \mathbb{R}^3$ and let $S$ be a surface in $\mathbb{R}^3$ with area elements $d\underline{A} = \underline{\hat{n}} dA$ and let $C = \delta S$ be the boundary of $S$. Then

	\[ \oint_C \underline{F} \cdot d\underline{x} = \int_S \left( \underline{\nabla} \times \underline{F} \right) \cdot dA \]
	We need to ensure that the orientations of $S$ and of $C = \delta S$ match, which we can do with the \textbf{right-hand rule}:

	Curl the fingers of your right hand and extend your thumb. If you placed your hand on the surface near the boundary with your thumb pointing in the direction of the surface normal, then your fingers curl in the direction of the orientation of the boundary.
\end{theorem}

\begin{proof}
	Not examinable
\end{proof}

\begin{theorem}
	(\textbf{The divergence theorem}) Let $V \subset \mathbb{R}^3$ be a volume bounded by $S$ and $\underline{F}: V \rightarrow \mathbb{R}^3$ be continuously differentiable. Then

	\[ \int_S \underline{F} \cdot d\underline{A} = \int_V \left( \underline{\nabla} \cdot \underline{F} \right) dV \]
	where $d\underline{A} = \underline{\hat{n}} dA$ where $\underline{\hat{n}}$ is the outward unit normal
\end{theorem}

\begin{proof}
	Not examinable
\end{proof}

\begin{remark}
	These three theorems can be seen as higher dimensional analogues of the fundamental theorem of calculus:

	\[ \int_a^b \diff{f}{x} dx = f(b) - f(a) \]
\end{remark}

\subsection{Path independence of line integrals}

In general, line integrals depend on the path between the end points. However, there is a type of vector field for which the line integral is \textbf{path independent}, known as a \textbf{conservative} vector field.

\begin{example}
	Calculate the integral $\int_C \underline{F} \cdot d\underline{x}$ for $\underline{F} = (y \cos x, \sin x)$ between $(0, 0)$ and $(1, 1)$ on the paths $C_1$, the straight line from $(0, 0)$ to $(1, 1)$ and $C_2$, the stragiht line from $(0, 0)$ to $(1, 0)$ and then to $(1, 1)$.

	$C_1$ is parameterised as $\underline{x}(t) = (t, t)$ for $0 \le t \le 1$ so $\diff{\underline{x}}{t} = (1, 1)$. $\underline{F}(\underline{x}(t)) = (t \cos t, \sin t)$ so

	\[ \int_{C_1} \underline{F} \cdot d \underline{x} = \int_0^1 \underline{F}(\underline{x}(t)) \cdot \diff{\underline{x}}{t} dt = \sin(1) \]
	To calculate $\int_{C_2} \underline{F} \cdot d \underline{x}$, split $C_2$ into 2 arcs $C_{21}$ and $C_{22}$. $C_{21}$ is parameterised by $\underline{x}(t) = (t, 0)$ for $0 \le t \le 1$ and $C_{22}$ is parameterised by $\underline{x}(t) = (1, t)$ $0 \le t \le 1$. So

	\[ \int_{C_2} \underline{F} \cdot d \underline{x} = \int_{C_{21}} \underline{F} \cdot d \underline{x} + \int_{C_{22}} \underline{F} \cdot d \underline{x} = \int_0^1 \sin(t) dt = \sin(1) \]
	Which is the same result as for the integral along $C_1$. But we have only checked this for two paths, not infinitely many.
\end{example}

\begin{theorem}
	Let $\underline{F}$ be continuously differentiable on an open $D \subseteq \mathbb{R}^3$ and let $C_1$ and $C_2$ be any two paths from $\underline{a}$ to $\underline{b}$ in $D$. If $\underline{\nabla} \times \underline{F} = \underline{0}$ then
	
	\[ \int_{C_1} \underline{F} \cdot d\underline{x} = \int_{C_2} \underline{F} \cdot d\underline{x} \]
	and the line integral only depends on the end points: it is \textbf{path independent} and $F$ is \textbf{conservative}.
\end{theorem}
	
\begin{proof}
	Let

	\[ \Delta I := \int_{C_1} \underline{F} \cdot d\underline{x} - \int_{C_2} \underline{F} \cdot dx \]
	For path independence, we need $\Delta I = 0$.

	Let $C_2$ be parameterised in $t$ for $ta \le t \le tb$ and let $\overline{C_2}$ be the path along $C_2$ taken in the opposite direction.

	\[ \int_{\overline{C_2}} \underline{F} \cdot d\underline{x} = \int_{t_a}^{t_b} \underline{F}(\underline{x}(t)) \cdot \diff{\underline{x}}{t} dt = \int_{t_b}^{t_a} \underline{F}(\underline{x}(t)) \cdot \diff{\underline{x}}{t} dt = -\int_{C_2} \underline{F} \cdot d\underline{x} \]
	Then
	
	\[ \Delta I = \int_{C_1} \underline{F} \cdot d\underline{x} - \int_{C_2} \underline{F} \cdot dx = \int_{C_1} \underline{F} \cdot dx + \int_{\overline{C_2}} \underline{F} \cdot d\underline{x} = \oint_C \underline{F} \cdot d\underline{x} \]
	where $C$ is the closed path consisting of $C_1$ followed by $\overline{C_2}$. If $C$ is the boundary of a surface $S$ in $D$ then by Stoke's theorem

	\[ \Delta I = \oint_C \underline{F} \cdot d\underline{x} = \int_S (\underline{\nabla} \times \underline{F}) d\underline{A} \]
	so $\Delta I = 0$ if $\underline{\nabla} \times \underline{F} = \underline{0}$ throughout $D$, which implies path independence.

	For this to work, we need that every closed curve $C$ is the boundary of a surface in $D$, which is true if $D$ is \textbf{simply connected}, which means that any closed curve in $D$ can be continuously shrunk to a point.

	e.g. a sphere is simply connected, a torus is not simply connected.
\end{proof}

\begin{corollary}
	In a simply connected region $D$, $\underline{\nabla} \times \underline{F} = \underline{0} \Leftrightarrow \text{path independence of } \int_C \underline{F} \cdot d\underline{x} \Leftarrow \text{for some scalar field } \phi, \underline{F} = \underline{\nabla} \phi$.
\end{corollary}

\section{Non-cartesian systems}

\begin{definition}
	For a non-cartesian coordinate system in three dimensions, we use the letters $u, v, w$ to describe the coordinates.
\end{definition}

\begin{definition}
	A change of variable (change of coordinate system) is denoted by
	\[
		\begin{aligned}
			x & = g(u, v, w) \\
			y & = h(u, v, w) \\
			z & = k(u, v, w)
		\end{aligned}
	\]
	where $g, h, k$ are smooth and invertible functions, and
	\[
		\begin{aligned}
			u & = \tilde{g}(x, y, z) \\
			v & = \tilde{h}(x, y, z) \\
			w & = \tilde{k}(x, y, z)
		\end{aligned}
	\]
	where $\tilde{g}, \tilde{h}, \tilde{k}$ are smooth and invertible functions.
\end{definition}

\subsection{Change of variables in surface and volume integrals}

\subsection{Differential operators in polar and spehrical polar coordinates}

\begin{definition}
	In two dimensions, $P: (x, y)$ denotes a point $P$ of coordinates $(x, y) \in \mathbb{R}^2$. The \textbf{position vector} of $P$ is
	\[
		\underline{r} = \underline{OP} = x \underline{e_1} + y \underline{e_2}
	\]
	where $\{ \underline{e_1}, \underline{e_2} \}$ is an orthonormal basis and $\underline{e_1}$ and $\underline{e_2}$ do not depend on $x$ or $y$.
\end{definition}

\begin{definition}
	The \textbf{gradient operator} is defined as
	\[
		\underline{\nabla} = \underline{e_1} \partial_x + \underline{e_2} \partial_y
	\]
\end{definition}

\begin{definition}
	The Laplacian operator is defined as
	\[
		\begin{aligned}
			\underline{\nabla} . \underline{\nabla}
				& = (\underline{e_1} \partial_x + \underline{e_2} \partial_y) . (\underline{e_1} \partial_x + \underline{e_2} \partial_y) \\
				& = \partial_x^2 + \partial_y^2
		\end{aligned}
	\]
\end{definition}

\subsection{Polar coordinates}

\begin{definition}
	A point $P$ in polar coordinates is $P: (r, \theta)$ where $\underline{r} = r \cos (\theta) \underline{e_1} + r \sin (\theta) \underline{e_2}$, $r \in \left[0, \infty\right), \theta \in \left[ 0, 2 \pi \right)$

	$|\underline{r}| = r$ and $\partial_r \underline{r} = \cos(\theta) \underline{e_1} + \sin(\theta) \underline{e_2} =: \tilde{e_r}$, $\partial_{\theta} \underline{r} = - r \sin(\theta) \underline{e_1} + r \cos(\theta) \underline{e_2} =: \tilde{e_{\theta}}$
\end{definition}

\begin{definition}
	Let $x = g(u, v)$ and $y = h(u, v)$ be a change of variables. The norms of the partial derivatives of the position vector $\underline{r}$ are caled the \textbf{scale factors} for the mapping $(g, h)$.
\end{definition}

\begin{definition}
	$h_u = |\partial_u \underline{r}| = |\underline{e_u}|$, $h_v = |\partial_v \underline{r}| = |\underline{e_v}|$.

	So in polar coordinates, $h_r = 1, h_{\theta} = r$, $J(r, \theta) = h_r h_{\theta}$.
\end{definition}

\begin{definition}
	We define $\underline{e_r}$ and $\underline{e_{\theta}}$ to be unit vectors, so $\underline{e_r} = \cos(\theta) \underline{e_1} + \sin(\theta) \underline{e_2}$, $\underline{e_{\theta}} = -\sin(\theta) \underline{e_1} + \cos(\theta) \underline{e_2}$.
\end{definition}

\begin{definition}
	Let $f(r, \theta)$ be a scalar function.
	\[
		\begin{aligned}
			df
				& = \underline{\nabla} f . d \underline{r} \\
				& = (\partial_r f) dr + (\partial_{\theta} f) d \theta \\
				& = (\partial_r f) \underline{e_r} . d\underline{r} + (\partial_{\theta} f) \frac{1}{r} (\underline{e_{\theta}} d \underline{r}) \\
				& = (\underline{e_r} \partial_r + \underline{e_{\theta}} \frac{1}{r} \partial_{\theta} f) . d\underline{r}
		\end{aligned}
	\]
	So $\underline{\nabla} = \underline{e_r} \partial_r + \frac{1}{r} \underline{e_{\theta}} \partial_{\theta}$
\end{definition}

\begin{definition}
	(\textbf{Divergence in polar}) Let $\underline{A}(r, \theta) = A_r \underline{e_r} + A_{\theta} \underline{e_{\theta}}$, where $A_r$ and $A_{\theta}$ are functions of $r$ and $\theta$.

	Note that $\partial_r \underline{e_r} = 0, \partial_r \underline{e_{\theta}} = 0, \partial_{\theta} \underline{e_r} = \underline{e_{\theta}}, \partial_{\theta} \underline{e_{\theta}} = -\underline{e_r}$. Using these results, we get
	\[
		\underline{\nabla} . \underline{A} = \partial_r A_r + \frac{1}{r} A_r + \frac{1}{r} \partial_{\theta} A_{\theta}
	\]
\end{definition}

\begin{definition}
	The Laplacian for polar coordinates is
	\[
		\underline{\nabla} . \underline{\nabla} = \partial_r^2 + \frac{1}{r} \partial_r + \frac{1}{r^2} \partial_{\theta}^2
	\]
\end{definition}

\begin{proposition}
	Let $x = g(u, v)$, $y = h(u, v)$ be a change of variables. The Jacobian matrix $A = [[\partial_u x, \partial_v x], [\partial_u y, \partial_v y]]$ is the inverse of the matrix $B = [[\partial_x u, \partial_y u], [\partial_x v, \partial_y v]]$. TODO: write as matrices.
\end{proposition}

\begin{proof}
	$dx = \partial_u x du + \partial_v x dv$, $dv = \partial_x v dx + \partial_y v dy$, $dy = \partial_u y du + \partial_v y dv$, $du = \partial_x u dx + \partial_y u dy$.

	Substituting the second equation into the first:
	\[
		dx = \partial_u x du + \partial_v x (\partial_x v dx + \partial_y v dy)
	\]
	If $x$ is constant, then $0 = \partial_u x du + \partial_v x \partial_y v dy$ or $0 = \partial_u x du \partial_y u + \partial_v x \partial_y dv$.

	If $y$ is constant, then $dx = \partial_u x du + \partial_v x \partial_x v dx$ or $1 = \partial_u x \partial_x u + \partial_v x \partial_x v$.

	Now substituting the fourth euqation into the third:
	If $x$ is constant, $1 = \partial_u y \partial_y u + \partial_v y \partial_y v$, if $y$ is constant, $0 = \partial_u y \partial_x u + \partial_v y \partial_x v$.

	Multiplying $A$ and $B$ gives $AB = I$, the identity matrix.
\end{proof}

\section{Generalised functions}

\subsection{Birth of generalised functions}

\begin{definition}
	The \textbf{unit step function} is defined as
	\[
		\Theta (t - t_0) = \begin{cases}
			1 & \text{ if } t > t_0 \\
			0 & \text{ if } t \le t_0
		\end{cases}
	\]
\end{definition}

\begin{definition}
	The \textbf{delta function} is defined as the derivative of the unit step function:
	\[
		\delta(t) = \Theta'(t)
	\]
\end{definition}

\subsection{Test functions and distributions}

\begin{definition}
	A \textbf{topological space} $X$ is a set of points with a set of neighbourhoods obeying certain axioms.
\end{definition}

\begin{definition}
	An \textbf{open set} $O \subset X$ is a set which is a neighbourhood of all its points.
\end{definition}

\begin{definition}
	A set $B$ is a \textbf{closed set} if $X \backslash B$ is an open set.
\end{definition}

\begin{definition}
	For $A \subset X$, $p$ is a \textbf{limit point} of $A$ is every neighbourhood of $p$ contains at least one element of $A - \{ p \}$.
\end{definition}

\begin{definition}
	The \textbf{closure} of a set $A$ is the union of $A$ and all its limit points.
\end{definition}

\begin{example}
	In $A = \left[0, 1 \right)$, $1$ is a limit point.
\end{example}

\begin{definition}
	The \textbf{support} of $\Psi$, denoted $\text{supp} \Psi$, is defined as
	\[
		\text{supp} \Psi := \overline{\{ x \in \Omega: \Psi(x) \ne 0 \}}
	\]
	is the closure of the set of points $\{ x \in \Omega: \Psi(x) \ne 0 \}$.
\end{definition}

\begin{definition}
	Let $\Omega \subset \mathbb{R}^n$ be an open set. $\Psi: \mathbb{R}^n \rightarrow \mathbb{R} (\text{ or } \mathbb{C})$ is a \textbf{test function} if it satisfies:
	\begin{enumerate}
		\item $\Psi \in C^{\infty} (\Omega)$ i.e. $\Psi$ has finite derivatives of all orders on $\Omega$.
		\item $\text{supp} \Psi$ is compact.
	\end{enumerate}
\end{definition}

\begin{definition}
	The \textbf{space of test functions} is a vector space denoted $\mathcal{D}(\Omega)$ when the test functions are defined on $\Omega$. $\mathcal{D}(\Omega)$ is infinite dimensional, its elements are functions.
\end{definition}

\begin{proposition}
	If $\Psi \in \Omega(\mathbb{R}^n)$, then
	\begin{enumerate}
		\item $\Psi(\underline{x} + \underline{\xi}) \in \mathcal{D} (\mathbb{R}^n)$ for $\xi \in \mathbb{R}^n$.
		\item $\Psi(-\underline{x}) \in \mathcal{D}(\mathbb{R}^n)$ and $\Psi(a \underline{x}) \in \mathcal{D}(\mathbb{R}^n)$ for $a \in \mathbb{R} - \{ 0 \}$.
		\item $g(x) \Psi(x) \in \mathcal{D}(\mathbb{R}^n)$ for $g \in C^{\infty} (\mathbb{R}^n)$.
	\end{enumerate}
\end{proposition}

\begin{remark}
	For a vector space $V$ with norm $|| . ||$, a sequence of functions ${(f_m)}_{m \in \mathbb{N}}$ converges to a limit function $f$ if
	\[
		\lim_{m \rightarrow \infty} || f_m - f || = 0
	\]
	This is pointwise convergence. This requires $f_n(x)$ and $f(x)$ to be close for all $m > M_x$, but depending on the point $x$.
\end{remark}

\begin{definition}
	A sequence of test functions ${(\Psi_m)}_{m \in \mathbb{N}} \subseteq \mathcal{D}(\Omega)$ converges to a test function $\Psi \in \mathcal{D}(\Omega)$ as $m \rightarrow \infty$ if
	\begin{enumerate}
		\item For some compact set $K \subset \Omega$, $\text{supp} \Psi_m \subset K \quad \forall m \in \mathbb{N}$.
		\item ${(\Psi_m)}_{m \in \mathbb{N}} \rightarrow \Psi$ with uniform convergence.
		\item \[
			{(D_k \Psi_m)} := \left( \diffp[k_1]{}{x_1} \diffp[k_2]{}{x_2} \cdots \diffp[k_n]{}{x_n} \right) \Psi_m \rightarrow D^k \text{ as } m \rightarrow \infty
		\]
		with uniform convergence. We need uniform convergence of $D^k \Psi_m$ because the derivatives of all orders of test functions are also test functions.
	\end{enumerate}
	These criteria are called $\mathcal{D}$-convergence.
\end{definition}

\begin{definition}
	We can summarise the notion of $\mathcal{D}$-convergence through the use of the \textbf{sup norm}. It is defined as
	\[
		||f||_{\infty} := \sup \{ |f(x)|: x \in \Omega \}
	\]
	Here, the supremum is the maximum since $K$ is bounded.
\end{definition}

\begin{definition}
	The sequence ${(\Psi_m)}$ of test functions in $\mathcal{D}(\Omega)$ converges to $\Psi \in \mathcal{D} (\Omega)$ if
	\begin{enumerate}
		\item For some compact $K \subset \Omega$, $\text{supp} \Psi_m \subset K \quad \forall m$.
		\item For all multi-indices $k = (k_1, \dots, k_n)$,
		\[
			|| D^k - \Psi_m - D^k \Psi ||_{\infty} \rightarrow 0 \text{ as } m \rightarrow \infty
		\]
	\end{enumerate}
\end{definition}

\begin{definition}
	Let $\Omega \subset \mathbb{R}^n$ be an open set. A \textbf{distribution} is linear continuous map $T: \mathcal{D}(\Omega) \rightarrow \mathbb{R}$. It satisfies
	\begin{enumerate}
		\item \textbf{Linearity}: $\forall \Psi \in \mathcal{D}(\Omega), \forall \phi \in \mathcal{D}(\Omega), \forall a, b \in \mathbb{R}^n$,
		\[
			T(a \Psi + b \phi) = a T(\Psi) + b T(\phi)
		\]
		\item \textbf{Continuity}: $\forall \Psi \in \mathcal{D}(\Omega), \forall {(\Psi_m)}_{m \in \mathbb{N}} \subseteq \mathcal{D}(\Omega)$ with $(\Psi_m) \rightarrow \Psi$,
		\[
			T(\Psi_m) \rightarrow T(\Psi) \text{ as } m \rightarrow \infty
		\]
	\end{enumerate}
\end{definition}

\begin{definition}
	The vector space of distributions defined via test functions is denoted by $\mathcal{D}'(\Omega)$ and is also infinite-dimensional.
\end{definition}

\begin{example}
	The Dirac delta function is a distribution
	\[
		\delta: \mathcal{D}(\mathbb{R}^n) \rightarrow \mathbb{R}, \delta(\Psi) := \Psi(\underline{0})
	\]
\end{example}

\begin{proof}
	\hfill
	\begin{enumerate}
		\item Linearity: $\delta(a \Psi + b \phi) = (a \Psi + b \phi) (\underline{0}) = a \Psi(\underline{0}) + b \phi(\underline{0}) = a \delta (\Psi) + b \delta(\phi)$.
		\item Continuity: $\forall \Psi \in \mathcal{\Omega}, \forall {(\Psi_m)}_{m \in \mathbb{N}} \subseteq \mathcal{D}(\Omega)$ with $(\Psi_m) \rightarrow \Psi$, but uniform convergence in $\mathcal{D}$-convergence implies pointwise convergence. So $(\Psi_m(\underline{x})) \rightarrow \Psi(\underline{x}) \text{ as } m \rightarrow \infty \quad \forall \underline{x} \in \mathbb{R}^n$. Since $\underline{0} \in \mathbb{R}^n$,
		\[
			\delta(\Psi_m) = (\Psi_m (\underline{0})) \rightarrow \Psi (\underline{0}) = \delta (\Psi) \text{ as } m \rightarrow \infty
		\]
	\end{enumerate}
\end{proof}

\begin{example}
	Continuous functions on $\mathbb{R}^n$ are distributions. Let $f \in C^{0} (\mathbb{R})$. To treat $f$ as a distribution, we must define how this distribution acts on $\mathcal{D}(\Omega)$.
	
	Define $T_f: \mathcal{D}(\mathbb{R}^n) \rightarrow \mathbb{R}$,
	\[
		T_f (\psi) = \int_{\mathbb{R}^n} f(\underline{x}) \psi(\underline{x}) d \underline{x}
	\]
	Then $T_f$ is a distribution.
	\begin{itemize}
		\item Linearity:
		\[
			\begin{aligned}
				T_f (a \psi + b \phi)
					& = \int_{\mathbb{R}^n} f(\underline{x}) (a \psi(\underline{x}) + b \phi (\underline{x})) d \underline{x} \\
					& = a \int_{\mathbb{R}^n} f(\underline{x}) \psi(\underline{x}) d \underline{x} + b \int_{\mathbb{R}^n} f(\underline{x}) \phi(\underline{x}) d \underline{x} \\
					& = a T_f(\psi) + b T_f (\phi)
			\end{aligned}
		\]
		\item Continuity: $\forall \psi \in \mathcal{D}(\mathbb{R}^n), \forall {(\psi_m)}_{m \in \mathbb{N}} \subseteq \mathcal{D}(\mathbb{R}^n)$ with $(\psi_m) \rightarrow \psi$, we have in particular uniform convergence. Therefore we can interchange $\lim$ and $\int$, so
		\[
			\begin{aligned}
			\lim_{m \rightarrow \infty} T_f (\psi_m)
				& = \lim_{m \rightarrow \infty} \int_{\mathbb{R}^n} f(\underline{x}) \psi_m(\underline{x}) d\underline{x} \\
				& = \int_{\mathbb{R}^n} \lim_{m \rightarrow \infty} f(\underline{x}) \psi_m(\underline{x}) d\underline{x} \\
				& = \int_{\mathbb{R}^n} f(\underline{x}) \psi(\underline{x}) d\underline{x} \\
				& = T_f(\psi)
			\end{aligned}
		\]
		So
		\[
			T_{f_m} (\psi) = \int_{\mathbb{R}^n} f_m(\underline{x}) \psi(\underline{x}) d\underline{x} \rightarrow \psi(0) \int_{\mathbb{R}^n} f_m(\underline{x}) d\underline{x} = \psi(\underline{0}) \quad \text{as } m \rightarrow \infty
		\]
	\end{itemize}
\end{example}

\subsection{Regular and singular distributions}

\begin{definition}
	A function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is called \textbf{locally integrable} if for every compact set $K \subset \mathbb{R}^n$,
	\[
		\int_K |f(\underline{x})| d\underline{x} < \infty
	\]
\end{definition}

\begin{example}
	$f(x) = x^2$ is not integrable since
	\[
		\int_{\mathbb{R}} f(x) dx
	\]
	is not finite but it is locally integrable.
\end{example}

\begin{definition}
	The space of locally integrable functions is called $L_{\text{loc}}^1$.
\end{definition}

\begin{definition}
	$T \in \mathcal{D}'(\mathbb{R}^n)$ is called a \textbf{regular distribution} if for some locally integrable function $f$,
	\[
		T(\psi) = T_f(\psi) = \int_{\mathbb{R}^n} f(\underline{x}) \psi(\underline{x}) d\underline{x}, \quad \psi \in \mathcal{D}(\mathbb{R}^n)
	\]
	In general, if two locally integrable functions only differ by a finite amount at isolated points, they define the same distribution.
\end{definition}

\begin{example}
	Let $f: \mathbb{R} \rightarrow \mathbb{R}$,
	\[
		f(x) = \begin{cases}
			1 & \text{ if } x = 0 \\
			0 & \text{ otherwise}
		\end{cases}
	\]
	then $f \in L_{\text{loc}}^1 (\mathbb{R})$, and
	\[
		T_f(\psi) = \int_{\mathbb{R}} f(x) \psi(x) dx = 0
	\]
\end{example}

\begin{definition}
	If there is no $f \in L_{\text{loc}}^1 (\mathbb{R}^n)$ such that a distribution $T$ can be written as $T_f$, then this distribution is called \textbf{singular}. We write
	\[
		T(\psi) = \int_{\mathbb{R}^n} T(\underline{x}) \psi(\underline{x}) d\underline{x} = \langle T, \psi \rangle
	\]
	To specify the distribution $T$, we must define how it acts on test functions.
\end{definition}

\begin{example}
	$\delta$ is not a regular distribution: there is no $f \in L_{\text{loc}}^1$ such that
	\[
		\delta(\psi) = \int_{\mathbb{R}^n} f(\underline{x}) \psi(\underline{x}) d\underline{x}
	\]
	However, we can write symbolically
	\[
		\delta(\psi) = \int_{\mathbb{R}^n} \delta(\underline{x}) \psi(\underline{x}) d\underline{x} = \langle \delta, \psi \rangle := \psi(\underline{0})
	\]
	This is called the \textbf{sifting property} of the $\delta$ distribution.
\end{example}

\begin{remark}
	\textbf{Important:} $\delta(\underline{x})$ is \textbf{not} a function, we just use it as a function in the integral symbolically.
\end{remark}

\begin{remark}
	More generally, with $\Omega \subset \mathbb{R}^n$,
	\[
		\delta(\psi) = \int_{\mathbb{R}^n} \delta(\underline{x}) \psi(\underline{x}) d\underline{x} = \langle \delta, \psi \rangle = \begin{cases}
			\psi(\underline{0}) & \text{ if } \underline{0} \in \Omega \\
			0 & \text{ otherwise}
		\end{cases}
	\]
\end{remark}

\begin{definition}
	In one dimension, we write $\delta(\underline{x}) = \delta(\underline{x})$. In $n$ dimensions, we write $\delta(x_1, \dots, x_n) = \delta(x_1) \cdots \delta(x_n)$
\end{definition}

\subsection{Operations on distributions}



\begin{definition}
	$\forall \psi \in \mathcal{D}(\mathbb{R}^n), \ \forall T_1, T_2 \in \mathcal{D}'(\mathbb{R}^n)$, we define the following operations:
	\begin{enumerate}
		\item \textbf{Addition}: $(T_1 + T_2) (\psi) = T_1(\psi) + T_2(\psi)$.
		\item \textbf{Multiplication  by a constant}: $(c T) (\psi) = c T(\psi)$ for every constant $c$.
		\item \textbf{Shifting}: for $\underline{\xi} \in \mathbb{R}^n$,
		\[
			\begin{aligned}
				T_{\underline{\xi}} (\psi(\underline{x}))
					& := \int_{\mathbb{R}^n} T(\underline{x} - \underline{\xi}) \psi(\underline{x}) d \underline{x} \\
					& = \int_{\mathbb{R}^n} T(\underline{y}) \psi(\underline{y} + \underline{\xi}) d \underline{y} \\
					& = T(\psi(\underline{y} + \underline{\xi}))
			\end{aligned}
		\]
		\item \textbf{Transposition}:
		\[
			\begin{aligned}
				T^t (\psi(\underline{x}))
					& := \int_{\mathbb{R}^n} T(-\underline{x}) \psi(\underline{x}) d \underline{x} \\
					& = \int_{\mathbb{R}^n} T(\underline{y}) \psi(-\underline{y}) d \underline{y} \\
					& = T(\psi(-\underline{x}))
			\end{aligned}
		\]
		\item \textbf{Dilation}:
		\[
			\begin{aligned}
				T_{(\alpha)} (\psi(x))
					& = \int_{\mathbb{R}^n} T(\alpha \underline{x}) \psi(\underline{x}) d \underline{x} \\
					& = \frac{1}{|\alpha|^n} \int_{\mathbb{R}^n} T(\underline{y}) \psi \left( \frac{\underline{y}}{\alpha} \right) d \underline{y} \\
					& = \frac{1}{|\alpha|^n} T \left( \psi \left( \frac{\underline{y}}{\alpha} \right) \right)
			\end{aligned}
		\]
		\item \textbf{Multiplication by a smooth function} $\phi \in C^{\infty} (\mathbb{R}^n)$:
		\[
			(\phi T) (\psi) = T(\phi \psi)
		\]
	\end{enumerate}
\end{definition}

\begin{remark}
	The above rules are natural for regular distributions, since the integrals are meaningful in the classical sense. For singular distributions, we extend the formalism and use the (symbolic) notations
	\[
		T(\psi) = \int_{\mathbb{R}^n} T(\underline{x}) \psi(\underline{x}) d \underline{x}
	\]
\end{remark}

\begin{remark}
	\textbf{Important}: If $\Omega \subset \mathbb{R}$, with $\Omega$ open,
	\[
		\delta(\xi) (\psi(x)) = \begin{cases}
			\psi(\xi) & \text{ if } \xi \in \Omega \\
			0 & \text{ otherwise}
		\end{cases}
	\]
	This is called the \textbf{sifting} property of the delta distribution.
\end{remark}

\begin{example}
	Calculating with $\delta \in \mathcal{D}'(\mathbb{R})$:
	\begin{itemize}
		\item Shifting:
		\[
			\begin{aligned}
				\delta_{\xi} (\psi(x))
					& = \int_\mathbb{R} \delta(x - \xi) \psi(x) dx \\
					& = \int_{\mathbb{R}} \delta(y) \psi(y + \xi) dy \\
					& = \int_{\mathbb{R}} \delta(x) \psi(x + \xi) dx \\
					& = \psi(\xi) \\
					& = \delta(\psi(x + \xi))
			\end{aligned}
		\]
		\item $\phi \delta_{\xi} (\psi)$ for smooth $\phi$:
		\[
			\begin{aligned}
				\phi \delta_{\xi} (\psi)
					& = \int_{\mathbb{R}} \phi(x) \delta(x - \xi) \psi(x) dx \\
					& = \int_{\mathbb{R}} \delta(x - \xi) (\phi(x) \psi(x)) dx \\
					& = \phi(\xi) \psi(\xi) \\
					& = \phi(\xi) \int_{\mathbb{R}} \delta(x - \xi) \psi(x) dx \\
					& = \int_{\mathbb{R}} \phi(\xi) \delta(x - \xi) \psi(x) dx
			\end{aligned}
		\]
		\item $\delta_{(\alpha)} (\psi)$:
		\[
			\begin{aligned}
				\delta_{(\alpha)} (\psi)
					& = \int_{\mathbb{R}} \delta(\alpha x) \psi(x) dx \\
					& = \frac{1}{\alpha} \int_{\mathbb{R}} \delta(y) \psi \left( \frac{y}{\alpha} \right) dy
			\end{aligned}
		\]
		For $\alpha < 0$, $(-\alpha) > 0$, so $dy = \alpha dx = -|a| dx$ so the limits of integrations swap from $\int_{-\infty}^{\infty}$ to $\int_{\infty}^{-\infty}$ but the minus sign in $-|a| dx$ compensates for this.
		\item For $x_1, \dots, x_n$ are zeros of $f$.
		\[
			\int_{\Omega} \delta(f(x)) \psi(x) dx = \sum_{i = 1}^{n} \delta((x - x_i) f'(x_i)) \psi(x) dx = \sum_{i = 1}^{n} \frac{1}{|f'(x_i)|} \psi(x_i)
		\]
		by the sifting property of the delta distribution.
	\end{itemize}
\end{example}

\begin{example}
	For $b > 0$, let $f(x) = x^2 - b^2$, $f: \mathbb{R} \rightarrow \mathbb{R}$, then $f \in C^1(\mathbb{R})$, $f(b) = f(-b) = 0$, with $x_1, x_2 = b-, -b$ as the zeroes. $f'(x) = 2x$ so $f'(b) = 2b$ and $f'(-b) = -2b$. Then
	\[
		\int_{\mathbb{R}} \delta(x^2 - b^2) \psi(x) dx = \frac{1}{2|b|} (\psi(b) + \psi(-b))
	\]
\end{example}

\begin{definition}
	$T_1$ and $T_2 \in \mathcal{D}'(\Omega)$ are \textbf{equal} if
	\[
		\int_{\Omega} T_1(x) \psi(x) dx = \int_{\Omega} T_2(x) \psi(x) dx \quad \forall \psi \in \mathcal{D}(\Omega)
	\]
\end{definition}

\begin{example}
	For which value of $A \in \mathbb{R}$ are $T_1 = x^2 \delta(x^3)$ and $T_2 = A \delta(x)$ equal?
	\[
		\begin{aligned}
			\int_{\mathbb{R}} x^2 \delta(x^3) \psi(x) dx & = \frac{1}{3} \int_{\mathbb{R}} \delta(y) \psi(y^{1/3}) dy & = \frac{1}{3} \psi(0)
			\int_{\mathbb{R}} A \delta(x) \psi(x) dx = A \psi(0)
		\end{aligned}
	\]
	so $A = 1/3$.
\end{example}

\subsection{The derivative of a distribution}

\begin{definition}
	For a distribution $T \in \mathcal{D}(\Omega)$, its \textbf{derivative} is defined as
	\[
		T'(\psi) = -T(\psi') \quad \forall \psi \in \mathcal{D}(\Omega))
	\]
	or symbolically, $\langle T', \psi \rangle = -\langle T, \psi' \rangle$. The \textbf{$n$th derivative} of $T$ is defined as
	\[
		T^{(n)} (\psi) = {(-1)}^n T(\psi^{(n)})
	\]
	or symbolically, $\langle T^{(n)}, \psi \rangle = {(-1)}^n \langle T, \psi^{(n)} \rangle$.
\end{definition}

\begin{example}
	The unit step function $\Theta(t)$ is a locally integrable function, so defined a regular distribution:
	\[
		\Theta(\psi) = T_{\Theta} (\psi) = \int_{-\infty}^{\infty} \Theta(t) \psi(t) dt = \int_{0}^{\infty} \psi(t) dt < \infty
	\]
	because $\psi$ has compact support. So
	\[
		\begin{aligned}
			\Theta'(t) & = -\Theta(\psi') = -\int_{-\infty}^{\infty} \Theta(t) \psi'(t) dt \\
			& = -\int_{0}^{\infty} \psi'(t) dt = -\psi(\infty) + \psi(0) = \psi(0) = \delta(\psi)
		\end{aligned}
	\]
	because $\psi$ has compact support. So we have, in the sense of distributions,
	\[
		\Theta'(t) = \delta(t)
	\]
\end{example}

\begin{example}
	The derivative of the delta distribution is $\delta'(\psi) = -\delta(\psi') = -\psi'(0)$ and generally, $\delta^{(n)} (\psi) = {(-1)}^n \delta(\psi^{(n)}) = {(-1)}^n \psi^{(n)} (0)$.
\end{example}

\begin{example}
	Show that, in the sense of distributions, $\delta'(cx) = \frac{1}{c^2} \delta'(x)$ for a constant $c > 0$.
	\[
		\begin{aligned}
			\int_{-\infty}^{\infty} \delta'(cx) \psi(x) dx & = \frac{1}{c} \int_{-\infty}^{\infty} \delta'(y) \psi(y / c) dy = -\frac{1}{c} \int_{-\infty}^{\infty} \delta(y) \left( \psi(y / c) \right)' dy \\
			& = -\frac{1}{c^2} \int_{-\infty}^{\infty} \delta(y) \psi'(y / c) dy = \frac{1}{c^2} \psi'(0) = -\frac{1}{c^2} \int_{-\infty}^{\infty} \delta(x) \psi'(x) dx \\
			& = \frac{1}{c^2} \int_{-\infty}^{\infty} \delta'(x) \psi(x) dx
		\end{aligned}
	\]
\end{example}

\subsection{Leibniz rule for differentiation}

\begin{proposition}
	For a distribution $T$ and a smooth function $\phi$, the product rule holds:
	\[
		(\phi T)' = \phi'T + \phi T'
	\]
	and more generally, the Leibniz rule holds:
	\[
		{(\phi T)}^{(n)} = \sum_{k = 0}^{n} \binom{n}{k} \phi^{(k)} T^{(n - k)}
	\]
\end{proposition}

\begin{example}
	Let $\psi \in \mathcal{D}(\Omega)$. The symbolic representation of ${(\phi \delta_a)}^{(n)} (\psi)$ is given by
	\[
		\begin{aligned}
			{(\phi \delta_a)}^{(n)} (\psi)
				& = \int_{\Omega} {(\phi(x) \delta(x - a))}^{(n)} \psi(x) dx \\
				& = {(-1)}^n \int_{\Omega} (\phi(x) \delta(x - a)) \psi(x)^{(n)} dx = {(-1)}^n \phi(a) \psi^{(n)} (a)
		\end{aligned}
	\]
	We can write this in inner product symbolic notation as
	\[
		\langle {(\phi \delta_a)}^{(n)}, \psi \rangle = {(-1)}^n = \langle \phi \delta_a, \psi^{(n)} \rangle = {(-1)}^n \langle \delta_a, \phi \psi^{(n)} \rangle = {(-1)}^n \phi(a) \psi^{(n)} (a)
	\]
	Alternatively, by the sifting property of the delta distribution, we can write
	\[
		{(\phi \delta_a)}^{(n)} (\psi) = \int_{\Omega} {(\phi(x) \delta(x - a))}^{(n)} \psi(x) dx = \phi(a) \int_{\Omega} \delta^{(n)} (x - a) \psi(x) dx
	\]
	before using the definition of the generalised derivative on the $\delta$ distribution. So a smooth factor of the delta function may be replaced by a constant and taken outside the integral.
\end{example}

TODO: notes up to end of section 2.4.1

\begin{definition}
	A function $f$ is \textbf{piecewise continuous} for $x \in (a, b)$ if $(a, b)$ can be divided into a finite number of sub intervals and
	\begin{enumerate}
		\item $f$ is continuous on each sub interval.
		\item $f$ tends to a finite limit on the boundaries of each sub interval as approached from the interior of these sub intervals.
	\end{enumerate}
\end{definition}

\begin{example}
	\[
		f(x) = \begin{cases}
			\sin(x) & \text{ if } -3 \pi < x \pi \\
			\frac{1}{2} \cos(x) & \text{ if } \pi \le x < 2 \pi \\
			\frac{3}{2} & \text { if } 2 \pi \le x < 3 \pi
		\end{cases}
	\]
	on the interval $(a, b) = (-3 \pi, 3 \pi)$. The sub intervals are $(-3 \pi, \pi), (\pi, 2 \pi), (2 \pi 3 \pi)$. $f$ is piecewise continuous.
\end{example}

\begin{definition}
	A function $f$ is \textbf{piecewise smooth} if
	\begin{enumerate}
		\item $f$ is piecewise continuous.
		\item $f$ has piecewise continuous derivatives.
	\end{enumerate}
\end{definition}

\begin{example}
	\[
		f(x) = \begin{cases}
			x & \text{ if } -\pi < x \le 0 \\
			\pi & \text{ if } 0 < x < \pi
		\end{cases}
	\]
	on the interval $(-\pi, \pi)$. Then
	\[
		f'(x) = \begin{cases}
			1 & \text{ if } -\pi < x \le 0 \\
			0 & \text{ if } 0 < x < \pi
		\end{cases}
	\]
	which is piecewise continuous.
\end{example}

\begin{example}
	$\tan$ is not a piecewise smooth function.
\end{example}

\begin{definition}
	To calculate the derivative of a piecewise smooth function which can be split into $f_1, f_2, \dots$ on sub intervals $(a, x_1), (x_1, x_2), \dots, (x_n, b)$:
	\begin{enumerate}
		\item Define $\tilde{f}(x) = f_1(x) + (f_2(x) - f_1(x)) \Theta (x - x_1) + (f_3(x) - f(2)) \Theta (x - x_2) + \cdots$. But $\tilde{f}(x) = f(x)$ on $(a, b)$.
		\item Compute $f'(x) = \tilde{f}'(x)$.
	\end{enumerate}
\end{definition}

\begin{example}
	\[
		f(x) = \begin{cases}
			\sin(x) & \text{ if } -3 \pi < x \pi \\
			\frac{1}{2} \cos(x) & \text{ if } \pi \le x < 2 \pi \\
			\frac{3}{2} & \text { if } 2 \pi \le x < 3 \pi
		\end{cases}
	\]
	Then
	\[
		f(x) = \tilde{f}(x) = \sin(x) + \left( \frac{1}{2} \cos(x) - \sin(x) \right) \Theta(x - \pi) + \left( \frac{3}{2} - \frac{1}{2} \cos(x) \right) \Theta(x - 2 \pi)
	\]
	so
	\[
		\begin{aligned}
			f'(x)
				& = \cos(x) + \left( -\frac{1}{2} \sin(x) - \cos(x) \right) \Theta(x - \pi) + \left( \frac{1}{2} \cos(x) - \sin(x) \right) \delta(x - \pi) + \frac{1}{2} \sin(x) \Theta(x - 2 \pi) + \left( \frac{3}{2} - \frac{1}{2} \cos(x) \right) \delta(x - 2 \pi) \\
				& = \cos(x) - \left( \frac{1}{2} \sin(x) + \cos(x) \right) \Theta(x - \pi) + \left( -\frac{1}{2} - 0 \right) \delta(x - \pi) + \frac{1}{2} \sin9x) \Theta(x - 2 \pi) + \left( \frac{3}{2} - \frac{1}{2} \right) \delta(x - 2 \pi) \\
				& = -\frac{1}{2} \delta(x - \pi) + \delta(x - 2 \pi) + \begin{cases}
					\cos(x) & \text{ if } 3 \pi < x \le \pi \\
					-\frac{1}{2} \sin(x) & \text{ if } \pi < x \le 2 \pi \\
					0 & \text{ if } 2 \pi < x < 3 \pi
				\end{cases}
		\end{aligned}
	\]
\end{example}

\begin{definition}
	The \textbf{sign function} is defined as
	\[
		\text{sgn}(x) = \Theta(x) - \Theta(-x) = \begin{cases}
			1 & \text{ if } x > 0 \\
			0 & \text{ if } x = 0 \\
			-1 & \text{ if } x < 0
		\end{cases}
	\]
\end{definition}

\begin{example}
	\[
		\begin{aligned}
			\Theta'(-x) (\psi)
				& = \int_{-\infty}^{\infty} \Theta'(-x) \psi(x) dx \\
				& = -\int_{-\infty}^{\infty} \Theta(-x) \diff{}{(-x)} \psi(x) dx \\
				& = \int_{-\infty}^{\infty} \Theta(-x) \diff{}{x} \psi(x) dx \\
				& = \int_{-\infty}^{\infty} \diff{}{x} \psi(x) dx = \psi(0) - \psi(-\infty) = \psi(0) = \delta(\psi)
		\end{aligned}
	\]
	So the derivative of the sign function is
	\[
		\text{sgn}'(x) = \diff{}{x} \Theta(x) - \diff{}{x} \Theta(-x) = \delta(x) - \diff{}{x} \Theta(-x) = 2 \delta(x)
	\]
\end{example}

\subsection{The delta distribution in polar/spherical coordinates}

\begin{definition}
	For a change of coordinates the delta function is defined as
	\[
		\delta(\underline{x} - \underline{x_0}) = \frac{1}{|J|} \delta(\underline{\xi} - \underline{\xi_0})
	\]
	where $\underline{x} \in \mathbb{R}^n, \underline{\xi} \in \mathbb{R}^n$, $\underline{x}, \underline{\xi}$ are constant vectors of coordinates $(x_1, \dots, x_n), (\xi_1, \dots, \xi_n)$. $J$ is the Jacobian of the change of variable from $\underline{x}$ to $\underline{\xi}$.
\end{definition}

\begin{example}
	If the problem at hand does not depend on $\theta$, then
	\[
		\delta(\underline{x} - \underline{x_0}) = \frac{1}{2 \pi r} \delta(\underline{r} - \underline{r_0})
	\]
\end{example}

\section{Lebesgue Integration}

TODO: notes from lecture

\begin{theorem}
	Let $f: [a, b] \rightarrow \mathbb{R}$ be bounded. Then $f$ is Riemann integrable iff $f$ is continuous `almost everywhere'' (it is continuous except on sets with zero measure).
\end{theorem}

\subsection{Hilbert spaces: definitions and examples}

\begin{definition}
	A \textbf{Hilbert space} $\mathbb{H}$ is a real or complex vector space which satisfies:
	\begin{enumerate}
		\item $\mathbb{H}$ has a symmetric (or hermitian for a complex vector space) inner product, $\langle \cdot, \cdot \rangle: \mathbb{H} \times \mathbb{H} \rightarrow \mathbb{C}$, which is a continuous function of two variables. It associates a complex number $\langle u, v \rangle$ to every pair $(u, v)$ in $\mathbb{H} \times \mathbb{H}$. $\langle \cdot, \cdot \rangle$ and satisfies:
		\begin{enumerate}
			\item \textbf{Hermiticity or symmetry}: $\langle u, v \rangle = \overline{\langle v, u \rangle}$.
			\item \textbf{Anti-linearity in the first entry}: $\forall a \in \mathbb{C}, \langle a(u + v), w \rangle = \overline{a} \langle u + v, w \rangle = \overline{a} \langle u, w \rangle + \overline{a} \langle v, w \rangle$.
			\item \textbf{Positivity}: $\langle u, v \rangle \ge 0$ and $\langle u, u \rangle = 0 \Longleftrightarrow u = 0$.
		\end{enumerate}
		\item $\mathbb{H}$ is \textbf{complete} for the norm induced by the inner product, which is defined as $||u|| = \sqrt{\langle u, u \rangle}$. (\textbf{Complete} means that every Cauchy sequence in $\mathbb{H}$ has a limit in $\mathbb{H}$).
	\end{enumerate}
\end{definition}

\begin{remark}
	$\overline{\langle a(u + v), w \rangle} = \langle w, a(u + v) \rangle = a \overline{\langle u, w \rangle} + a \overline{\langle v, w \rangle} = a \langle w, u \rangle + a \langle w, v \rangle$. This is linearity in the second entry, or \textbf{sesquilinearity}.
\end{remark}

\begin{definition}
	A vector space with an inner product and induced norm $||\cdot|| = \sqrt{\langle \cdot, \cdot \rangle}$ is called a \textbf{inner product space} (or \textbf{pre-Hilbert space}).
\end{definition}

\begin{definition}
	If $V$ is an inner product space, it can be viewed as a \textbf{metric space} with metric $d$, where
	\[
		d(u, v) := ||u - v|| = \sqrt{\langle u - v, u - v \rangle}
	\]
	Then a sequence ${(v_n)}_{n \in \mathbb{N}} \in (V, d)$ converges to $v$ \textbf{in norm} if
	\[
		\lim_{n \rightarrow \infty} ||v_n - v|| = 0
	\]
	or equivalently,
	\[
		\forall \epsilon > 0, \exists n \in \mathbb{N}, \forall n \ge N, \quad ||v_n - v|| < \epsilon
	\]
\end{definition}

\begin{definition}
	A sequence ${(v_n)}_{n \in \mathbb{N}} \in (V, d)$ is called a \textbf{Cauchy sequence} if
	\[
		\forall \epsilon > 0, \exists n \in \mathbb{N}, \forall m \ge N, \forall n \ge N, \quad d(v_m, v_n) < \epsilon
	\]
\end{definition}

\begin{remark}
	Every Cauchy sequence is convergent in $\mathbb{R}$, since $\mathbb{R}$ is complete, but this is not true in all spaces.
\end{remark}

\begin{definition}
	A metric space $(V, d)$ is called \textbf{complete} if every Cauchy sequence in $(V, d)$ converges in $V$.
\end{definition}

\subsection{Inner products defined by integrals}

\begin{definition}
	Consider the vector space $V$ of complex-valued functions, defined on an interval $[a, b]$ and let $\omega: [a, b] \rightarrow \mathbb{R}^+$ be a non-negative function with finitely many zeroes on $[a, b]$. $\omega$ is called a \textbf{weight} function.

	The inner product with weight $\omega$ for every pair of functions $(u, v) \in V^2$ is defined as
	\[
		{\langle u, v \rangle}_{\omega} = \int_{a}^{b} \bar{u}(x) v(x) \omega(x) dx
	\]
\end{definition}

\begin{remark}
	If $\omega = 1$, then ${\langle u, v \rangle}_1$ is just written as $\langle u, v \rangle$.
\end{remark}

\begin{remark}
	${\langle u, v \rangle}_{\omega} = {\langle u, v \omega \rangle}_1 = \langle u, v \omega \rangle = \langle u \omega, v \rangle$, since $\omega$ is a real function.
\end{remark}

\begin{example}
	The space $C^0 (\omega, [a, b])$ of continuous functions on $[a, b]$ with inner product $*$ and norm
	\[
		||u||_{L^2 (w, [a, b])} := \sqrt{\int_{a}^{b} \bar{u}(x) u(x) \omega(x) dx} = \sqrt{\int_{a}^{b} |u(x)|^2 \omega(x) dx}
	\]
	is not complete for the norm $L^2$. Its completion is $L^2 (\omega, [a, b])$, the space of square-integrable functions on $[a, b]$ with inner product ${\langle u, v \rangle}_{\omega}$.
\end{example}

\begin{example}
	Consider $C^0(1, [0, 1])$, written as $C^0 ([a, b])$. Let
	\[
		f_n(x) = \begin{cases}
			1 & \text{ if } x > \frac{1}{2} \\
			0 & \text{ if } x \le \frac{1}{2} - \frac{1}{n} \\
			n(x - \frac{1}{2} - \frac{1}{n}) & \text{ if } \frac{1}{2} - \frac{1}{n} < x < \frac{1}{2}
		\end{cases}
	\]
	${(f_n)}_{n \in \mathbb{N}}$ is a Cauchy sequence, but it does not converge to a function in $C^0([0, 1])$.
\end{example}

\subsection{Bounded and unbounded linear operators}

\begin{definition}
	Let $(V, d)$ be a metric space. $W \subset V$ is called \textbf{dense} in $V$ if
	\[
		\forall v \in V, \forall \epsilon > 0, \exists w \in W, \quad d(v, w) < \epsilon
	\]
	So $W$ is dense in $V$ if every point in $V$ has points in $W$ that are arbitrarily close.
\end{definition}

\begin{definition}
	A \textbf{linear Operator} on $\mathbb{H}$ is a pair $(L, D(L))$ which satisfies
	\begin{enumerate}
		\item $D(L) \subseteq \mathbb{H}$ and $D(L)$ is dense in $\mathbb{H}$.
		\item $L: D(L) \rightarrow \mathbb{H}$ is linear, so it satisfies
		\[
			L(a u + b v) = a L(u) + b L(v)
		\]
		for every $a, b \in \mathbb{C}$, for every $u, v \in D(L)$.
	\end{enumerate}
\end{definition}

\begin{example}
	Let $L = -\diffp[2]{}{x}$.
	\begin{enumerate}
		\item The pair $(L, D_N(L))$ with $D_N(L) := \{ u \in C^2([a, b]) \subset L^2 ([a, b]): u'(a) = u'(b) = 0 \}$ is a linear Operator on $L^2 ([a, b])$. $D_N(L)$ is dense in $L^2 ([a, b])$ since $C^{\infty} ([a, b]) \subset C^2 ([a, b])$ is dense in $L^2 ([a, b])$. Here, $D_N$ stands for Neumann boundary conditions.
		\item The pair $(L, D_D (L)) := \{ u \in C^2 ([a, b]) \subset L^2 ([a, b]): u(a) = u(b) = 0 \}$ is a linear Operator. Here, $D_D$ stands for Dirichlet boundary conditions. This linear Operator is different from $(L, D_N(L))$, since for example, if $\lambda = 0$, then $L(u) = \lambda u$ and $L(u) = -\diffp[2]{u}{x} = 0$, so $u(x) = Ax + B$ for constants $A$ and $B$. But if $u(a) = u(b) = 0$, then $u(x) = 0$ for all $x \in [a, b]$. Whereas if $u'(a) = u'(b) = 0$, then $A = 0$ so $u(x) = B$.
	\end{enumerate}
\end{example}

\begin{definition}
	Let $(\mathbb{H}_1, ||\cdot||_{H_1})$ and $(\mathbb{H}_2, ||\cdot||_{H_2})$ be Hilbert spaces. A linear operator $L: \mathbb{H}_1 \rightarrow \mathbb{H}_2$ is called \textbf{bounded} if for some $M > 0$,
	\[
		\forall u \in \mathbb{H}_1, \quad ||L(v)||_{H_2} \le M ||v||_{H_1}
	\]
	If no such $M$ exists, then $L$ is called \textbf{unbounded}.
\end{definition}

\begin{definition}
	The \textbf{norm} of $L$ is defined as
	\[
		||L|| := \inf \{ M: ||L(v)||_{H_2} \le M ||v||_{H_1} \text{ for every } v \in \mathbb{H}_1 \}
	\]
\end{definition}

\begin{example}
	$I: \mathbb{H} \rightarrow \mathbb{H}$ defined as $I(v) = v$ is bounded, since
	\[
		||I(v)||_{\mathbb{H}} = ||v||_{\mathbb{H}} \le M ||v||_{\mathbb{H}}
	\]
	for every $M \ge 1$. So $||I|| = 1$ as $M = 1$ is the lower bound.
\end{example}

\begin{example}
	Differential operators are usually unbounded. For example,
	\[
		A: L^2 ([a, b]) \rightarrow L^2 ([a, b]), \quad A(x(t)) = x'(t)
	\]
	The domain, $D(A) = \{ x: x \in L^2([-\pi, \pi]), x \in C^1 ([a, b]) \}$. Consider a family of functions $x_n(t)$ in $L^2([-\pi, \pi])$, for example, $x_n(t) = \cos(nt)$ then $A(x_n) = -n \sin(nt)$. Now,
	\[
		\begin{aligned}
			||x_n||_{L^2} & = \sqrt{\int_{\pi}^{\pi} \overline{\cos(nt)} \cos(nt) dt} = \sqrt{\pi} \\
			||A(x_n)||_{L^2} & = \sqrt{\int_{\pi}^{\pi} \overline{-n \sin(nt)} (-n \sin(nt)) dt} = \sqrt{\pi n^2}
		\end{aligned}
	\]
	Hence
	\[
		\frac{||A(x_n)||_{L^2}}{||x_n||_{L^2}} = n
	\]
	which is unbounded.
\end{example}

\begin{definition}
	Let $\mathbb{H}_1, \mathbb{H}_2$ be Hilbert spaces, with ${\langle \cdot, \cdot \rangle}^{(1)}$ and ${\langle \cdot, \cdot \rangle}^{(2)}$. Let $L: D(L) \subset \mathbb{H}_1 \rightarrow \mathbb{H}_2$ be an unbounded linear Operator on $\mathbb{H}_1$, with $D(L)$ dense in $\mathbb{H}_1$.
	
	The \textbf{adjoint} $(L^*, D(L^*))$ of $L$ is defined as the linear Operator with $L*: D(L^*) \subset \mathbb{H}_2 \rightarrow \mathbb{H}_1$ defined by
	\[
		{\langle L(v_1), v_2 \rangle}^{(2)} = {\langle v_1, L*(v_2) \rangle}^{(1)}, \quad v_1 \in \mathbb{H}_1
	\]
	where
	\[
		D(L*) = \{ v_2 \in \mathbb{H}_2: \exists v_2^* \in \mathbb{H}_1, \  {\langle v_1, v_2^* \rangle}^{(1)} = {\langle L(v_1), v_2 \rangle}^{(2)} \ \forall v_1 \in \mathbb{H}_1 \}
	\]
	For every $v_2 \in D(L*)$, $v_2^* \in D(L)$ is unique and $v_2^* = L*(v_2)$.
\end{definition}

\subsection{Sturm-Liouville operators}

\begin{proposition}\label{prop:greensFormula}
	(\textbf{Green's formula}) Let $(L, D(L))$ be a linear differential Operator  and define
	\[
		L* := \overline{p_0} (x) d_x^2 + (2 \overline{p_0}'(x) - \overline{p_1}(x)) d_x + (\overline{p_0}''(x) - \overline{p_1}'(x) + \overline{p_2}(x))
	\]
	Then
	\[
		\forall u, v \in C^2 ([a, b]), \quad \langle L(u), v \rangle - \langle u, L^*(v) \rangle = {\left[ \overline{p_0} (v \overline{u}' - v' \overline{u}) + (\overline{p_1} - \overline{p_0}') v \overline{u} \right]}_a^b
	\]
\end{proposition}

\begin{proof}
	Omitted.
\end{proof}

\begin{definition}
	The operator $L^*$ is called the \textbf{formal adjoint} of $L$ (since $D(L^*)$ has not yet been specified in order to determine the boundary terms in Green's formula).
\end{definition}

TODO: define BVP or IVP

\begin{definition}
	Consider a BVP or IVP involving $(L, D(L))$. The adjoint $(L*, D(L*))$ is given by $L*$, defined in \hyperref[prop:greensFormula]{Green's formula} and its domain $D(L*)$ consists of all functions $v$ whose boundary coniditions ensure that the boundary terms in Green's formula vanish, i.e.
	\[
		\left[ \overline{p_0} (v \overline{u}' - v' \overline{u}) + (\overline{p_1} - \overline{p_0}') uv \right] = 0
	\]
	If this is true, then $\langle L(u), v \rangle - \langle u, L^*(v) \rangle = 0$ by Green's formula and $(L*, D(L*))$ is the adjoint of $(L, D(L))$.
\end{definition}

\begin{definition}
	The BVP $L(u) = f$, $B_1(u) = B_2(u) = 0$ is called \textbf{self-adjoint} if $L^* = L$ and $D(L*) = D(L)$.
\end{definition}

\begin{example}
	Let $L = \diffp[2]{}{x}$, $L(u) = f$, $a < x < b$, $B_1(u) := u(a) = 0$, $B_2(u) := u(b) = 0$ (Dirichlet boundary conditions). Then
	\[
		D(L) = \{ u: u \in C^2 ([a, b]): u(a) = u(b) = 0 \}
	\]
	Also,
	\[
		L* := \overline{p_0} (x) d_x^2 + (2 \overline{p_0}'(x) - \overline{p_1}(x)) d_x + (\overline{p_0}''(x) - \overline{p_1}'(x) + \overline{p_2}(x))
	\]
	Here, $p_0 = 1, p_1 = p_2 = 0, \overline{p_0} = p_0, p_0' = 0$. So
	\[
		L* = d_x^2 = L
	\]
	so $L$ is formally self-adjoint. By \hyperref[prop:greensFormula]{Green's formula}, the boundary terms are
	\[
		\begin{aligned}
			{\left[ \overline{p_0} (v \overline{u}' - v' \overline{u}) + (\overline{p_1} - \overline{p_0}') v \overline{u} \right]}_a^b 
				& = {\left[ v \overline{u}' - v' \overline{u} \right]}_a^b \\
				& = v(b) \overline{u}'(b) - v'(b) \overline{u}(b) - v(a) \overline{u}'(a) + v'(a) \overline{u}(a) \\
				& = v(b) \overline{u}'(b) - v(a) \overline{u}'(a) \\
				& = 0 \text{ iff } v(a) = v(b) = 0
		\end{aligned}
	\]
	since $u(a) = 0 \Longrightarrow \overline{u}(a) = 0$ and $u(b) = 0 \Longrightarrow \overline{u}(b) = 0$. So $D(L*) = D(L)$ and the BVP is self-adjoint.
	\[
		L = p_0 d_x^2 + p_1 d_x + p_2, \quad L* = \overline{p_0} d_x^2 + (2 \overline{p_0}' - \overline{p_1}) d_x + (\overline{p_0}'' - \overline{p_1}' + \overline{p_2})
	\]
	so $L = L*$ if $p_0 = \overline{p_0}$ (so $p_0$ is a real-valued function), $p_1 = 2 \overline{p_0}' - \overline{p_1}$, and $p_2 = \overline{p_0}'' - \overline{p_1}' + \overline{p_2}$. So $p_1 + \overline{p_1} = 2 \text{Re} (p_1) = 2 p_0'$. Also $p_2 - \overline{p_2} = 2 \text{Im}(p_2) = p_0'' - \overline{p_1}'$ so $2 \text{Im}(p_2) = \text{Im}(p_1')$.

	Hence $\{ p_0, \text{Im}(p_0), \text{Re}(p_2) \}$ characterises a formally self-adjoint operator.
\end{example}

\begin{definition}
	Let $L$ be a second-order linear operator with real valued coefficients. If $p_0' = p_1$ then
	\[
		L^* = p_0 d_x^2 + p_0' d_x + p_2 = \diff{}{x} \left( p_0 \diff{}{x} \right) + p_2 = L
	\]
	Operators of this form are called \textbf{Sturm-Liouville} operators.
\end{definition}

\begin{example}
	(Problems class) Let
	\[
		L = \diff{}{x} \left( r(x) \diff{}{x} \right) + s(x)
	\]
	be a differential operator on $[a, b]$, with $r(x) \in C^1 ([a, b])$ and $s \in C^0 ([a, b])$ and $r$ and $s$ real-valued functions. Assume that $r(x) \ge c$ on $[a, b]$ for some constant $c$. Consider the BVP
	\[
		L(u(x)) = f(x), \quad x \in (a, b), u \in C^2 ([a, b])
	\]
	where $f(x)$ is a source term and sufficiently smooth. The boundary conditions are $B_1(u) := u'(a) - \alpha u(a) = 0$ and $B_2(u) = u'(b) + \beta u(b) = 0$ for some constants $\alpha, \beta \in \mathbb{R}$. Find the adjoint boundary conditions $B_1^*(v) = 0$ and $B_2^*(v) = 0$ and determine whether the BVP is self-adjoint.

	A BVP is self-adjoint if $L = L^*$ ($L$ is formally self-adjoint) and $D(L) = D(L^*)$. $L$ is a Sturm-Liouville operator so $L = L^*$. $D(L) = \{ u \in C^2 ([a, b]): u'(a) - \alpha u(a), u'(b) + \beta u(b) = 0 \}$. Using \hyperref[prop:greensFormula]{Green's formula},
	\[
		\begin{aligned}
			\langle L(u), v \rangle - \langle u, L^*(v) \rangle
				& = (\langle L(u), v \rangle - \langle u, L(v) \rangle) \\
				& = {\left[ \overline{p_0} (v \overline{u}' - v' \overline{u}) + (\overline{p_1} - \overline{p_0}') v \overline{u} \right]}_a^b \\
				& = \left[ r(x) v(x) u'(x) - r(x) v'(x) \overline{u}(x) \right]_a^b \\
				& = r(b) v(b) \overline{u}'(b) - r(b) v'(b) \overline{u}(b) - r(a) v(a) \overline{u}'(a) + r(a) v'(a) \overline{u}(a)
		\end{aligned}
	\]
	Now, $\overline{u}'(a) = \alpha \overline{u}(a)$ (since $u'(a) = \alpha u(a)$ and $\alpha \in \mathbb{R}$) and $\overline{u}'(b) = -\beta \overline{u}(b)$ (since $u'(b) = -\beta u(b)$ and $\beta \in \mathbb{R}$). So we get the boundary terms
	\[
		\begin{aligned}
			& r(b) v(b) (-\beta \overline{u}(b)) - r(b) v'(b) \overline{u}(b) - r(a) v(a) (\alpha \overline{u}(a)) + r(a) v'(a) \overline{u}(a) \\
			& = r(b) \overline{u}(b) (-\beta v(b) - v'(b)) + r(a) \overline{u}(a) (v'(a) - \alpha v(a)) \\
			& = 0 \text{ iff } v'(b) = -\beta v(b) \text{ and } v'(a) = \alpha v(a)
		\end{aligned}
	\]
	Hence the adjoint boundary conditions are
	\[
		B_1^*(v) = v'(a) - \alpha v(a) = 0 \text{ and } B_2^*(v) = v'(b) + \beta v(b) = 0
	\]
	and
	\[
		D(L^*) = D(L) = \{ v \in C^2 ([a, b]): B_1(v) = 0, B_2(v) = 0 \}
	\]
	So the BVP is self-adjoint.
\end{example}

\begin{example}
	(Problems class) Is the operator
	\[
		L = (1 - x^2) \diff[2]{}{x} - 3x \diff{}{x} + v(v + 2)
	\]
	where $v$ is a constant and $-1 \le x \le 1$ formally self-adjoint of Sturm-Liouville type? If not, convert $L$ into a formally self-adjoint operator of Sturm-Liouville type.

	$L$ is not a Sturm-Liouville operator since $p_0 = 1 - x^2$, $p_1 = -3x \ne p_0'$. We can convert $L$ into a Sturm-Liouville operator by multiplying $L$ by a factor
	\[
		\rho(x) = \frac{1}{p_0(x)} \exp \left( \int \frac{p_1(x)}{p_0(x)} \right) = \frac{1}{1 - x^2} \exp \left( \int \frac{-3x}{1 - x^2} \right) = {(1 - x^2)}^{1 / 2}
	\]
	so
	\[
		(\rho L) (x) = (1 - x^2)^{3 / 2} \diff[2]{}{x} - 3x (1 - x^2)^{1 / 2} \diff{}{x} + v(v + 2) (1 - x^2)^{1 / 2}
	\]
\end{example}

\begin{remark}
	More generally, for an operator
	\[
		L = p_0 \diff[2]{p_1}{x} + p_1 \diff{}{x} + p_2
	\]
	we have
	\[
		\rho L = \rho p_0 \diff[2]{p_1}{x} + \rho p_1 \diff{}{x} + \rho p_2
	\]
	The adjoint of $L$ is
	\[
		L^* = \overline{p_0} \diff[2]{}{x} + (2 \overline{p_0}' - \overline{p_1}) \diff{}{x} + \overline{p_0}'' - \overline{p_1}' + \overline{p_2} =  \
		p_0 \diff[2]{}{x} + (2 p_0' - p_1) \diff{}{x} + p_0'' - p_1' + p_2
	\]
	since $p_i$ are real-valued. Hence, with $p_i \rightarrow \rho p_i$,
	\[
		{(\rho L)}^* = \rho p_0 \diff[2]{}{x} + (2 (\rho p_0)' - (\rho p_1)) \diff{}{x} + (\rho p_0)'' - (\rho p_1)' + \rho p_2 = \rho L
	\]
	So $2 (\rho p_0)' - \rho p_1 = \rho p_1$ so
	\[
		\int \frac{\rho'}{\rho} dx = \int \frac{p_1 - p_0'}{p_0} dx
	\]
	which gives
	\[
		\rho = \frac{1}{p_0} \exp \left( \int \frac{p_1}{p_0} dx \right)
	\]
\end{remark}

\end{document}