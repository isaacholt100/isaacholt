\input{../header.tex}

\let\oldforall\forall
\renewcommand{\forall}{\ \oldforall}

\let\oldexist\exists
\renewcommand{\exists}{\ \oldexist}

\newcommand\existu{\ \oldexist!}

\title{Mathematical Physics Course Notes}
\author{Isaac Holt}

\begin{document}

\input{../titletoc.tex}

\section{The action principle}

\subsection{Calculus of variatons}

\begin{definition}
	A \textbf{functional} is a map from a set of functions to $\mathbb{R}$, e.g. $f: (\mathbb{R} \rightarrow \mathbb{R}) \rightarrow \mathbb{R}$.
\end{definition}

\begin{definition}
	Let $y(t)$ be a function with fixed values at endpoints $a$ and $b$. $y$ is \textbf{stationary} for a functional $S$ if

	\[ \diff{S (y(t) + \epsilon z(t))}{\epsilon}\Big|_{\epsilon = 0} = 0 \]
	for every smooth (continuous derivative to every order) $z(t)$ such that $z(a) = z(b) = 0$.
\end{definition}

\begin{remark}
	Functions $y(t)$ may be referred to as \textbf{paths} and so functions that satisfy the above definition are referred to as \textbf{stationary paths}.
\end{remark}

\begin{definition}
	Let $S$ be an \textbf{action functional} (or just \textbf{action}). \textbf{The action principle} states that the paths described by particles are stationary paths of $S$.

	Mathematically, given a particle moving in one dimension with position given by $x(t)$, for arbitrary smooth small deformations $\delta x(t)$ around the true path $x(t)$ (the path the particle follows):

	\[ \delta S := S(x + \delta x) - S(x) = O({(\delta x)}^2) \]
\end{definition}

\begin{lemma}
	(Fundamental lemma of the calculus of variations) Let $f(x)$ be a continuous function in the interval $[a, b]$ such that

	\[ \int_a^b f(x) g(x) dx = 0 \]
	for every smooth function $g(x)$ in $[a, b]$ such that $g(a) = g(b) = 0$. Then $f(x) = 0 \forall x \in [a, b]$.
\end{lemma}

\begin{definition}
	Let $L(r, s)$ be a function of two real variables. If a functional $S$ can be expressed as the time integral of $L$, i.e. if

	\[ S(x) = \int_{t_0}^{t_1} L(x(t), \dot{x}(t)) dt \]
	then $L$ is called a \textbf{Lagrangian}.
\end{definition}

\begin{definition}
	For a Lagrangian $L$, the \textbf{Euler-Lagrange equation} is given by

	\[ \diffp{L}{x} - \diff{}{t} \left( \diffp{L}{\dot{x}} \right) = 0 \]
	where

	\[ \diffp{L}{x} = \diffp{L(r, s)}{r} \Big|_{(r, s) = (x(t), \dot{x}(t))} \text{ \ and \ } \diffp{L}{\dot{x}} = \diffp{L(r, s)}{s} \Big|_{(r, s) = (x(t), \dot{x}(t))} \]
\end{definition}

\begin{remark}\label{rem:lagrangianParametersIndependent}
	The Lagrangian $L$ is just an ordinary function of (here) two independent parameters $(r, s)$t. When constructing the action, $L$ is evaluated at $(r, s) = (x(t), \dot{x}(t))$, but $r$ and $s$ as parameters in the definition of the Lagrangian are independent.

	So $\dot{x}$ is treated as a variable which does not depend on $x$:

	\[ \diffp{x}{\dot{x}} = \diffp{\dot{x}}{x} = 0 \]
\end{remark}

\begin{remark}
	The Euler-Lagrange equation only applies to one-dimensional cases.
\end{remark}

\subsection{Configuration space and generalised coordinates}

\begin{definition}
	\textbf{Configuration space}, denoted $C$, is the set of all possible (in principle) instantaneous configurations for a given a physical system.
\end{definition}

\begin{remark}
	This definition includes positions, but does not include velocities.
\end{remark}

\begin{remark}
	A configuration space must be constructed before a Lagrangian is constructed. The Lagrangian describes the dynamics of this configuration space.
\end{remark}

\begin{example}
	A particle moving in $\mathbb{R}^d$ has configuration space $\mathbb{R}^d$.
\end{example}

\begin{example}
	$N$ distinct particles moving in $\mathbb{R}^d$ have configuration space ${(\mathbb{R}^d)}^N = \mathbb{R}^{dN}$. The configuration space would still be $\mathbb{R}^{dN}$ if the particles were electrically charged, as the charge of the particles does not affect their positions, at least initially.
\end{example}

\begin{example}
	Two distinct particles joined by a rigid rod have configuration space $\mathbb{R}^{2d - 1}$. One particle has configuration space $\mathbb{R}^d$ and there are $d - 1$ angles that must specified to choose the position of the second particle relative to the other.
\end{example}

\begin{definition}
	Let $S$ be a physical system with configuration space $C$. Then $S$ has $\dim(C)$ \textbf{degrees of freedom}.
\end{definition}

\begin{remark}
	For every configuration space, any choice of coordinate system is valid, and the Lagrangian formalism holds regardless of this choice.
\end{remark}

\begin{definition}
	For a configuration space $C$, a set of coordinates in this space is called a set of \textbf{generalised coordinates}. Often generalized coordinates are represented with $q_i$, $i \in \{ 1, \dots, \dim(C) \}$ where $\underline{q}$ is the coordinate vector with components $q_i$.
\end{definition}

\begin{example}
	A particle moving in $\mathbb{R}^2$, with configuration space $\mathbb{R}^2$. We could use Cartesian or polar coordinates to describe the position of the particle in this space (both are equally valid).
\end{example}

\begin{definition}
	Let $C$ be a configuration space and let $\underline{q}(t) \in C$ be a path. For a Lagrangian function $L(\underline{q}, \underline{\dot{q}})$, the \textbf{Euler-Lagrange equations} state that

	\[ \diffp{L}{q_i} - \diff{}{t} \left( \diffp{L}{\dot{q}_i} \right) = 0 \quad \forall i \in \{ 1, \dots, \dim(C) \} \]
\end{definition}

\begin{remark}
	The Euler-Lagrange equations are valid in any coordinate system.
\end{remark}

\begin{remark}
	Similarly to the one-dimensional case:

	\[ \diffp{q_i}{\dot{q}_j} = \diffp{\dot{q}_i}{q_j} = 0 \]
	and

	\[ \diffp{q_i}{q_j} = \diffp{\dot{q}_i}{\dot{q}_j} = \delta_{ij} \]
\end{remark}

\subsection{Lagrangians for classical mechanics}

\begin{definition}
	In a system with kinetic energy $T(\underline{q}, \underline{\dot{q}})$ and potential energy $V(\underline{q})$, the Lagrangian that describes the equations of motion in that system is given by

	\[ L(\underline{q}, \underline{\dot{q}}) = T(\underline{q}, \underline{\dot{q}}) - V(\underline{q}) \]
\end{definition}

\subsection{Ignorable coordinates and conservation of generalised momenta}

\begin{definition}
	Let $\{ q_1, \dots, q_N \}$ be a set of generalised coordinates. A specific coordinates $q_i$ is \textbf{ignorable} if the Lagrangian function expressed in these generalised coordinates does not depend on $q_i$, i.e. if

	\[ \diffp{L}{q_i} = 0 \]
\end{definition}

\begin{definition}
	The \textbf{generalised momentum} $p_i$ associated with a generalised coordinate $q_i$ is given by

	\[ p_i := \diffp{L}{\dot{q_i}} \]
\end{definition}

\begin{proposition}\label{prop:generalisedIgnorableMomentumConserved}
	The generalised momentum associated to an ignorable coordinate is conserved.
\end{proposition}

\begin{proof}
	From the Euler-Lagrange equation for $q_i$,

	\[ 0 = \diff{}{t} \left( \diffp{L}{\dot{q_i}} \right) - \diffp{L}{q_i} = \diff{p_i}{t} - 0 = \diff{p_i}{t} \]
\end{proof}

\begin{example}
	For a free particle moving in $d$ dimensions, in Cartesian coordinates we have

	\[ L = T - V = \frac{1}{2} m \sum_{i = 1}^d \dot{x}_i^2 \]
	so every coordinate is ignorable. The generalised momenta are

	\[ p_i = \diffp{L}{\dot{x}_i} = m \dot{x}_i \]
	So here the conservation of generalised momenta is the conservation of the linear momenta.
\end{example}

\section{Symmetries, Noether's theorem and conservation laws}

\subsection{Ordinary symmetries}

\begin{definition}
	For a uniparametric family of smooth maps $\phi(\epsilon): C \rightarrow C$ from configuration space to itself, with $\phi(0)$ the identity map, this family of maps is called a \textbf{transformation depending on $\epsilon$}. In any coordinates system this transformation can be written as

	\[ q_i \rightarrow \phi_i(q_1, \dots, q_N, \epsilon) \]
	where the $\phi_i$'s are a set of $N := \dim(C)$ functions representing the transformation in the coordinate system. The change in velocities is defined as

	\[ \dot{q}_i \rightarrow \diff{}{t} \phi_i \]
\end{definition}

\begin{remark}
	$q_i'$ is used to denote $\phi(q_i, \epsilon)$, so often we write $q_i \rightarrow q_i' = \dots$, where $\dots$ is a function of $q_i$ and $\epsilon$.
\end{remark}

\begin{definition}
	The \textbf{generator} of $\phi$ is

	\[ \diff{\phi(\epsilon)}{\epsilon} \Big|_{\epsilon = 0} := \lim_{\epsilon \rightarrow 0} \frac{\phi(\epsilon) - \phi(0)}{\epsilon} \]
	In any coordinate system,

	\[ q_i \rightarrow \phi_i(\underline{q}, \epsilon) = q_i + \epsilon a_i(\underline{q}) + O(\epsilon^2) \]
	where

	\[ a_i = \diffp{\phi_i(\underline{q}, \epsilon)}{\epsilon} \Big|_{\epsilon = 0} \]
	is a function of the generalised coordinates. Hence the transformation generator is $a_i$. For the velocities the transformation is

	\[ \dot{q}_i \rightarrow \dot{q}_i + \epsilon a_i(q_1, \dots, q_N, \dot{q}_1, \dots, \dot{q}_N) + O(\epsilon^2) \]
	where the generator is $\dot{a}_i$.
\end{definition}

\begin{example}
	For a particle moving in two dimensions, the finite transformations given by rotations around the origin, in Cartesian coordinates, are
	\[
		\begin{aligned}
			& x \rightarrow x \cos(\epsilon) - y \sin(\epsilon) \\
			& y \rightarrow x \sin(\epsilon) + y \cos(\epsilon)
		\end{aligned}
	\]
	The associated infinitesimal transformations can be derived using the expansions $\sin(\epsilon) = \epsilon + O(\epsilon^3)$ and $\cos(\epsilon) = 1 + O(\epsilon^2)$. Then
	\[
		\begin{aligned}
			& x \rightarrow x - y \epsilon + O(\epsilon^2) \\
			& y \rightarrow y + x \epsilon + O(\epsilon^2)
		\end{aligned}
	\]
	Then the generators of the transformation are
	\[
		a_x = -y, \quad a_y = x, \quad \dot{a}_x = -\dot{y}, \quad \dot{a}_y = \dot{x}
	\]
\end{example}

\begin{lemma}
	Equations of motion do not change if the Lagrangian is modified by adding the total derivative of a function of coordinates and time, i.e.
	\[
		L \rightarrow L + \diff{F(q_1, \dots, q_N, t)}{t}
	\]
	results in the same equations of motion.
\end{lemma}

\begin{proof}
	The effect on the action is
	\[
		S = \int_{t_0}^{t_1} L dt \rightarrow S' = S + F(q_1(t_1), \dots, q_N(t_1), t_1) - F(q_1(t_0), \dots, q_N(t_0), t_0)
	\]
	From the action principle, we must have that $\delta S$ vanishes to first order in $\delta q_i(t)$, with the $q_i$'s fixed at the path's endpoints. Therefore $F(q_1(t_1), \dots, q_N(t_1), t_1)$ and $F(q_1(t_0), \dots, q_N(t_0), t_0)$ are fixed. Hence
	\[
		\begin{aligned}
			\delta S' & = S'(\underline{q} + \delta \underline{q}) - S'(\underline{q}) \\
			& = (S(\underline{q} + \delta \underline{q}) + F(q_1(t_1), \dots, q_N(t_1), t_1) - F(q_1(t_0), \dots, q_N(t_0), t_0)) \\
			& \quad - (S(\underline{q}) + F(q_1(t_1), \dots, q_N(t_1), t_1) - F(q_1(t_0), \dots, q_N(t_0), t_0)) \\
			& = S(\underline{q} + \delta\underline{q}) - S(\underline{q}) = \delta S
		\end{aligned}
	\]
	So the variation of the action is not affected, therefore the equations of motion cannot be affected.
\end{proof}

\begin{definition}
	A transformation $\phi(\epsilon)$ is a \textbf{symmetry} if, to first order in $\epsilon$, for some function $F(\underline{q}, t)$, the change in the Lagrangian is a total time derivative of $F(\underline{q}, t)$, i.e.

	\[
		L \rightarrow L + \epsilon \diff{F(\underline{q}, t)}{t} + O(\epsilon^2)
	\]
\end{definition}

\begin{remark}
	$F(\underline{q}, t)$ is only defined up to a constant. If some $F(\underline{q}, t)$ satisfies the above equation, then $G(\underline{q}, t) = F(\underline{q}, t) + c$ will also satisfy the equation.
\end{remark}

\begin{example}
	If $q_i$ is an ignorable coordinate, the transformation $q_i \rightarrow q_i + c_i$, where $c_i$ is a constant, is a symmetry, because $q_i$ does not appear in the Lagrangian by definition so here $F(\underline{q}, t) = 0$.
\end{example}

\begin{theorem}
	(Noether's theorem) In a given set of generalised coordinates, let $a_i(\underline{q})$ be the generator of a transformation such that
	\[
		L \rightarrow L + \epsilon \diff{F(\underline{q}, t)}{t} + O(\epsilon^2)
	\]
	so that it is a symmetry. Let
	\[
		Q := \left( \sum_{i = 1}^N a_i \diffp{L}{\dot{q}_i} \right) - F
	\]
	Then $\diff{Q}{t} = 0$, so $Q$ is conserved. $Q$ is called the \textbf{Noether charge}.
\end{theorem}

\begin{proof}
	Consider the variation of the action under the transformation $q_i \rightarrow q_i + \epsilon a_i$. Using the chain rule and then the Euler-Lagrange equations,
	\[
		\begin{aligned}
			\delta S
				& = \int_{t_0}^{t_1} \sum{i = 1}^N \left( \epsilon a_i \diffp{L}{q_i} + \epsilon \dot{a}_i \diffp{L}{\dot{q}_i} \right) dt + O(\epsilon^2) \\
				& = \int_{t_0}^{t_1} \sum_{i = 1}^N \left( \epsilon a_i \diff{}{t} \left( \diffp{L}{\dot{q}_i} \right) + \epsilon \dot{a}_i \diffp{L}{\dot{q}_i} \right) + O(\epsilon^2) \\
				& = \int_{t_0}^{t_1} \epsilon \diff{}{t} \left( \sum_{i = 1}^N a_i \diffp{L}{\dot{q}_i} \right) + O(\epsilon^2) \\
				& = \epsilon {\left[ \sum_{i = 1}^N a_i \diffp{L}{\dot{q}_i} \right]}_{t_0}^{t_1} + O(\epsilon^2)
		\end{aligned}
	\]
	Note that having used the Euler-Lagrange equations in the second line, the result is only valid along the path satisfying the equations of motion.

	Now since the transformation is a symmetry,
	\[
		\begin{aligned}
			\delta S
				& = S(\underline{q} + \delta \underline{q}) - S(\underline{q}) \\
				& = \int_{t_0}^{t_1} \left( \left( L + \epsilon \diff{F}{t} + O(\epsilon^2) \right) - L \right) \\
				& = \epsilon {[F]}_{t_0}^{t_1} + O(\epsilon^2)
		\end{aligned}
	\]
	Equating these two expressions for $\delta S$, we see that $Q(t_1) = Q(t_0)$. We didn't specify what $t_0$ and $t_1$ were so this equality holds for every $t_0$ and $t_1$. So let $t_1 = t_0 + \epsilon$, then
	\[
		Q(t_1) - Q(t_0) = Q(t_0 + \epsilon) - Q(t_0) = \epsilon \diff{Q}{t} + O(\epsilon^2) = 0
	\]
	hence $\diff{Q}{t} = 0$.
\end{proof}

\begin{example}
	If a coordinate $q_i$ is ignorable, there is a symmetry generated by $q_i \rightarrow q_i + \epsilon$ and leaving the other coordinates constant. So
	\[
		a_k = \delta_{ik} := \begin{cases}
			1 & \quad \text{if } i = k \\
			0 & \quad \text{otherwise}
		\end{cases}
	\]
	The Noether charge is
	\[
		Q = \sum_{k = 1}^{N} a_k \diffp{L}{\dot{q}_k} = \sum_{k = 1}^{N} \delta_{ik} \diffp{L}{\dot{q}_k} = \diffp{L}{\dot{q}_i}
	\]
	which agrees with Proposition \ref{prop:generalisedIgnorableMomentumConserved}.
\end{example}

\subsection{Energy conservation}

\begin{definition}
	Given a Lagrangian that explicitly depends on time, $L(\underline{q}, \underline{\dot{q}}, t)$, the \textbf{energy}, $E$, is defined as
	\[
		E := \left( \sum_{i = 1}^{N} \dot{q}_i \diffp{L}{\dot{q}_i} \right) - L
	\]
\end{definition}

\begin{theorem}
	Along a path $\underline{q}(t)$ which satisfies the equations of motion,
	\[
		\diff{E}{t} = -\diffp{L}{t}
	\]
\end{theorem}

\begin{proof}
	\[
		\begin{aligned}
			\diff{E}{t}
				& = \diff{}{t} \left( \left( \sum_{i = 1}^{N} \dot{q}_i \diffp{L}{\dot{q}_i} \right) - L \right) \\
				& = \sum_{i = 1}^{N} \left( \ddot{q}_i \diffp{L}{\dot{q}_i} + \dot{q}_i \diff{}{t} \left( \diffp{L}{\dot{q}_i} \right) \right) - \diff{L}{t} \\
				& = \sum_{i = 1}^{N} \left( \diffp{L}{\dot{q}_i} \ddot{q}_i + \dot{q}_i \diffp{L}{q_i} \right) - \diff{L}{t}
		\end{aligned}
	\]
	using the Euler-Lagrange equations. By the chain rule,
	\[
		\diff{L}{t} = \sum_{i = 1}^{N} \left( \diffp{L}{\dot{q}_i} + \diffp{L}{q_i} \dot{q}_i \right) + \diffp{L}{t}
	\]
	and so substituting this into the expression for $\diff{E}{t}$, we get
	\[
		\diff{E}{t} = -\diffp{L}{t}
	\]
\end{proof}

\begin{remark}
	Note that $\diffp{L}{t}$ means the partial derivative of $L$ with respect to $t$, \textbf{keeping $\underline{q}$ and $\underline{\dot{q}}$ fixed}. This is because the Lagrangian is (here) just an ordinary function of three parameters which are unrelated, so $t$ is independent of $\underline{q}$ and $\underline{\dot{q}}$. The parameters only become related when the Lagrangian is used to build the action. (See \hyperref[rem:lagrangianParametersIndependent]{this remark}).
\end{remark}

\begin{corollary}
	Energy is conserved iff the Lagrangian does not depend explicitly on time.
\end{corollary}

\begin{proof}
	Energy is conserved iff $\diff{E}{t} = 0$, and $\diffp{L}{t} = 0$ iff the Lagrangian does not depend explicitly on time.
\end{proof}

\section{Normal modes}

\subsection{Canonical kinetic terms}

\begin{definition}
	Given a Lagrangian $L = T - V$, $T$ is a \textbf{canonical} kinetic term if it is of the form
	\[
		T = \frac{1}{2} \sum_{i = 1}^{n} \dot{q}_i^2
	\]
\end{definition}

Given a Lagrangian with a canonical kinetic term, assume $\underline{q} = \underline{0}$ is a stationary point of $V(\underline{q})$, so
\[
	\diffp{V}{q_i} \Big|_{\underline{q} = 0} = 0 \quad \forall i \in \{ 1, \dots, n \}
\]

If $\underline{q} = \underline{0}$ is not a stationary point but $\underline{q} = \underline{a}$ for some $\underline{a}$ is, then the new variables defined as $q_i' = q_i - a_i$ are such that the stationary point is at $\underline{q}' = \underline{0}$. This preserves the form of the Lagrangian, so we can assume $\underline{q} = 0$ is a stationary point.

To determine the motion around this extremum, expand $V(\underline{q})$ to second order in $\underline{q}$ to define an approximate Lagrangian:
\[
	L_{\text{approx}} = \frac{1}{2} \sum_{i = 1}^{n} \dot{q}_i^2 - \frac{1}{2} \sum_{i, j} A_{i, j} q_i q_j
\]
where
\[
	A_{i, j} = \diffp[1, 1]V{q_i, q_j} \Big|_{\underline{q} = 0}
\]
The Euler-Lagrange equations for $L_{\text{approx}}$ are given in matrix notation by
\[
	\underline{\ddot{q}} + A \underline{q} = D_A \underline{q} = \left( \diff[2]{}{t} + A \right) \underline{q} = 0
\]
where $D_A$ is defined as $D_A := \diff[2]{}{t} + A$, which is a linear operator ($D_A (\underline{a} + \underline{b}) = D_A \underline{a} + D_A \underline{b}$ and $D_A (c \underline{a}) = c D_A \underline{a}$ for any vectors $a$ and $b$ and any $c \in \mathbb{R}$).

$A$ is an $n \times n$ matrix which is real and symmetric, due to symmetry of second partial derivatives (we assume $V$ has continuous second partial derivatives). So $A$ has $n$ real eigenvaules and eigenvectors. Let the eigenvalues be $\lambda^{(i)}$ and the corresponding eigenvectors be $\underline{v}^{(i)}$, then
\[
	A \underline{v}^{(i)} = \lambda^{(i)} \underline{v}^{(i)}
\]
Define the ansatz (an assumed solution)
\[
	\underline{q}^{(i)} (t) = f^{(i)} (t) \underline{v}^{(i)}
\]
for some function $f^{(i)} (t)$ to be determined. Then
\[
	\begin{aligned}
		\left( \diff[2]{}{t} + A \right) \underline{q}^{(i)} (t)
			& = \left( \diff[2]{}{t} + A \right) f^{(i)} (t) \underline{v}^{(i)}
			& = \underline{v}^{(i)} \left( \diff[2]{}{t} + \lambda^{(i)} \right) f^{(i)} (t)
			& = 0
	\end{aligned}
\]
and so since $\underline{v}^{(i)} \ne 0$,
\[
	\left( \diff[2]{}{t} + \lambda^{(i)} \right) f^{(i)} (t) = 0
\]
The solution to this equation is
\[
	f^{(i)} (t) = \begin{cases}
		\alpha^{(i)} \cos(\sqrt{\lambda^{(i)}} t) + \beta^{(i)} \sin(\sqrt{\lambda^{(i)}} t) & \quad \text{if } \lambda^{(i)} > 0 \\
		C^{(i)} t + D^{(i)} & \quad \text{if } \lambda^{(i)} = 0 \\
		\alpha^{(i)} \cosh(\sqrt{-\lambda^{(i)}} t) + \beta^{(i)} \sinh(\sqrt{-\lambda^{(i)}} t) & \quad \text{if } \lambda^{(i)} < 0 \\
	\end{cases}
\]
where $\alpha^{(i)}, \beta^{(i)}, C^{(i)}, D^{(i)}$ are constants determined by initial conditions.

The behaviour this solution describes depends on the sign of the $\lambda^{(i)}$. If every $\lambda^{(i)}$ is positive, there is a local minimum and there are oscillations around this minimum. If there is a negative eigenvalue, there is exponential behaviour away from the stationary point, which matches with the intuitioin that small perturbations at a maximum will quickly grow. Zero eignvaleues correspond with motion with constant velocity, with no oscillations.

The general solution is
\[
	\underline{q}(t) = \sum_{i = 1}^{N} \underline{v}^{(i)} f^{(i)} (t)
\]

\begin{definition}
	Given an eigenvalue $\lambda^{(i)} > 0$, a \textbf{normal mode} is a solution to
	\[
		\underline{q}(t) = \underline{v}^{(i)} \left( \alpha^{(i)} \cos(\sqrt{\lambda^{(i)}} t) + \beta^{(i)} \sin(\sqrt{\lambda^{(i)}} t) \right)
	\]
\end{definition}

\begin{definition}
	Given an eigenvalue $\lambda^{(i)} = 0$, a \textbf{zero mode} is a solution to
	\[
		\underline{q}(t) = \underline{v}^{(i)} \left( C^{(i)} t + D^{(i)} \right)
	\]
\end{definition}

\begin{definition}
	Given an eigenvalue $\lambda^{(i)} < 0$, an \textbf{instability} is a solution to
	\[
		\underline{q}(t) = \underline{v}^{(i)} \left( \alpha^{(i)} \cosh(\sqrt{-\lambda^{(i)}} t) + \beta^{(i)} \sinh(\sqrt{-\lambda^{(i)}} t) \right)
	\]
\end{definition}

\begin{definition}
	When there are no instabilities, the general solution is the superposition of normal modes for non-zero eigenvalues and zero modes:
	\[
		\underline{q}(t) = \sum_{i = 1, \ \lambda^{(i)} \ne 0}^{n} \underline{v}^{(i)} \left( \alpha^{(i)} \cos(\sqrt{\lambda^{(i)}} t) + \beta^{(i)} \sin(\sqrt{\lambda^{(i)}} t) \right) + \sum_{i = 1, \ \lambda^{(i)} = 0}^{n} \underline{v}^{(i)} \left( C^{(i)} t + D^{(i)} \right)
	\]
\end{definition}

\section{Hamiltonian Formalism}

\begin{definition}
	The classical \textbf{state} of a system at a given instant in time is a \textbf{complete} set of data that fully specifies the future evolution of the system.
\end{definition}

\begin{remark}
	\textbf{Any} set of data that fully fixes future evolution is valid.
\end{remark}

\begin{definition}
	The \textbf{phase (or state) space} is the set of all possible states for a system at a given time.
\end{definition}

\begin{example}
	A free particle moving in $\mathbb{R}$. The phase space is $\mathbb{R}^2$ ($\mathbb{R}$ for position, $\mathbb{R}$ for velocity).
\end{example}

\begin{definition}
	The \textbf{Hamiltonian formalism} studies dynamics in a phase space, parameterised by $\underline{q}(t)$ and $\underline{p}(t)$, where $p_i = \diffp{L}{\dot{\underline{q_i}}}$, the momentum.
\end{definition}

\begin{example}
	A particle moving in $\mathbb{R}$, with $L(x, \dot{x}) = \frac{1}{2} m \dot{x}^2$.

	Then $p_x = \diffp{L}{\dot{x}} = m \dot{x}$ so $\dot{x}(x, p_x) = \frac{p_x}{m}$.

	In the Hamltonian formalism, $L(x, p_x) = \frac{p_x^2}{2m}$.
\end{example}

\begin{example}
	A particle moving in $\mathbb{R}^2$ (in polar coordinates).

	$L(r, \theta, \dot{r}, \dot{\theta}) = \frac{1}{2} m (\dot{r}^2 + r^2 \dot{\theta}^2)$. So $p_r = m\dot{r}$ and $p_{\theta} = m r^2 \dot{\theta}$.

	So $\dot{r}(r, \theta, p_r, p_{\theta}) = \frac{p_r}{m}$, $\dot{\theta}(r, \theta, p_r, p_{\theta}) = \frac{p_{\theta}}{m r^2}$.

	$L(r, \theta, \dot{r}, \dot{\theta}) = L(r, \theta, p_r, p_{\theta}) = \frac{1}{2} (\frac{p_r^2}{m} + \frac{p_{\theta}^2}{m r^2})$.
\end{example}

\begin{definition}
	Given two functions $f(\underline{q}, \underline{p}, t)$ and $g(\underline{q}, \underline{p}, t)$ in phase space their \textbf{Poisson bracket} is:

	\[ \{f, g\} := \sum_{i = 1}^n \left( \diffp{f}{q_i} \diffp{g}{p_i} - \diffp{f}{p_i} \diffp{g}{q_i} \right)\] where $n$ is the dimension of the configuration space.
\end{definition}

\begin{remark}
	In the Hamiltonian formalism, $\diffp{q_i}{p_j} = \diffp{p_j}{q_i} = 0$.

	Similarly, $\diffp{q_i}{q_j} = \diffp{p_i}{p_j} = \delta_{i, j}$
\end{remark}

\begin{example}
	Let $f = q_i$, $g = q_j$. $\{q_i, q_j\} = 0$, and $\{p_i, p_j\} = 0$. $\{q_i, p_j\} = \sum_{k = 1}^n \delta_{i, j} \delta_{j, k} = \delta_{i, j}$.
\end{example}

\begin{definition}
	Let $\mathbb{F}$ be the set functions from a phase space $P$ to $\mathbb{R}$
\end{definition}

\begin{definition}
	The Hamiltonian flow $\Phi_f^{(s)}$, with $(s) \in \mathbb{R}$, $f \in F$ operator maps $\mathbb{F}$ to $\mathbb{F}$ and is defined as

	\[ \Phi_f^{(s)} (g) := e^{s \{\cdot, f\}} g := g + s \{g, f\} + \frac{s^2}{2} \{ \{g, f\}, f\} + \cdots \]
\end{definition}

\begin{remark}
	The transformation generated by $f$ has generator $a_i = \{q_i, f\}$ where $q_i \rightarrow q_i + \epsilon a_i$.

	Infinitesimally, $\Phi_f^{(s)} (g) := g + \epsilon \{g, f\} + O(\epsilon^2)$
\end{remark}

TODO: properties on poisson bracket

\begin{example}
	(Rotation in $\mathbb{R}^2$ in Cartesian coordinates) As a guess, choose $f = q_1 \dot{q_2} - \dot{q_1} q_2$, the angular momentum.

	$L = \frac{1}{2} (\dot{q_1}^2 + \dot{q_2}^2) - V(q_1, q_2)$ so $p_1 = \diffp{L}{\dot{q_1}} = \dot{q_1}$ and $p_2 = \diffp{L}{\dot{q_2}} = \dot{q_2} \Rightarrow f = q_1 p_2 - q_2 p_1$.

	Then $q_1 \rightarrow q_1 + \epsilon \{ q_1, f \} + O(\epsilon^2) = q_1 + \epsilon \{ q_1, q_1 p_2 - q_2 p_1 \} = q_1 + \epsilon \{ q_1, q_1 p_2 \} - \epsilon \{ q_1, q_2 p_1 \} = q_1 + \epsilon \{q_1, q_1\} p_2 + \epsilon \{q_1, p_2\} q_1 - \epsilon \{q_1, q_2\} p_1 - \epsilon \{q_1, p_1\} q_2 = q_1 - \epsilon q_2$

	Similarly, $q_2 \rightarrow q_2 + \epsilon q_1$ so $(q_1, q_2) \rightarrow (q_1, q_2) + \epsilon ((0, -1), (1, 0)) (q_1, q_2)$ TODO make into matrices and column vectors.
\end{example}

\begin{definition}
	The \textbf{Hamiltonian} is the energy expressed in Hamiltonian coordinates:

	\[ H = \sum_{i = 1}^n \dot{q_i(\underline{q}, \underline{p})} p_i - L(\underline{q}, \dot{\underline{q}} (\underline{q}, \underline{p})) \]
\end{definition}

\begin{example}
	(Harmonic oscillator in one dimension) Let $\frac{1}{2} m \dot{x}^2 - \frac{1}{2} k x^2 \Rightarrow p = m\dot{x} \Rightarrow \dot{x} = \frac{p}{m}$.

	$H = \dot{x} p - L = \frac{p^2}{m} - (\frac{1}{2} \frac{p^2}{m} - \frac{1}{2} k x^2) = \frac{1}{2} \frac{p^2}{m} + \frac{1}{2} k x^2$
\end{example}

\begin{theorem}
	The time evolution of the phase space coordinates $\underline{q}, \underline{p}$ is generated by Hamiltonian flow $\Phi_H$:

	\[ q_i(t + a) = \Phi_H^{(a)} q_i(t), p_i(t + a) = \Phi_H^{(a)} p_i(t) \]
	Infinitesimally, $q_i(t) + \epsilon \dot{q_i}(t) + O(\epsilon^2) = q_i(t + \epsilon) = q_i(t) + \epsilon \{ q_i, H \} + O(\epsilon^2) \Leftrightarrow \dot{q_i} = \{q_i, H \} = \diffp{H}{p_i}$ and similarly, $\dot{p_i} = \{ p_i, H \} = -\diffp{H}{q_i}$.
	
	These equations are called \textbf{Hamilton's equations}.
\end{theorem}

\begin{proof}
	$\diffp{H}{q_i}$. TODO: complete this proof, finish rest of notes from lecture.
\end{proof}

\begin{corollary}
	The time evolution of any function $f(\underline{q}, \underline{p})$ in phase space is generated by $\Phi_H$:

	\[ \diff{f}{t} = \{ f, H \} \]
	If $f(\underline{q}, \underline{p}, t)$ depends explicitly on time then

	\[ \diff{f}{t} = \{ f, h \} + \diffp{f}{t} \]
\end{corollary}

\begin{proof}
	$\diff{f}{t} = \sum_{i = 1}^n \left( \diffp{f}{q_i} \dot{q_i} + \diffp{f}{p_i} \dot{p_i} \right) + \diffp{f}{t} = \sum_{i = 1}^n \left( \diffp{f}{q_i} \diffp{H}{p_i} - \diffp{f}{p_i} \diffp{H}{q_i} \right) + \diffp{f}{t} = \{ f, H \} + \diffp{f}{t}$.
\end{proof}

\section{Quantum mechanics introduction}

\begin{definition}
	The \textbf{photo-electric effect} is the phenomena where, when a light source is shone on a metal, electrons are released from the surface of the metal.
\end{definition}

\begin{definition}
	The \textbf{classical} description of light would lead to these results:
	\begin{itemize}
		\item The energy of the released electrons depends on the intensity of light $I$ but is independent of the angular frequency of the light, $\omega$.
		\item Electrons are released for any value of $I$
	\end{itemize}
\end{definition}

\begin{definition}
	The quantum mechanics prediction of the photoelectric effect is that the energy of the electrons $E$ is independent of $I$, and not released at all below some value of angular frequency.

	This led to the discovery that light consists of packets (photons).
\end{definition}

\section{The wave function and probabilities}

\begin{definition}
	The \textbf{wave function} is a function of position $x$ and time $t$:
	\[
		\psi: \mathbb{R}^2 \rightarrow \mathbb{C}
	\]
\end{definition}

\begin{remark}
	We require that $\psi$ is continuous, and that $\diff{\psi(x)}{x}$ is continuous ($\psi$ must be differentiable), except when the potential energy $V(x)$ is not finite. 
\end{remark}

\begin{definition}
	The \textbf{probability density} of finding a particle at position $x$ and time $t$ is defined as
	\[
		P(x, t) := |\psi(x, t)|^2
	\]

	The probability of finding the particle at $x \in [a, b]$ is therefore
	\[
		\int_a^b |\psi(x, t)|^2 dx
	\]
	The probability of finding the particle anywhere is must be equal to $1$, which means that
	\[
		\int_{-\infty}^{\infty} P(x, t) dx = 1 \quad \forall t \in \mathbb{R}
	\]
	This puts a constraint on $\psi$.
\end{definition}

\begin{definition}
	A wave function $\psi$ is \textbf{square-normalisable} if the integral
	\[
		\int_{-\infty}^{\infty} P(x, t) dx
	\]
	exists.
\end{definition}

\begin{definition}
	A wave function $\psi$ is \textbf{normalised} if
	\[
		\int_{-\infty}^{\infty} P(x, t) dx = 1 \quad \forall t \in \mathbb{R}	
	\]
\end{definition}

\begin{definition}
	For a given time $t$, the \textbf{expectation} value of a polynomial function $f(x)$ of position $x$, is defined as
	\[
		\langle f(x) \rangle := \int_{-\infty}^{-\infty} P(x, t) f(x) dx
	\]
\end{definition}

\begin{remark}
	The expectation of position, $\langle x \rangle$ is the mean of measurements of the positions of many particles with wave function $\psi$.
\end{remark}

\begin{definition}
	The \textbf{uncertainty} of $x$ is defined as
	\[
		\Delta x = \sqrt{\langle x^2 \rangle - \langle x \rangle ^2}
	\]
\end{definition}

\subsection{Examples of wave functions}

\begin{remark}
	For now, we will look at wave functions at a fixed time, $\psi(x)$.
\end{remark}

\begin{definition}
	The \textbf{Gaussian wave function} is defined as
	\[
		\psi(x) = C e^{-x^2 / 4 \Delta^2}
	\]
	where $\Delta > 0$ has units of length and $C$ is a normalisation constant.
	\[
		\int_{-\infty}^{\infty} |\psi(x)|^2 = 1
	\]
	so $C = e^{i \theta} {(2 \pi \Delta^2)}^{-1/4}$, with $\theta \in \mathbb{R}$. $\theta$ is a free parameter here, so is often set to $0$.

	So the normalised probability distribution is
	\[
		P(x) = \frac{1}{\sqrt{2 \pi \Delta^2}} e^{-x^2 / (2 \Delta^2)}
	\]
	which is a standard Gaussian probability distribution.
\end{definition}

\begin{remark}
	For the Gaussian wave function:
	\begin{itemize}
		\item $\langle x^{2n + 1} \rangle \ \forall n \in \mathbb{N}_0$ since the integral is an odd function of $x$ over a symmetric interval.
		\item As $\Delta$ is the only quantity with units of length and $x$ has units of length, $\langle x^{2n} \rangle \propto \Delta^{2n} \ \forall n \in \mathbb{N}_0$.
		\item $\langle x^2 \rangle = \Delta^2 \Longrightarrow \Delta x = \Delta$.
	\end{itemize}
\end{remark}

\begin{definition}
	If a particle's position $x$ is confined to $0 < x < L$, particle is said to be in an \textbf{infinite potential well}, defined as
	\[
		V(x) = \begin{cases}
			0 & \text{ if } 0 < x < L \\
			\infty & \text{ otherwise}
		\end{cases}
	\]
	To have position $x \le 0$ or $x \ge L$, the particle would need infinite energy, so the probability of finding it there is $0$, therefore the wave function for the particle should vanish in these regions.
\end{definition}

\begin{example}
	A possible wave function for a particle in an infinite potential well is
	\[
		\Psi(x) = \begin{cases}
			C \sqrt{x(L - x)} & \text{ if } 0 < x < L \\
			0 & \text{ otherwise }
		\end{cases}
	\]
	For some constant $C$. The probability of finding the particle anywhere in $(0, L)$ is $1$, so
	\[
		\int_{0}^{L} |\psi(x)|^2 dx = |C|^2 \int_{0}^{L} x(L - x) dx = |C|^2 \frac{L^3}{6} = 1
	\]
	which implies $C = \sqrt{6 / L^3} e^{i \theta}$ for any $\theta$. We choose $\theta = 0$ here.
\end{example}

\begin{remark}
	\textbf{Important:} multiplying a wave function $\psi(x)$ by $e^{i \theta}$ for any $\theta \in \mathbb{R}$ does not change the probability density function $P$, which means that measuring position cannot distinguish between $\psi(x)$ and $e^{i \theta} \psi(x)$.
	
	Measuring momentum can distinguish between these two, unless $\theta(x)$ is constant, in which case they cannot be distinguished by any measurement, and they describe the same physical state.
\end{remark}

\subsection{Collapse of the wave function}

\begin{definition}
	If a particle has a wave function $\psi(x, t)$ for $t < t_0$, then at $t = t_0$, the position $x$ is measured to be $x = x_0$. Then another measurement immediately after the first will be $x = x_0$ with probability $1$. This phenomenon is called the \textbf{wave function collapse}. Measuring $x$ changes the wave function so that it is very localised around $x_0$. This is why $\langle x \rangle$ is the average of measurements of many particles with the same wave function, not repeated measurements of the same wave function.
\end{definition}

\section{Momentum and Planck's constant}

\begin{definition}
	The \textbf{momentum operator} $\hat{p}$ is defined as
	\[
		\hat{p} (\psi(x)) := -i \hbar \diffp{}{x} (\psi(x))
	\]
	so that
	\[
		\begin{aligned}
			\delta_{\epsilon} \psi(x)
				& := \psi(x - \epsilon) - \psi(x) \\
				& = -\epsilon \diffp{}{x} \psi(x) + O(\epsilon^2) \\
				& = -\epsilon \frac{i}{\hbar} \hat{p} (\psi(x))
		\end{aligned}
	\]
	where $\hbar$ is a constant of proportionality.
\end{definition}

\begin{definition}
	In the above definition, $\hbar$ is called the \textbf{reduced Planck constant} which has units $\text{energy} \times \text{time}$. \textbf{Planck's constant} is defined as
	\[
		h = 2 \pi \hbar
	\]
\end{definition}

\begin{definition}
	We define a \textbf{position operator} $\hat{x}$ which multiplies a wave function by $x$:
	\[
		\hat{x} (\psi(x)) = x \psi(x)
	\]
\end{definition}

\begin{definition}
	The \textbf{commutator} of the position and momentum operators $\hat{x}$ and $\hat{p}$ is defined as
	\[
		\begin{aligned}
			\relax [\hat{x}, \hat{p}] (\psi(x))
				& = \hat{x} (\hat{p} (\psi(x))) - \hat{p} (\hat{x} (\psi(x))) \\
				& = \hat{x} \left( -i \hbar \diffp{}{x} \psi(x) \right)	+ i \hbar \diffp{}{x} (x \psi(x)) \\
				& = i \hbar \psi(x)
		\end{aligned}
	\]
	The relation $[\hat{x}, \hat{p}] (\psi(x)) = i \hbar \psi(x)$ is called the \textbf{canonical commutation relation}.
\end{definition}

\begin{remark}
	This commutator is similar to the Poisson bracket, where $\{ x, p \} = 1$ in classical mechanics. So we can transform a classical system into a quantum system by replacing the Poisson bracket $\{ , \}$ with $-\frac{i}{\hbar} [,]$. This replacement is called \textbf{canonical quantisation}.
\end{remark}

\begin{definition}
	The \textbf{expectation value of momentum}, $\langle p \rangle$ is defined as
	\[
		\begin{aligned}
			\langle p \rangle
				& = \int_{-\infty}^{\infty} \overline{\psi(x, t)} \hat{p} (\psi(x, t)) dx \\
				& = -i \hbar \int_{-\infty}^{\infty} \overline{\psi(x, t)} \diffp{}{x} \psi(x, t) dx
		\end{aligned}
	\]
\end{definition}

\begin{remark}
	Notice the similarity between this definition and the one for the expectation value of position:
	\[
		\begin{aligned}
			\langle x \rangle
				& = \int_{-\infty}^{\infty} x |\psi(x, t)|^2 dx \\
				& = \int_{-\infty}^{\infty} \overline{\psi(x, t)} \hat{x} (\psi(x, t)) dx
		\end{aligned}
	\]
	As with $\langle x \rangle$, $\langle p \rangle$ is interpreted as the average of measurements of the momenta of many particles with the same wave function $\psi$.
\end{remark}

\begin{proposition}
	$\hbar$ must be a real number (it is not complex).
\end{proposition}

\begin{proof}
	The measurement of momentum must be a real number, so $\langle p \rangle \in \mathbb{R}$. Using integration by parts and the fact that $|\psi(x, t)|^2$ vanishes as $x \rightarrow \pm \infty$ if $\psi$ is square normalisalbe, the complex conjuate of $\langle p \rangle$ is
	\[
		\begin{aligned}
			\overline{\langle p \rangle}
				& = i \bar{\hbar} \int_{-\infty}^{\infty} \psi(x, t) \diffp{}{x} \overline{\psi(x, t)} dx \\
				& = -i \bar{\hbar} \int_{-\infty}^{\infty} \overline{\psi(x, t)} \diffp{}{x} \psi(x, t) dx + i \hbar {[|\psi(x, t)|^2]}_{-\infty}^{\infty} \\
				& = -i \bar{\hbar} \int_{-\infty}^{\infty} \overline{\psi(x, t)} \diffp{}{x} \psi(x, t) dx \\
				& = \frac{\bar{\hbar}}{\hbar} \langle p \rangle
		\end{aligned}
	\]
	Therefore $\langle p \rangle \in \mathbb{R} \Longleftrightarrow \hbar \in \mathbb{R}$, so $\hbar$ must be real.
\end{proof}

\begin{definition}
	The \textbf{expectation value} of any polynomial $f$ of $p$ can be calculated as
	\[
		\begin{aligned}
			\langle f(p) \rangle
				& = \int_{-\infty}^{\infty} \overline{\psi(x, t)} f(\hat{p}) \psi(x, t) dx \\
				& = \int_{-\infty}^{\infty} \overline{\psi(x, t)} f \left( -i \hbar \diffp{}{x} \right ) (\psi(x, t)) dx
		\end{aligned}
	\]
\end{definition}

\begin{definition}
	The \textbf{momentum uncertainty}, $\Delta p$ is defined as
	\[
		\Delta p = \sqrt{\langle p^2 \rangle - {\langle p \rangle}^2}
	\]
	This quantity describes the spread around $\langle p \rangle$ of measurements of the momenta of many particles with the same wave function $\psi$.
\end{definition}

\begin{example}
	For the Gaussian wave function, $\langle x \rangle = 0$ and $\langle x^2 \rangle = \Delta^2$ so the uncertainty in position is $\Delta x = \Delta$. The momentum operator applied to $\psi$ gives
	\[
		\begin{aligned}
			\hat{p} (\psi(x)) & = \frac{i \hbar}{2 \Delta^2} x \psi(x) \\
			\hat{p}^2 (\psi(x))
				& = -\hbar^2 \diffd[2]{}{x} (\psi(x)) \\
				& = \frac{\hbar^2}{2 \Delta^2} \psi(x) - \frac{\hbar^2}{4 \Delta^4} x^2 \psi(x)
		\end{aligned}
	\]
	Notice that the momentum operator always gives a polynomial in $x$ multiplied by $\psi(x)$, so we can use the position expectations to calculate the momentum expectations:
	\[
		\begin{aligned}
			\langle p \rangle & = \frac{i \hbar}{2 \Delta^2} \int_{-\infty}^{\infty} x |\psi(x)|^2 dx = \frac{i \hbar}{2 \Delta^2} \langle x \rangle = 0 \\
			\langle p^2 \rangle
				& = \frac{\hbar^2}{2 \Delta^2} \int_{-\infty}^{\infty} |\psi(x)|^2 dx - \frac{\hbar^2}{4 \Delta^4} \int_{-\infty}^{\infty} x^2 |\psi(x)|^2 dx \\
				& = \frac{\hbar^2}{2 \Delta^2} - \frac{\hbar^2}{4 \Delta^4} \langle x^2 \rangle \\
				& = \frac{\hbar^2}{2 \Delta^2} - \frac{\hbar^2}{4 \Delta^4} \Delta^2 \\
				& = \frac{\hbar^2}{4 \Delta^2}
		\end{aligned}
	\]
	So the uncertainty in momentum is
	\[
		\Delta p = \sqrt{\langle p^2 \rangle - {\langle p \rangle}^2} = \frac{\hbar}{2 \Delta}
	\]
	So the product of the uncertainties of position and momentum is a constant, independent of $\Delta$:
	\[
		\Delta x \Delta p = \frac{\hbar}{2}
	\]
	So as the uncertainty in position or momentum decreases, the other must increase.
\end{example}

\begin{definition}
	\textbf{Heisenberg's uncertainty principle} states that for every normalised wave function,
	\[
		\Delta x \Delta p \ge \frac{\hbar}{2}
	\]
\end{definition}

\begin{remark}
	Heisenberg's uncertainty principle shows that there is a limit on the how much the uncertainty in both position and momentum can be reduced.
\end{remark}

\begin{definition}
	A \textbf{minimal uncertainty wave function} is a wave function that minimises the quantity $\Delta x \Delta p$, i.e. $\Delta x \Delta p = \frac{\hbar}{2}$.
	
	The Gaussian wave function is an example of this.
\end{definition}

\subsection{The Hilbert space}

\begin{definition}
	A \textbf{Hermitian inner product} on a vector space $V$ is a map $\langle \cdot, \cdot \rangle: V \times V \rightarrow \mathbb{C}$ which satisifes the following properties:
	\begin{enumerate}
		\item $\forall v, w \in V, \langle v, w \rangle = \overline{\langle w, v \rangle}$.
		\item $\forall v, w_1, w_2 \in V, a_1, a_2 \in \mathbb{C}, \langle v, a_1 w_1 + a_2 w_2 \rangle = a_1 \langle v, w_1 \rangle + a_2 \langle v, w_2 \rangle$.
		\item $\forall v, w_1, w_2 \in V, a_1, a_2 \in \mathbb{C}, \langle a_1 w_1 + a_2 w_2, v \rangle = \overline{a_1} \langle w_1, v \rangle + \overline{a_2} \langle w_2, v \rangle$.
		\item $\forall v \in V, \langle v, v \rangle \ge 0$ and $\langle v, v \rangle = 0 \Longleftrightarrow v = 0$.
	\end{enumerate}
\end{definition}

\begin{remark}
	A basis $\{ e_j \}$ is an orthonormal basis for an $n$-dimensional vector space $V$ if
	\[
		\langle e_i, e_j \rangle = \delta_{i, j}
	\]
	This means for every $v \in V$,
	\[
		v = \sum_{i = 1}^{n} v_i e_i
	\]
	where the $i$th component of the vector is $v_i = \langle v, e_i \rangle$. The Hermitian inner product of $v, w \in V$ is therefore
	\[
		\langle v, w \rangle = \sum_{i = 1}^{n} \overline{v_i} w_i
	\]
\end{remark}

\begin{definition}
	The \textbf{norm} of a vector $v \in V$ is defined as
	\[
		|v| := \sqrt{\langle v, v \rangle}
	\]
\end{definition}

\begin{remark}
	With an orthonormal basis, the squared norm of $v$ is simply
	\[
		|v|^2 := \langle v, v \rangle = \sum_{i = 1}^{n} |v_i|^2
	\]
\end{remark}

\begin{definition}
	At a fixed time $t$, a wave function $\psi: \mathbb{R} \rightarrow \mathbb{C}$ is called \textbf{square normalisable} if
	\[
		\int_{-\infty}^{\infty} |\psi(x)|^2 dx < \infty
	\]
	which means it has a probabilistic interpretation (as we can multiply it by a constant to make the probability of finding a particle anywhere equal to $1$).
\end{definition}

\begin{proposition}
	The set of continuous square-integrable wave functions forms a complex vector space. In particular, for all square-integrable wave functions $\psi_1, \psi_2$ and for every $a_1, a_2 \in \mathbb{C}$, the wave function
	\[
		a_1 \psi_1 + a_2 \psi_2
	\]
	is also square-integrable.
\end{proposition}

\begin{proof}
	Clearly, if $\psi_1$ is square-integrable then for every $a_1 \in \mathbb{C}$, $a_1 \psi_1$ is square-integrable. So now we must prove $\psi_1 + \psi_2$ is square-integrable. For every $x \in \mathbb{R}$,
	\[
		\begin{aligned}
			|\psi_1(x) + \psi_2(x)|^2
				& = \langle \psi_1(x) + \psi_2(x), \psi_1(x) + \psi_2(x) \rangle \\
				& = \langle \psi_1(x) + \psi_2(x), \psi_1(x) \rangle + \langle \psi_1(x) + \psi_2(x), \psi_2(x) \rangle \\
				& = \langle \psi_1(x), \psi_1(x) \rangle + \langle \psi_2(x), \psi_1(x) \rangle + \langle \psi_1(x), \psi_2(x) \rangle + \langle \psi_2(x), \psi_2(x) \rangle \\
				& = \langle \psi_1(x), \psi_1(x) \rangle + \overline{\langle \psi_1(x), \psi_2(x) \rangle} + \langle \psi_1(x), \psi_2(x) \rangle + \langle \psi_2(x), \psi_2(x) \rangle \\
				& = |\psi_1(x)|^2 + |\psi_2(x)|^2 + 2 \text{Re}(\overline{\psi_1(x)} \psi_2(x)) \\
				& \le |\psi_1(x)|^2 + |\psi_2(x)|^2 + 2 |\overline{\psi_1(x)} \psi_2(x)| \\
				& \le |\psi_1(x)|^2 + |\psi_2(x)|^2 + 2 |\overline{\psi_1(x)}| |\psi_2(x)| \\
				& = |\psi_1(x)|^2 + |\psi_2(x)|^2 + (|\psi_1(x)|^2 + |\psi_2(x)|^2) - {(|\psi_1(x)| + |\psi_2(x)|)}^2 \\
				& \le 2 |\psi_1(x)|^2 + 2 |\psi_2(x)|^2
		\end{aligned}
	\]
	Hence,
	\[
		\int_{-\infty}^{\infty} |\psi_1(x) + \psi_2(x)|^2 dx \le 2 \int_{-\infty}^{\infty} |\psi_1(x)|^2 + 2 \int_{-\infty}^{\infty} |\psi_2(x)|^2
	\]
	Hence $\psi_1 + \psi_2$ is square-integrable.
\end{proof}

\begin{definition}
	We define an \textbf{inner product} over the set of \textbf{wave functions} as
	\[
		\langle \psi_1, \psi_2 \rangle := \int_{-\infty}^{\infty} \overline{\psi_1(x)} \psi_2(x) dx
	\]
\end{definition}

\begin{proposition}
	The inner product defined above is a Hermitian inner product.
\end{proposition}

\begin{proof}
	Properties 1, 2 and 3 are clear from the definition. For property 4,
	\[
		\langle \psi, \psi \rangle := \int_{-\infty}^{\infty} |\psi(x)|^2 dx \ge 0
	\]
	as the integrand $|\psi(x)|^2$ is non-negative. If $\langle \psi, \psi \rangle = 0$, then $|\psi(x)|^2 = 0$ as $|\psi(x)|^2$ is continuous, hence $\psi(x) = 0$.
\end{proof}

\begin{definition}
	The vector space of wave functions with the Hermitian inner product above form a \textbf{Hilbert space}.
\end{definition}

\begin{definition}
	We define an \textbf{orthonormal basis for the set of wave functions} to be a set of wave functions $\{ \phi_n(x) \}$ such that
	\[
		\langle \phi_m, \phi_n \rangle = \delta_{m, n}
	\]
	and every continuous square-integrable wave function $\psi$ can be expressed uniquely as
	\[
		\psi(x) = \sum_n c_n \phi_n(x)
	\]
	where
	\[
		c_n = \langle \phi_n, \psi \rangle = \int_{-\infty}^{\infty} \overline{\phi_n(x)} \psi(x)
	\]
	This allows us to express the Hermitian inner product in terms of the $c_n$:
	\[
		\langle \psi_1, \psi_2 \rangle := \int_{-\infty}^{\infty} \overline{\psi_1(x)} \psi_2(x) dx = \sum_n \overline{c_{1, n}} c_{2, n}
	\]
	which gives the squared norm as
	\[
		\langle \psi, \psi \rangle = \int_{-\infty}^{\infty} |\psi(x)|^2 dx = \sum_n |c_n|^2
	\]
\end{definition}

\begin{example}\label{exa:infinitePotentialWellBasisFunctions}
	For an infinite potential well in the region $(0, L)$, we can restrict the square-integrable to ones that vanish outside $(0, L)$. For every $n \in \mathbb{N}$, define a basis
	\[
		\phi_n(x) = \sqrt{\frac{2}{L}} \sin \left( \frac{n \pi x}{L} \right)
	\]
	The $\phi_n$ are orthogonal with respect to the inner product:
	\[
		\begin{aligned}
			\langle \phi_m, \phi_n \rangle
				& = \int_{0}^{L} \overline{\phi_m(x)} \phi_n(x) dx \\
				& = \frac{2}{L} \sin \left( \frac{m \pi x}{L} \right) \sin \left( \frac{n \pi x}{L} \right) dx \\
				& = \frac{1}{L} \int_{0}^{L} \left( \cos \left( \frac{(m - n) \pi x}{L} \right) - \cos \left( \frac{(m + n) \pi x}{L} \right) \right) dx \\
				& = \delta_{m, n} - \delta_{m, -n} \\
				& = \delta_{m, n}
		\end{aligned}
	\]
	So any continuous square-integrable wave function in this region has the unique form
	\[
		\phi(x) = \sum_{n =0}^{\infty} c_n \phi_n(x) = \sqrt{\frac{2}{L}} \sum_{n = 1}^{\infty} \sin \left( \frac{n \pi x}{L} \right)
	\]
	This is precisely Fourier's theorem, with the Fourier coefficients $c_n$ given by
	\[
		c_n = \langle \phi_n, \psi \rangle = \sqrt{\frac{2}{L}} \int_{0}^{L} \sin \left( \frac{n \pi x}{L} \right) \psi(x) dx
	\]
	The norm squared of $\psi$ is
	\[
		\langle \psi, \psi \rangle = \int_{0}^{L} |\psi(x)|^2 dx = \sum_{n = 1}^{\infty} |c_n|^2
	\]
	which is the statement of Parseval's theorem.
\end{example}

\begin{example}
	Consider a pyramid wave function in an infinite potential well in $(0, L)$ defined as
	\[
		\psi(x) = \sqrt{\frac{12}{L}} \cdot \begin{cases}
			\frac{x}{L} & \text{ if } 0 \le x \le \frac{L}{2} \\
			\frac{L - x}{L} & \text{ if } \frac{L}{2} \le x \le L
		\end{cases}
	\]
	Then the Fourier coefficients are given by
	\[
		\begin{aligned}
			c_n
				& = \sqrt{\frac{2}{L}} \int_{0}^{L} \sin \left( \frac{n \pi x}{L} \right) \psi(x) dx \\
				& = \sqrt{\frac{24}{L^2}} \left( \int_{0}^{L / 2} \frac{x}{L} \sin \left( \frac{n \pi x}{L} \right) dx + \int_{L / 2}^{L} \left( 1 - \frac{x}{L} \right) \sin \left( \frac{n \pi x}{L} \right) dx \right) \\
				& = \sqrt{\frac{24}{L^2}} (1 - {(-1)}^n) \int_{0}^{L / 2} \frac{x}{L} \sin \left( \frac{n \pi x}{L} \right) dx \\
				& = \sqrt{24} (1 - {(-1)}^n) \frac{{(-1)}^{(n + 1)/2}}{n^2 \pi^2} \\
				& = \begin{cases}
					\frac{\sqrt{96} {(-1)}^{m + 1}}{{(2m + 1)}^2 \pi^2} & \text{ if } n = 2m + 1 \text{ for some } m \in \mathbb{N} \\
					0 & \text{ otherwise}
				\end{cases}
		\end{aligned}
	\]
	We see that
	\[
		\langle \psi, \psi \rangle = \sum_{n = 1}^{\infty} |c_n|^2 = \frac{96}{\pi^4} \sum_{m = 0}^{\infty} \frac{1}{{(2m + 1)}^4} = 1
	\]
\end{example}

\subsection{Hermitian operators}

\begin{definition}
	Let $V$ be a finite-dimensional complex vector space with Hermitian inner product $\langle \cdot, \cdot \rangle$ and an orthonormal basis $\{ e_j \}$. A \textbf{linear operator} is a map $A: V \rightarrow V$ which satisifes
	\[
		\forall v_1, v_2 \in V, \forall a_1, a_2 \in \mathbb{C}, \quad A (a_1 v_1 + a_2 v_2) = a_1 A(v_1) + a_2 A(v_2)
	\]
\end{definition}

\begin{remark}
	Any linear combination $a_1 A_1 + a_2 A_2$ and composition $A_1 \circ A_2$ of two linear operators $A_1$ and $A_2$ is also a linear operator.
\end{remark}

\begin{definition}
	The \textbf{matrix elements} of a linear operator $A$ in an orthonormal basis $\{ e_j \}$ are defined by
	\[
		A_{i, j} = \langle e_i, A \cdot e_j \rangle
	\]
\end{definition}

\begin{definition}
	The adjoint $A^{\dagger}$ of a linear operator $A$ is defined by
	\[
		\forall v_1, v_2 \in V, \quad \langle v_1, A v_2 \rangle = \langle A^{\dagger} v_1, v_2 \rangle
	\]
\end{definition}

\begin{proposition}
	$A^{\dagger}_{i, j} = \overline{A_{j, i}}$, i.e. $A^{\dagger}$ is the conjugate of the transpose of $A$.
\end{proposition}

\begin{proof}
	\[
		A^{\dagger}_{i, j} = \langle e_i, A^{\dagger} e_j = \langle A e_i, e_j \rangle = \overline{\langle e_j, A e_i} = \overline{A_{i, j}}
	\]
\end{proof}

\begin{proposition}
	The adjoint operation satisfies
	\begin{itemize}
		\item ${(a_1 A_1 + a_2 A_2)}^{\dagger} = \overline{a_1} A_1^{\dagger} + \overline{a_2} A_2^{\dagger}$.
		\item ${(A_1 A_2)}^{\dagger} = A_2^{\dagger} A_1^{\dagger}$.
	\end{itemize}
\end{proposition}

\begin{proof}
	TODO.
\end{proof}

\begin{corollary}
	For every $n \in \mathbb{N}$ and for every polynomial function $f$,
	\[
		{(A^n)}^{\dagger} = {(A^{\dagger})}^n, \quad {f(A)}^{\dagger} = f(A^{\dagger})
	\]
\end{corollary}

\begin{definition}
	A \textbf{Hermitian operator} $A$ is a linear operator that satisfies
	\[
		A = A^{\dagger}
	\]
	Equivalently,
	\[
		\forall v_1, v_2 \in V, \quad \langle v_1, A v_2 \rangle = \langle A v_1, v_2 \rangle \quad \text{or} \quad \overline{A_{j, i}} = A_{i, j}
	\]
	hence the matrix of a Hermitian operator is a Hermitian matrix.
\end{definition}

\begin{definition}
	The matrix elements of $A$, where $A$ is the Hermitian operator that acts on the space of wave functions with the Hermitian inner product defined as before, are given by
	\[
		A_{m, n} := \langle \phi_m, A (\phi_n) \rangle = \int_{-\infty}^{\infty} \overline{\phi_m(x)} A (\phi_n)
	\]
\end{definition}

\begin{definition}
	The \textbf{adjoint} $A^{\dagger}$ of a \textbf{linear differential operator} $A$ is defined by
	\[
		\langle \psi_1, A^{\dagger} (\psi_2) \rangle = \langle A(\psi_1), \psi_2 \rangle
	\]
	and has the same properties as the adjoint of a linear operator. Its matrix elements are given by
	\[
		A^{\dagger}_{m, n} = \overline{A_{n, m}}
	\]
\end{definition}

\begin{theorem}
	The position and momentum operators are Hermitian operators.
\end{theorem}

\begin{proof}
	For the position operator, $\hat{x}$:
	\[
		\begin{aligned}
			\langle \hat{x}(\psi_1), \psi_2 \rangle
				& = \int_{-\infty}^{\infty} \overline{x \psi_1(x)} \psi_2(x) dx \\
				& = \int_{-\infty}^{\infty} \overline{\psi_1(x)} x \psi_2(x) dx \\
				& = \langle \psi_1, \hat{x}(\psi_2)
		\end{aligned}
	\]
	For the momentum operator, $\hat{p}$, by integrating parts:
	\[
		\begin{aligned}
			\langle \hat{p}(\psi_1), \psi_2 \rangle
				& = \int_{-\infty}^{\infty} \overline{-i \hbar \diffp{\psi_1(x)}{x}} \psi_2(x) dx \\
				& = \int_{-\infty}^{\infty} i \hbar \diffp{\overline{\psi_1(x)}}{x} \psi_2(x) dx \\
				& = \int_{-\infty}^{\infty} \overline{\psi_1(x)} \left( -i \hbar \diffp{\psi_2(x)}{x} \right) dx + i \hbar {\left[ \overline{\psi_1(x)} \psi_2(x) \right]}_{\infty}^{\infty} \\
				& = \langle \psi_1, \hat{p}(\psi_2)
		\end{aligned}
	\]
	Since the wave functions must vanish as $|x| \rightarrow \infty$.
\end{proof}

\begin{definition}
	The \textbf{Hamiltonian operator} is defined as
	\[
		\hat{H}(\psi(x)) = \frac{\hat{p}^2 (\psi(x))}{2m} + V(x) = -\frac{\hbar^2}{2m} \diffp[2]{}{x} \psi(x) + V(x)
	\]
	and is a Hermitian operator. It relates to measurements of energy.
\end{definition}

\begin{example}
	For an infinite potential well in $[0, L]$, in Example~\ref{exa:infinitePotentialWellBasisFunctions}, we defined a basis
	\[
		\phi_n(x) = \sqrt{\frac{2}{L}} \sin \left( \frac{n \pi x}{L} \right)
	\]
	The matrix elements of position are given by
	\[
		\begin{aligned}
			x_{m, n}
				& := \langle \phi_m, \hat{x}(\phi_n) \rangle \\
				& = \int_{0}^{L} x \overline{\phi_m(x)} \phi_n(x) dx \\
				& = \frac{2}{L} \int_{0}^{L} x \sin \left( \frac{m \pi x}{L} \right) \sin \left( \frac{n \pi x}{L} \right) dx \\
				& = \frac{1}{L} \int_{0}^{L} x \left( \cos \left( \frac{(m - n) \pi x}{L} \right) - \cos \left( \frac{(m + n) \pi x}{L} \right) \right) dx \\
				& = \begin{cases}
					L / 2 & \text{ if } m = n \\
					\frac{4Lmn}{\pi^2 (m^2 - n^2)} ({(-1)}^{m + n} - 1) & \text{ if } m \ne n
				\end{cases}
		\end{aligned}
	\]
	using the substitution $y = \pi x \ l$ with the fact that for $n \ne 0$,
	\[
		\int_{0}^{\pi} y \cos(ny) dy = \frac{{(-1)}^n - 1}{n^2}
	\]
	The matrix elements $x_{m, n}$ form a Hermitian matrix.
\end{example}

\begin{example}
	For an infinite potential well in $[0, L]$, with the same basis functions as in the last example, the matrix elements of momentum are given by
	\[
		\begin{aligned}
			p_{m, n}
				& := \langle \phi_m, \hat{p}(\phi_n) \rangle \\
				& = -i \hbar \int_{0}^{L} \overline{\phi_m(x)} \diffp{\phi_n(x)}{x} dx \\
				& = -i \hbar \frac{2}{L} \sin \left( \frac{m \pi x}{L} \right) \frac{n \pi}{L} \cos \left( \frac{n \pi x}{L} \right) dx \\
				& = -\frac{i \hbar n \pi}{L^2} \int_{0}^{L} \left( \sin \left( \frac{(m + n) \pi x}{L} \right) + \sin \left( \frac{(m - n) \pi x}{L} \right) \right) dx \\
				& = \begin{cases}
					0 & \text{ if } m = n \\
					\frac{2i \hbar mn}{L (m^2 - n^2)} ({(-1)}^{m + n} - 1) & \text{ if } m \ne n
				\end{cases}
		\end{aligned}
	\]
	using the substitution $y = \pi x \ L$ with the fact that for $n \ne 0$,
	\[
		\int_{0}^{\pi} \sin(ny) = \frac{1 - {(-1)}^n}{n}
	\]
	The matrix elements $p_{m, n}$ form a Hermitian matrix.
\end{example}

\begin{example}
	For an infinite potential well in $[0, L]$, with the same basis functions as in the last example, the matrix elements of Hamiltonian operator are given by
	\[
		\begin{aligned}
			H_{m, n}
				& := \langle \phi_m, H(\phi_n) \rangle \\
				& = -\frac{\hbar^2}{2m} \int_{0}^{L} \overline{\phi_m(x)} \diffp[2]{\phi_n(x)}{x} dx \\
				& = E_n \frac{2}{L} \int_{0}^{L} \sin \left( \frac{m \pi x}{L} \right) \sin \left( \frac{n \pi x}{L} \right) dx \\
				& = E_n \delta_{m, n}
		\end{aligned}
	\]
	where
	\[
		E_n = \frac{\hbar^2 \pi^2 n^2}{2m L^2}
	\]
	The matrix elements $H_{m, n}$ form a diagonal matrix. The above calculation shows that the $\phi_n$ are eigenfunctions of the Hamiltonian operator, since
	\[
		\hat{H}(\phi_n(x)) = E_n \phi_n(x)
	\]
\end{example}

\subsection{The spectrum of a Hermitian operator}

\begin{definition}
	A wave function $\psi_a(x)$ is an \textbf{eigenfunction} of a Hermitian differential operator $A$ with eigenvalue $a$ if it satisfies
	\[
		A(\psi_a(x)) = a \psi_a(x)
	\]
\end{definition}

\begin{proposition}
	For a normalised eigenfunction $\psi_a(x)$ of a Hermitian differential operator $A$ with eigenvalue $a$,
	\begin{itemize}
		\item The expectation value of $A^n$ is $a^n$ for every $n \in \mathbb{N}$.
		\item The uncertainty of $A$ is $0$.
	\end{itemize}
\end{proposition}

\begin{proof}
	\hfill
	\begin{itemize}
		\item $\langle A^n \rangle = \langle \psi_a, A^n(\psi_a) \rangle = \langle \psi_a, a6n \psi_a \rangle = a^n \langle \psi_a, \psi_a \rangle = a^n$.
		\item $\Delta A = \sqrt{\langle A^2 \rangle - {\langle A \rangle}^2} = \sqrt{a^2 - a^2} = 0$.
	\end{itemize}
\end{proof}

\begin{remark}
	This means that $\psi_a$ is a wave function such that measurements of $A$ will be $a$ with probability $1$.
\end{remark}

\begin{theorem}
	Let $A$ be a Hermitian operator. Then
	\begin{enumerate}
		\item The eigenvalues of $A$ are real, and
		\item If two eigenfunctions $\psi_1(x), \psi_2(x)$ of $A$ have distinct eigenvalues $a_1 \ne a_2$, then $\psi_1(x)$ and $\psi_2(x)$ are orthogonal.
	\end{enumerate}
\end{theorem}

\begin{proof}
	Let $\psi_1(x), \psi_2(x)$ be eigenfunctions of $A$ with eigenvalues $a_1$ and $a_2$, so
	\[
		A(\psi_1(x)) = a_1 \psi_1(x), \quad A(\psi_2(x)) = a_2 \psi_2(x)
	\]
	Then
	\[
		\begin{aligned}
			\langle \psi_1, A(\psi_2) \rangle & = \langle \psi_1, a_2 \psi_2 \rangle = a_2 \langle \psi_1, \psi_2 \rangle \\
			\langle A(\psi_2), \psi_1 \rangle & = \langle a_1 \psi_1, \psi_2 \rangle = \overline{a_1} \langle \psi_1, \psi_2 \rangle \\
			\Longrightarrow 0 & = (\overline{a_1} - a_2) \langle \psi_1, \psi_2 \rangle
		\end{aligned}
	\]
	If $a_1 = a_2$ and 4 then $(\overline{a_1} - a_1) \langle \psi, \psi \rangle = 0$, so if $\psi$ is non-zero, then $\overline{a_1} = a_1$, so $a_1 = a_2 \in \mathbb{R}$. If $a_1 \ne a_2$ then $(a_1 - a_2) \langle \psi_1, \psi_2 \rangle = 0 \Longrightarrow \langle \psi_1, \psi_2 \rangle = 0$
\end{proof}

\begin{definition}
	For a Hermitian operator $A$, a discrete spectrum of eigenvalues $\{ a_n \}$ is called \textbf{non-degenerate} if there is one linearly independent eigenfunction $\phi_n$ for each eigenvalue $a_n$.
\end{definition}

\begin{corollary}
	We can construct an orthonormal basis with normalised eigenfunctions $\phi_n$, so that
	\[
		\langle \phi_m, \phi_n \rangle = \delta_{m, n}
	\]
	This means any continuous square-integrable wave function $\psi$ has a unique expansion
	\[
		\psi(x) = \sum_n c_n \phi_n(x)
	\]
	where
	\[
		c_m = \langle \phi_m, \psi \rangle = \sum_n c_n \langle \phi_m, \phi_n \rangle = \sum_n c_n \delta_{m, n}
	\]
\end{corollary}

\begin{corollary}
	The norm of a wave function $\psi$ is
	\[
		\langle \psi, \psi \rangle = \sum_{m, n} \overline{c_m} c_n \langle \phi_m, \phi_n \rangle = \sum_n |c_n|^2
	\]
	and if $\psi$ is normalised, then
	\[
		\langle \psi, \psi \rangle = 1
	\]
	So the $|c_n|^2$ can be interpreted as the probability of a measurement of $A$ being $a_n$.
\end{corollary}

\begin{example}
	In an infinite potential well in $(0, L)$, the $\phi_n$ defined in Example~\ref{exa:infinitePotentialWellBasisFunctions} eigenfunctions for the Hamiltonian operator $\hat{H}$ with eigenvalues
	\[
		E_n = \frac{\hbar^2 \pi^2 n^2}{2m L^2}
	\]
	So every wave function $\psi$ can be written as
	\[
		\psi(x) = \sum_{n > 0} c_n \phi_n(x)
	\]
	and $|c_n|^2$ is the probability that a measurement of energy will be $E_n$. These probabilities summing to $1$ agrees with Parseval's theorem.
\end{example}

\begin{definition}
	A Hermitian operator $A$ can have a continuous spectrum of eigenvalues instead a discrete one. Here, we cannot choose eigenfunctions to form a complete orthonormal basis as before. To help with this case, we define the \textbf{Dirac delta function}, $\delta$, (which is not actually a function but a distribution). Roughly speaking,
	\[
		\delta(a) = \begin{cases}
			0 & \text{ if } a \ne 0 \\
			\infty & \text{ if } a = 0
		\end{cases}
	\]
	$\delta$ also satisfies
	\[
		\int_{-\infty}^{\infty} \delta(a) da = 1
	\]
	$\delta$ can be more precisely defined as the limit of the Gaussian function
	\[
		\delta_{\epsilon} (a) = \frac{1}{\epsilon \sqrt{\pi}} e^{-a^2 / \epsilon^2}
	\]
	as $\epsilon \rightarrow 0^+$.
\end{definition}

\begin{proposition}
	(\textbf{Properties of the Dirac delta function})
	\begin{enumerate}
		\item For every continuous function $f(a)$,
		\[
			\int_{-\infty}^{\infty} \delta(a - a') f(a') da = f(a)
		\]
		\item The Dirac delta function is the Fourier transform of $1$:
		\[
			\delta(a) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} e^{i a a'} da'
		\]
		This implies that $\delta(a) = \delta(-a) = \overline{\delta(a)}$.
	\end{enumerate}
\end{proposition}

\begin{remark}
	Property 1 in the above proposition is the continuous version of $\sum_n \delta_{m, n} f_n = f_m$, so $\delta(a - a')$ is the continuous version of the identity matrix $\delta_{m, n}$.
\end{remark}

\begin{proposition}
	For a Hermitian operator $A$ with a continuous spectrum, there exists a basis of eigenfunctions $\phi_a(x)$ with eigenvalues $a \in \mathbb{R}$ that satisfy
	\[
		\langle \phi_a, \phi_{a'} \rangle = \delta(a - a')
	\]..
\end{proposition}

\begin{corollary}
	Eigenfunctions $\phi_a(x)$ are therefore not square-normalisable since $\langle \phi_a, \phi_a \rangle = \infty$. However, every square-normalisable wave function can be uniquely expanded as
	\[
		\psi(x) = \int_{-\infty}^{\infty} c(a) \phi_a(x) da
	\]
	where $c(a)$ are complex coefficients that are continuous functions of $a$. $c(a)$ are given by
	\[
		\begin{aligned}
			c(a)
				& = \int_{-\infty}^{\infty} c(a') \delta(a - a') da' \\
				& = \int_{-\infty}^{\infty} c(a') \langle \phi_a, \phi_{a'} \rangle da' \\
				& = \langle \phi_a, \psi \rangle
		\end{aligned}
	\]
	which is the continuous version of $\langle \phi_n, \psi \rangle = c_n$.
\end{corollary}

\begin{corollary}
	The norm of a wave function $\psi$ can be written as
	\[
		\begin{aligned}
			\langle \psi, \psi \rangle
				& = \int_{-\infty}^{\infty} \overline{c(a)} c(a') \langle \phi_a, \phi_{a'} \rangle da da' \\
				& = \int_{-\infty}^{\infty} \overline{c(a)} c(a') \delta(a - a') da da' \\
				& = \int_{-\infty}^{\infty} |c(a)|^2 da
		\end{aligned}
	\]
	which is the continuous version of $\langle \psi, \psi \rangle = \sum_n |c_n|^2$.

	Note that for a normalised wave function $\psi$,
	\[
		\langle \psi, \psi \rangle = \int_{-\infty}^{\infty} |c(a)|^2 da = 1
	\]
	so $|c(a)|^2$ can be interpreted as a probability distribution for measurements of $A$.
\end{corollary}

\begin{example}
	The eigenfunctions of the momentum operator $\hat{p} = -i \hbar \diffp{}{x}$ are $e^{ipx / \hbar}$ with eigenvalue $p$. We normalise these eigenfunctions, choosing
	\[
		\phi_p(x) = \frac{1}{\sqrt{2 \pi \hbar}} e^{ipx / \hbar}
	\]
	which gives
	\[
		\langle \psi_p, \psi_{p'} \rangle = \frac{1}{2 \pi \hbar} \int_{-\infty}^{\infty} e^{i(p - p')x / \hbar} dx = \delta(p - p')
	\]
	So we can write a wave function $\psi$ as
	\[
		\psi(x) = \int_{-\infty}^{\infty} c(p) \phi_p(x) dp = \frac{1}{\sqrt{2 \pi \hbar}} \int_{-\infty}^{\infty} c(p) e^{ipx / \hbar} dp
	\]
	which is the result of the Fourier transform.
\end{example}

\end{document}